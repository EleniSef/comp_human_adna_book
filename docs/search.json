[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Methods for human population genetics and ancient DNA",
    "section": "",
    "text": "Preface\nThis book summarises prepared mini-courses for various computational tools and methods in the field of human archaeogenetic data analysis, with a particular emphasis on population genetics.\nThe chapters are contributed by different authors, as indicated in the Yaml-frontmatter of each chapter’s .qmd source file."
  },
  {
    "objectID": "Quarto_intro.html#setting-up",
    "href": "Quarto_intro.html#setting-up",
    "title": "1  Introduction to Quarto",
    "section": "1.1 Setting up",
    "text": "1.1 Setting up\nQuarto is the “next generation” of R Markdown and is usable on different tools.\n\nHere, we will describe how to set up your environment to use Quarto in RStudio, and VSCode.\nQuarto in general is set up to be very intuitive and user friendly. And while it is possible to set up different documents simultaneously, those can also easily be set up to in just the way you need for whatever occasion. So, for either communicating your results to collaborators, discuss code with your supervisor, setting up a website or writing your paper, to just name some scenarios, quarto comes in quite handy. So, let’s begin:"
  },
  {
    "objectID": "Quarto_intro.html#rstudio",
    "href": "Quarto_intro.html#rstudio",
    "title": "1  Introduction to Quarto",
    "section": "1.2 RStudio",
    "text": "1.2 RStudio\nFor this, you have to download RStudio first. If you have done this already, we can get started.\n\n1.2.1 Getting started\nFirst, we have to install the quarto package using the following command in our console:\ninstall.packages(\"quarto\")\nNow we are ready to set up a quarto document.\nFor this we open a new project and select the quarto document we want to create. You can choose to set up a git repository as well. For practicality, I usually also tick the box visual markdown editor.\n\nThe new project will look like this:\n\n\n\n1.2.2 Universal instructions\nWhen setting up your document, quarto will always provide you with some presets. So first, we will have a look into the .qmd files, for they are handled the same way, regardless the format you are setting up (website, book, presentation, etc.).\n\n1.2.2.1 Render & save\nIf we now click on render, we will be provided with the preview of our final version if the project in the viewer.\nImportant: Do not mistake “save” with “render”. Just by saving, your document did not get rendered automatically, unless you tick the box “Render on Save”. You have to tick that box on each .qmd file individually though.\n\n\n\n1.2.2.2 Visual and Source\nIf you have chosen the Visual option on your toolbar, the preview will mostly resemble your .qmd files. If you are more comfortable with the R markdown optics, you can switch to Source.\nIn the Source version, you can write up your document in LaTeX.\n\n\n\n1.2.2.3 .qmd files\nQuarto will automatically provide you with two .qmd files, as well as a .yml file.\nThe .qmd files respond to the individual pages of your website or chapter of your book or pages of your presentation, etc. You can shape them individually or define the layout for all of them in the .yml file, to which we will get later.\nIn your .qmd files you also find a yaml at the top of your document, separated by\n---\n---\nWithin these, you can define the outline of .qmd file individually, starting with the page header. Other options, you might be using in the future are:\nauthor: Jessi Doe\n-&gt; will add an author underneath the header\nexecute:    echo: true\n-&gt; if &gt;true&lt;, code will be visible\ntoc:true\n-&gt; if &gt;true&lt;, a table of content will be automatically added\nbibliography: your_references.bib\n-&gt; file or list of files for your references\nIt is crucial to stick to the spacing. Otherwise, an error will occur.\n\n1.2.2.3.1 Insert\nIf you click Insert, you will realize, quarto provides you with a lot of options and shortcuts as well. So by just selecting on your chosen item to insert, it will be added to the document, while you are also provided with options on the appearance (in the case of figure/images or tables, etc). We will here have a brief look into how to work with R code and how to use the reference option.\n\n\n1.2.2.3.2 R code\nTo add R code to your file, you select Insert, select Code Cell and choose the kind of code you want to insert. In this case, R. There are some things to keep in mind though.\nDepending on how you have set up your .qmd file (or your .yml), your code will be visible or not on your website. To check your output, you can click the green arrow for the latest bit of code or the grey arrow above a bar to run the previous code.\n\nBut in case there are some chunks of code, you do not want to show all the time, there are some nice sets.\nSo if we just load the library tidyverse, for example, the additional information regardless and it will be also visible on our website.\n\nTo avoid this, we can set up a code chunk, looking like this:\n\nThis will prevent the output of this code chunk to be depicted on your website, while the package is otherwise active and can be used in the following R code. This is, by the way, true for all R code and data sets you will use: they will be active in your document and can be used in different code chunks, once provided.\nA code block included in your document could look like the following. Here I used the option\ncode-fold: true so the code can be extended. This option is only available in html though.\n\n\n\n1.2.2.3.3 References\nDepending on which citation program you are using, quarto is able to connect to it (Zotero works, for example). So, when selecting to insert a Citation, you can choose to simply add a reference from your program.\nAlternatively, you are provided with some options to choose from:\n\nIn your source code and your website, a citation will be depicted as follows:\n\nThe citation will also automatically be added to a references.bib as well as to a references.qmd and is therefore available on your website on the page “References”, which also will be created automatically.\n\n\n\n\n1.2.3 Render your document\nWhen done with setting up your documents, you would like to have the actual output. Depending on the format you set in your .yml file, your output can be a html, pdf, MS Word, OpenOffice, or ePUB file. To create those, the terminal in your RStudio Project is used.\nBy using the command:\nquarto render\nall formats you predefined in your .yml file will be rendered.\nIn case you are only interested in one format to be rendered, you can specify your command:\nquarto render -to pdf\nYour rendered document should now look like this:\n\n\n\n1.2.4 Quarto website\nIn the provided .qmd files, the index is also the first page of your website. As you might have noticed, quarto already sets up a navbar as well as a search function on your website.\n\n\n1.2.4.1 .yml files\nWhile your .qmd files contain information about one page, the .yml file defines the overall looks of the website.\nHere, you can define the type of your project (in this case a website), you can change the name of the website (Title), define your navbar (which shall be your landing page and in what order your pages shall be set up) or the overall appearance of your website in general (theme, css, toc, backgroundcolor, etc.). For different styles and layouts, check out the quarto website again.\n\n\n\n\n1.2.5 Quarto book\nThe setup of the .yml file in a quarto book is slightly different than that of the website. So here are some general remarks about them.\n\n1.2.5.1 .yml files\nWe see, for example, that instead of a navbar, we find chapters. Those will appear in the listed order in your book.\nWe also already get provided with a bibliography and the responding .bib file. If you have other .bib files, those can be included in your references, by just adding them to bibliography.\n\n_________________________________________________________________________\nWith this, you should at least have some ideas on how you can use quarto in your daily work routine. Happy coding and please feel free to contact me for any remarks or questions. I am happy to try and help."
  },
  {
    "objectID": "Quarto_intro.html#vscode",
    "href": "Quarto_intro.html#vscode",
    "title": "1  Introduction to Quarto",
    "section": "1.3 VSCode",
    "text": "1.3 VSCode\nMuch of the concepts as described in the RStudio tutorial above apply equally well to using Quarto in VSCode, just with a different interface to execute them.\nHere we will describe how to set up VSCode and Quarto, and how to preview and render Quarto objects within the VSCode interface.\nTo understand about more about the details of which files to edit etc., please see the RStudio description above.\n\n1.3.1 Getting Started\n\nInstall the Quarto CLI tool for your operating system from the Quarto Website\nInstall the VSCode Quarto extension\n\n\n\n1.3.2 Using Quarto\nThe basic workflow is as follows:\n\nCreate or modify .qmd objects etc as described above in the Rstudio section about Quarto markdown files\nWithin VSCode, make sure you’ve opened it in the repository containing all the files\nPress ctrl + shift + p to bring up your command palette\n\nTo preview a local ‘live’ version of HTML or website versions, you can type Quarto: preview. To close the live preview, press ctrl+c in the VSCode terminal.\nTo render all the files e.g. final HTML and/or PDF versions, you can type Quarto: Render Project, where you will be given different options depending on the formats defined in the _quarto.yml file.\n\n\nFor further VSCode integrations, just type Quarto: into your command palette and explore all the options."
  },
  {
    "objectID": "poseidon.html#the-components-of-poseidon",
    "href": "poseidon.html#the-components-of-poseidon",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.1 The components of Poseidon",
    "text": "2.1 The components of Poseidon\n\n\n\nOverview of the Poseidon framework\n\n\nPoseidon is an entire ecosystem build on top of the data format specification of the Poseidon package.\n\n2.1.1 The Poseidon package\nA Poseidon package bundles genotype data in EIGENSTRAT/PLINK format with human- and machine-readable meta-data.\n\n\n\nThe files in a Poseidon package\n\n\nIt includes sample-wise context information like spatio-temporal origin and genetic data quality in the .janno-file, literature in the .bib, and pointers to sequencing data in the .ssf file.\n.janno and .ssf have many predefined and specified columns, but can store arbitrary additional variables.\n\n\n2.1.2 The software tools\ntrident is a command line application to create, download, inspect and merge Poseidon packages – and therefore the central tool of the Poseidon framework. The init subcommand creates new packages from genotype data, fetch downloads them from the public archives through the Web-API, and forge merges and subsets them as specified. list gives an overview over entities in a set of packages and validate confirms their structural integrity.\nxerxes is derived from trident and allows to directly perform various basic and experimental genomic data analyses on Poseidon packages. It implements allele sharing statistics (\\(F_2\\), \\(F_3\\), \\(F_4\\), \\(F_{ST}\\)) with a flexible permutation interface.\njanno is an R package to simplify reading .janno files into R and the popular tidyverse ecosystem (Wickham et al. (2019)). It provides an S3 class janno that inherits from tibble.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\nqjanno is another command line tool to perform SQL queries on .janno files. On start-up it creates an SQLite database in memory and reads .janno files into it. It then sends any user-provided SQL query to the database server and forwards its output.\n\n\n2.1.3 The public archives\nThe Poseidon community maintains public archives for Poseidon packages to establish a central, open point of access for published, archaeogenetic genotype data.\n\nThe Community Archive: Author supplied, per-paper packages with the genotype data published in the respective papers. Partially pre-populated from various versions of the AADR.\nThe AADR Archive: Complete and structurally unified releases of the Allen Ancient DNA Resource (Mallick et al. (2023)) repackaged in the Poseidon package format.\nThe Minotaur Archive: Per-paper packages with genotype data reprocessed by the Minotaur workflow (see below).\n\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif Lazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023. “The Allen Ancient DNA Resource (AADR): A Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\nThe data is versioned with Git and hosted on GitHub for easy co-editing and automatic structural validation.\nIt can be accessed through a Web-API with various endpoints at server.poseidon-adna.org, e.g. /packages for a JSON list of packages in the community archive.\nThis API enables a little Archive explorer web app on the Poseidon website.\n\n\n2.1.4 The Minotaur workflow\nThe Minotaur workflow is a semi-automatic workflow to reproducibly process published sequencing data from the International Nucleotide Sequence Database Collaboration (INSDC) archives into Poseidon packages.\nCommunity members can request new packages by submitting a build recipe as a Pull Request against a dedicated submission GitHub repository. This recipe is derived from a Sequencing Source File (.ssf), describing the sequencing data for the package and where it can be downloaded.\nUsing the recipe, the sequencing data gets processed through nf-core/eager (Fellows Yates et al. (2021)) on computational infrastructure of MPI-EVA, using a standardised, yet flexible, set of parameters.\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\nThe generated genotypes, together with descriptive statistics of the sequencing data (Endogenous, Damage, Nr_SNPs, Contamination), are compiled into a Poseidon package, and made available to users in the Minotaur archive."
  },
  {
    "objectID": "poseidon.html#forging-a-dataset-with-trident",
    "href": "poseidon.html#forging-a-dataset-with-trident",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.2 Forging a dataset with trident",
    "text": "2.2 Forging a dataset with trident\nforge creates new Poseidon packages by extracting and merging packages, populations and individuals/samples from your Poseidon repositories. It can also work directly with your genotype data. In addition, forge allows merging of multiple data sets (packages), in contrast to mergeit which merges only two data sets at a time.\n(-f/--forgeString) can be used to query entire packages, groups or individuals. In general --forgeString query consists of multiple entities, inside \"\" separated by , .\n\nTo include all individuals in a Poseidon package, use * to surround the package title.*2019_Jeong_InnerEurasia* . In cases where only genotype files are available, use the file name prefix.\nTo include certain group(s) from a Poseidon package, simply add them to the -f query. No specific markers are required. Russia_HG_Karelia\nTo extract individuals only, surround them by &lt; and &gt;. &lt;ALA026&gt; . To exclude individuals add package name *package* and &lt;individual&gt; with a dash sign. \"*2021_Saag_EastEuropean-3.2.0*,-&lt;NIK003&gt;\"\n\n\ntrident forge \\\n  -p pileupcaller.double.geno \\\n  -d 2021_Saag_EastEuropean-3.2.0 \\\n  -d 2016_FuNature-2.1.1 \\\n  -f \"*pileupcaller.double*,Russia_AfontovaGora3,&lt;NIK003&gt;\" \\\n  -o testpackage \\\n  --outFormat EIGENSTRAT \\\n  /\nForge selection language\nforge has a an optional flag --intersect, that defines, if the genotype data from different packages should be merged with an intersect instead of the default union operation. The default is to output the union of all SNPs, by setting the additional SNPs from the other merged package as missing in the samples that did not have them originally. This option is useful for making a data set based on Human Origins (HO) SNPs for analysis like PCA and ADMIXTURE.\ntrident forge \\\n  --intersect \\\n  -p pileupcaller.double.geno \\\n  -d 2012_PattersonGenetics-2.1.3 \\\n  -o testpackage_HO \\\n  --outFormat EIGENSTRAT \\\n  /\nIn case of PCA, --forgeFile can be used to merge necessary populations/groups from the available packages in the community archive to create specific PCA configurations.\ntrident forge \\\n  -d /path/to/community/archive \\\n  --forgeFile WestEurasia_poplist.txt \\\n  -o WE_PCA \\\n  /\nIn addition, --selectSnps allows to provide forge with a SNP file in EIGENSTRAT (.snp) or PLINK (.bim) format to create a package with a specific selection. This option generates a package with exactly the SNPs listed in this file."
  },
  {
    "objectID": "poseidon.html#contributing-to-the-community-archive",
    "href": "poseidon.html#contributing-to-the-community-archive",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.3 Contributing to the community archive",
    "text": "2.3 Contributing to the community archive\n\n\n\nPoseidon needs your data as soon as it is published\n\n\nTo maintain the public data archives, specifically the community archive and the minotaur archive, Poseidon depends on work donations by an interested community.\nMany practitioners of archaeogenetics both produce genotype data from archaological contexts and require the reference data from other publications, provided in public archives, to contextualize it.\nIf authors themselves provide high-quality, easily accessible versions of their data beyond the raw data available at the INSDC databases, they gain at least three important advantages:\n\nTheir work will be easily findable and potentially cited more often.\nThey have primacy over how their data is communicated. That is, which genotypes, dates or group names they consider correct.\nTheir results for derived, genotype based analyses (PCA, F-Statistics, etc.) can be reproduced exactly.\n\nAnd the whole community wins, because sharing the tedious data preparation tasks empowers all researchers to achieve more in shorter time.\n\n\n\n\nThis tutorial explains the main cornerstones of a workflow to add a new Poseidon package to the community archive after publishing the corresponding dataset. The process is documented in more detail in a Submission guide on the website.\n\nMake yourself familiar with a number of core technologies. This is less daunting than it sounds, because: Superficial knowledge is sufficient and knowing them is useful beyond this particular task.\n\n\nCreating and validating Poseidon packages with the trident tool.\nFree and open source distributed version control with Git.\nCollaborative working on Git projects with GitHub.\nHandling large files in Git using Git LFS.\n\n\nCreate a package from your genotype data and fill it with a suitable set of meta and context information.\n\n\ntrident init allows to wrap genotype data in a dummy Poseidon package. Imagine we had genotype data for a number of individuals in EIGENSTRAT format:\n\n\nmyData.ind\n\nSample1  M       ExamplePop1\nSample2  F       ExamplePop1\nSample3  M       ExamplePop2\n\n\n\nmyData.snp\n\n           rs3094315     1        0.020130          752566 G A\n          rs12124819     1        0.020242          776546 A G\n          rs28765502     1        0.022137          832918 T C\n\n\n\nmyData.geno\n\n099\n922\n999\n\nWith trident init -p myData.geno -o myPackage we can create a basic package around this data.\n$ ls myPackage\nmyData.geno  myData.snp     myPackage.janno\nmyData.ind   myPackage.bib  POSEIDON.yml\nIn a next step we modify POSEIDON.yml, .janno and .bib to include the context information we consider relevant. All of these files are well specified and documented, so we only demonstrate a minimal change for this example:\nWe replace the main contributor for the package.\n\n\nmyPackage/POSEIDON.yml\n\nposeidonVersion: 2.7.1\ntitle: myPackage\ndescription: Empty package template. Please add a description\ncontributor:\n- name: Clemens Schmid               #- name: Josiah Carberry\n  email: clemens_schmid@eva.mpg.de   #  email: carberry@brown.edu\n  orcid: 0000-0003-3448-5715         #  orcid: 0000-0002-1825-0097\npackageVersion: 0.1.0\nlastModified: 2023-10-18\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: myData.geno\n  snpFile: myData.snp\n  indFile: myData.ind\n  snpSet: Other\njannoFile: myPackage.janno\nbibFile: myPackage.bib\n\nWhen we applied all necessary modifications we can confirm that the package is still valid with trident validate -d myPackage.\n\n\nSubmit the package to the community archive.\n\n\nTo submit the package we have to create a fork of the community archive repository on GitHub. This requires a GitHub account.\n\n\n\n\nPress the fork button in the top right corner to fork a repository on GitHub\n\n\n\nAnd then clone the fork to our computer, while omitting the large genotype data files. Note that this requires several setup steps to work correctly:\n\nGit has be installed for your computer (see here)\nYou must have created an ssh key pair to connect to GitHub via ssh (see here)\nGit LFS has to be installed (see here) and and configured for your user with git lfs install\n\nGIT_LFS_SKIP_SMUDGE=1 git clone git@github.com:&lt;yourGitHubUserName&gt;/community-archive.git\nWith the cloned repository on our system we can copy the files into the repositories directory and commit the changes.\n\n\nin the community-archive directory\n\ncp -r ../myPackage myPackage\ngit add myPackage\ngit commit -m \"added a first draft of myPackage\"\ngit push\n\nIn a last step we can open a Pull Request on GitHub from our fork to the original archive repository. Poseidon core members will take it from here.\n\n\n\n\nWhen you pushed to your fork, GitHub will automatically offer to “contribute” to the source repository"
  },
  {
    "objectID": "fstats.html#admixture---f3-statistics",
    "href": "fstats.html#admixture---f3-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.1 Admixture - F3 Statistics",
    "text": "3.1 Admixture - F3 Statistics\nF3 statistics are a useful analytical tool to understand population relationships. F3 statistics, just as F4 and F2 statistics measure allele frequency correlations between populations and were introduced by Nick Patterson (Patterson et al. 2012), but see also (Peter 2016) for another introduction.\nF3 statistics are used for two purposes: i) as a test whether a target population (C) is admixed between two source populations (A and B), and ii) to measure shared drift between two test populations (A and B) from an outgroup (C).\nF3 statistics are in both cases defined as the product of allele frequency differences between population C to A and B, respectively:\n\\[F3(A,B;C)=\\langle(c−a)(c−b)\\rangle\\]\nHere, \\(\\langle\\cdot\\rangle\\) denotes the average over all genotyped sites, and a, b and c denote the allele frequency for a given site in the three populations A, B and C.\nIt can be shown that if \\(F3(A, B; C)\\) is negative, it provides unambiguous proof that population C is admixed between populations A and B, as in the following phylogeny (taken from Figure 1 from (Patterson et al. 2012):\n\nIntuitively, an F3 statistics becomes negative if the allele frequency of the target population C is on average intermediate between the allele frequencies of A and B. Consider as an extreme example a genomic site where \\(a=0\\), \\(b=1\\) and \\(c=0.5\\). Then we have \\((c−a)(c−b)=−0.25\\), which is negative. So if the entire statistics is negative, it suggests that in many positions, the allele frequency c is indeed intermediate, suggesting admixture between the two sources.\nOne way to understand this is by looking what happens to a list of SNPs and allele frequencies for groups A, B and C:\n\n\n\nSNP\nA\nB\nC\n\\((c-a)(c-b)\\)\n\n\n\n\n1\n1\n0\n0.5\n-0.25\n\n\n2\n0.8\n0\n0\n0\n\n\n3\n0\n0.7\n0\n0\n\n\n4\n0.1\n0.5\n0.3\n-0.04\n\n\n5\n0\n0.1\n0.2\n0.02\n\n\n6\n1\n0.2\n0.9\n-0.07\n\n\n\\(F_3(A, B;C)\\)\n\n\n\n-0.057\n\n\n\nEvery SNP where C has an allele frequency intermediate between A and B contributes negatively. Here, the average is also negative, providing evidence for admixture. For statistical certainty, an error bar for this estimate is needed, which is typically computed via Jackknife (see for example the xerxes whitepaper).\n\n\n\n\n\n\nCaution\n\n\n\nIf an F3 statistics is not negative, it does not proof that there is no admixture!\n\n\n\n3.1.1 Computing Admixture-F3 with xerxes\nWe will use this statistics to test if Finnish are admixed between East and West, using different Eastern and Western sources. In the West, we use French, Icelandic, Lithuanian and Norwegian as source, and in the East we use Nganasan and one of the ancient individuals analysed in (Lamnidis et al. 2018), from the site of Bolshoy Oleni Ostrov from the Northern Russian Kola-peninsula, and dating to 3500 years before present.\n\n\n\n\n\n\nTip\n\n\n\nIf you happen to have downloaded a copy of the Poseidon Community Archive already, then just use the path to that archive in the following commands. Otherwise you download the entire archive via\n\ntrident fetch -d /somewhere/to/store/the/archive --downloadAll \n\nor just the relevant packages for the examples in this chapter:\n\ntrident fetch -d /somewhere/to/store/the/archive -f \"*2012_PattersonGenetics*,*2014_RaghavanNature*,*2014_LazaridisNature*,*2018_Lamnidis_Fennoscandia*\"\n\n\n\nWe use the software xerxes fstats from the Poseidon Framework. Here is a command line that computes 8 statistics for us:\n\nxerxes fstats -d ~/dev/poseidon-framework/community-archive \\\n    --stat \"F3(Nganasan,French,Finnish)\" \\\n    --stat \"F3(Nganasan, Icelandic, Finnish)\" \\\n    --stat \"F3(Nganasan, Lithuanian, Finnish)\" \\\n    --stat \"F3(Nganasan, Norwegian, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, French, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Icelandic, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Lithuanian, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Norwegian, Finnish)\" \\\n\n\n\n\n\n\n\nNote\n\n\n\nNote that xerxes fstats will automatically find the right packages from your local archive that contain these groups. You can see in the output of the program which packages contribute:\n[Info]    5 relevant packages for chosen statistics identified:\n[Info]    *2012_PattersonGenetics-2.1.3*\n[Info]    *2014_LazaridisNature-4.0.2*\n[Info]    *2016_LazaridisNature-2.1.3*\n[Info]    *2018_Lamnidis_Fennoscandia-2.1.0*\n[Info]    *2019_Flegontov_PalaeoEskimo-2.2.1*\nSo these five packages contain the samples requested in these statistics. You can inquire about this also more manually using trident list\n\n\nHere is the result that you should get, nicely layouted in a Text-table:\n.-----------.----------------.------------.---------.---.---------.----------------.--------------------.------------------.---------------------.\n| Statistic |       a        |     b      |    c    | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife |  Z_score_Jackknife  |\n:===========:================:============:=========:===:=========:================:====================:==================:=====================:\n| F3        | Nganasan       | French     | Finnish |   | 593124  | -1.0450e-3     | -1.0451e-3         | 1.2669e-4        | -8.249133659451905  |\n| F3        | Nganasan       | Icelandic  | Finnish |   | 593124  | -1.1920e-3     | -1.1920e-3         | 1.3381e-4        | -8.908381869946188  |\n| F3        | Nganasan       | Lithuanian | Finnish |   | 593124  | -1.1605e-3     | -1.1605e-3         | 1.5540e-4        | -7.4680182465607245 |\n| F3        | Nganasan       | Norwegian  | Finnish |   | 593124  | -1.0913e-3     | -1.0914e-3         | 1.3921e-4        | -7.83945981272796   |\n| F3        | Russia_Bolshoy | French     | Finnish |   | 542789  | -6.1807e-4     | -6.1809e-4         | 1.0200e-4        | -6.059872900228102  |\n| F3        | Russia_Bolshoy | Icelandic  | Finnish |   | 542789  | -6.2801e-4     | -6.2802e-4         | 1.1792e-4        | -5.325695961373772  |\n| F3        | Russia_Bolshoy | Lithuanian | Finnish |   | 542789  | -3.7310e-4     | -3.7310e-4         | 1.2973e-4        | -2.8760029685791637 |\n| F3        | Russia_Bolshoy | Norwegian  | Finnish |   | 542789  | -3.7646e-4     | -3.7653e-4         | 1.1630e-4        | -3.2375830440434323 |\n'-----------'----------------'------------'---------'---'---------'----------------'--------------------'------------------'---------------------'\n\n\n\n\n\n\nTip\n\n\n\nUse the option -f &lt;FILE&gt; to output the results additionally to a tab-separated file, or --raw if you prefer the standard output to be tab-separated\n\n\nYou can see that in all cases this statistic is negative (Estimate_Total). The next two columns ( Estimate_Jackknife and StdErr_Jackknife) are computed using Jackknifing (see the xerxes whitepaper for details). The last column is the Z score, and it is important here: It gives the deviation of the f3 statistic from zero in units of the standard error. As general rule, a Z score of -3 or more suggests a significant rejection of the Null hypothesis that the statistic is not negative. In this case, all of the statistics are significantly negative (with one borderline exception), proving that Finnish have ancestral admixture of East and West Eurasian ancestry. Here, Eastern ancestry is represented by Nganasan or Russia_Bolshoy, respectively, while Western ancestry by French, Icelandic, Lithuanian and Norwegian, respectively. Note that the statistics does not suggest when this admixture happened!\n\n\n3.1.2 Running xerxes via a configuration file\nAs you will have noticed, the command line above is getting quite long, since a separate --stat option has to be entered for every statistic to be computed. There is a more powerful and elegant interface to xerxes, which uses a configuration file in YAML format. To illustrate it, let us consider the configuration file (which can be found in fstats_working/F3_finnish.config in the git-repository of this book) needed to compute the same statistic as above:\nfstats:\n- type: F3\n  a: [\"Nganasan\", \"Russia_Bolshoy\"]\n  b: [\"French\", \"Icelandic\", \"Lithuanian\", \"Norwegian\"]\n  c: [\"Finnish\"]\nYou can then run xerxes as\n\nxerxes fstats -d ~/dev/poseidon-framework/community-archive --statConfig fstats_working/F3_finnish.config\n\nSo xerxes then automatically creates all combinations of populations listed in slots a, b and c.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are actually three types of F3-statistics supported by xerxes:\n\nF3vanilla: The purest form, defined literally as \\(\\langle(c−a)(c−b)\\rangle\\)\nF3: A bias-corrected version, which is only valid for groups in C that have non-zero heterozygosity\nF3star: This one is normalised by the heterozgygosity of the third population, C, as suggested in (Patterson et al. 2012) and implemented in the Admixtools package.\n\nThe white-paper explains this in detail.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe configuration file format has a lot more options. Here is a bit more complex example, but see also the documentation:\n# You can define groups right within the configuration file.\n# here we use negative selection to remove individuals from the\n# newly defined groups\ngroupDefs:\n  CEU2: [\"CEU.SG\", \"-&lt;NA12889.SG&gt;\", \"-&lt;NA12890.SG&gt;\"]\n  FIN2: [\"FIN.SG\", \"-&lt;HG00383.SG&gt;\", \"-&lt;HG00384.SG&gt;\"]\n  GBR2: [\"GBR.SG\", \"-&lt;HG01791.SG&gt;\", \"-&lt;HG02215.SG&gt;\"]\n  IBS2: [\"IBS.SG\", \"-&lt;HG02238.SG&gt;\", \"-&lt;HG02239.SG&gt;\"]\nfstats:\n- type: F2 # this will create 2x2 = 4 F2-Statistics\n  a: [\"French\", \"Spanish\"]\n  b: [\"Han\", \"CEU2\"]\n- type: F3vanilla # This will create 3x2x1 = 6 Statistics\n  a: [\"French\", \"Spanish\", \"Mbuti\"]\n  b: [\"Han\", \"CEU2\"]\n  c: [\"&lt;Chimp.REF&gt;\"]\n- type: F4 # This will create 5x5x4x1 = 100 Statistics\n  a: [\"&lt;I0156.SG&gt;\", \"&lt;I0157.SG&gt;\", \"&lt;I0159.SG&gt;\", \"&lt;I0160.SG&gt;\", \"&lt;I0161.SG&gt;\"]\n  b: [\"&lt;I0156.SG&gt;\", \"&lt;I0157.SG&gt;\", \"&lt;I0159.SG&gt;\", \"&lt;I0160.SG&gt;\", \"&lt;I0161.SG&gt;\"]\n  c: [\"CEU2\", \"FIN2\", \"GBR2\", \"IBS2\"]\n  d: [\"&lt;Chimp.REF&gt;\"]\n# Altogether: 110 statistics of different types\nwhich will not just create multiple statistic using row-combinations, as described, but also uses newly defined groups and combines multiple statistic types (F2, F3 and F4) in one run."
  },
  {
    "objectID": "fstats.html#f4-statistics",
    "href": "fstats.html#f4-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.2 F4 Statistics",
    "text": "3.2 F4 Statistics\nA different way to test for admixture is by “F4 statistics” (or “D statistics” which is very similar), also introduced in (Patterson et al. 2012).\nF4 statistics are also defined in terms of correlations of allele frequency differences, similarly to F3 statistics, but involving four different populations, not just three. Specifically we define\n\\[F4(A,B;C,D)=\\langle(a−b)(c−d)\\rangle.\\]\n\n3.2.1 Shaping intuition - the ABBA- and BABA-sites\nTo understand the statistics, consider the following tree:\n\nIn this tree, without any additional admixture, the allele frequency difference between A and B should be completely independent from the allele frequency difference between C and D. In that case, F4(A, B; C, D) should be zero, or at least not statistically different from zero. However, if there was gene flow from C or D into A or B, the statistic should be different from zero. Specifically, if the statistic is significantly negative, it implies gene flow between either C and B, or D and A. If it is significantly positive, it implies gene flow between A and C, or B and D.\nIt is helpful to again consider an example using a SNP list, this time assuming that every population is just a single (haploid) individual, so each allele frequency can just be 0 or 1. For example:\n\n\n\nSNP\nA\nB\nC\nD\n\\((a-b)(c-d)\\)\n\n\n\n\n1\n1\n0\n0\n0\n0\n\n\n2\n1\n0\n1\n1\n0\n\n\n3\n0\n1\n1\n0\n-1\n\n\n4\n0\n1\n0\n1\n1\n\n\n5\n1\n0\n0\n1\n-1\n\n\n6\n1\n0\n0\n0\n0\n\n\n\\(F_4(A, B;C, D)\\)\n\n\n\n\n-0.0167\n\n\n\nYou can see that the only SNPs that contribute positively to this statistics are SNPs where the alleles are distributed as 1010 and 0101, and the only SNPs that contribute negatively are 1001 and 0110. In the literature, the two patterns have been dubbed “ABBA” and “BABA”, which is why the statistical test behind this statistic (see below) was sometimes called the ABBA-BABA test (see for example (Martin, Davey, and Jiggins 2015)).\nThe intuition here is straight-forward: In positions that are polymorphic in both \\((A,B)\\) and \\((C,D)\\), this statistic asks whether B is genetically more similar to C than it is to D. This is most useful as a test for “treeness”: If A, B, C, D are related to each other as indicated in the above tree, then C should be equally closely related to C as to D. But if we actually find evidence that B is closer to C than to D, or vice versa, then this means that the tree above cannot be correct, but that there must be a closer connection between B and C or B and D, depending on the sign of the statistic.\n\n\n3.2.2 From single samples to allele frequencies\nSo the ABBA- and BABA-categories of SNPs help shape intuition for how this statistic behaves for single haploid genomes. But what about population allele frequencies? Looking back at the formula \\(\\langle(a−b)(c−d)\\rangle\\) this doesn’t help very much with intuition how this behaves with frequencies. Well, a nice feature of F4-Statistics is that averages factor out. This means, that if you have multiple samples in one or multiple slots A, B, C or D, the total F4-statistic of the groups is exactly equal to the average of F4-Statistics of the individuals. Here is a more mathematical definition.\nLet’s say we have 2 individuals in each of A and B, so we may perhaps write \\(A=\\{A_1,A_2\\}\\) and \\(B=\\{B_1,B_2\\}\\). Then one can show to have\n\\[F4(A, B; C, D) = \\text{Average of}[F4(A_1, B_1; C, D), F4(A_1, B_2; C, D), F4(A_2, B_1; C, D), F4(A_2, B_2; C, D)]\\]\nso just thte average over all individual-based F4-statistics. And this can be shown to be true for arbitrary numbers of samples. So in other words: An F4-Statistic always measures the average excess of pairwise BABA SNPs over ABBA SNPs. To me, this is a useful insight, as I find thinking in terms of ABBA-BABA somehow more helpful than thinking in terms of correlations of allele-frequency differences (which is really what the original formula is).\n\n\n\n\n\n\nNote\n\n\n\nF4-statistics have been famously used to show that Neanderthals are more closely related to Non-African populations than to Africans, suggesting gene-flow between Neanderthals and Non-Africans (shown in (Green et al. 2010)). You can reproduce this famous result with\n\nxerxes fstats -d ~/poseidon_repo --stat 'F4(&lt;Chimp.REF&gt;,&lt;Altai_published.DG&gt;,Yoruba,French)' \\\n  --stat 'F4(&lt;Chimp.REF&gt;,&lt;Altai_published.DG&gt;,Sardinian,French)'\n\nwhich shows that the first statistic is significantly positive with a Z-score of 7.99, while the second one is insignificantly different from zero (Z=1.01)\n\n\nThe way this statistic is often used, is to put a divergent outgroup as population A, for which we know for sure that there was no admixture into either C or D. With this setup, we can then test for gene flow between B and D (if the statistic is positive), or B and C (if it is negative).\n\n\n3.2.3 Running F4-Statistics with xerxes\nHere, we can use this statistic to test for East Asian admixture in Finns, similarly to the test using Admixture F3 statistics above. We will again use xerxes fstats. We again prepare a configuration file (in fstats_working/F4_finish.config in the git-repository of this book), this time with four populations (A, B, C, D):\nfstats:\n- type: F4\n  a: [\"Mbuti\"]\n  b: [\"Nganasan\", \"Russia_Bolshoy\"]\n  c: [\"French\", \"Icelandic\", \"Lithuanian\", \"Norwegian\"]\n  d: [\"Finnish\"]\nYou can again run via\n\nxerxes fstats -d ~/dev/poseidon-framework/community-archive --statConfig fstats_working/F4_finnish.config\n\nThe result is:\n.-----------.-------.----------------.------------.---------.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |   a   |       b        |     c      |    d    | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=======:================:============:=========:=========:================:====================:==================:====================:\n| F4        | Mbuti | Nganasan       | French     | Finnish | 593124  | 2.3114e-3      | 2.3115e-3          | 1.2676e-4        | 18.235604067907143 |\n| F4        | Mbuti | Nganasan       | Icelandic  | Finnish | 593124  | 1.6590e-3      | 1.6590e-3          | 1.4861e-4        | 11.163339072181776 |\n| F4        | Mbuti | Nganasan       | Lithuanian | Finnish | 593124  | 1.3290e-3      | 1.3290e-3          | 1.4681e-4        | 9.052979707622278  |\n| F4        | Mbuti | Nganasan       | Norwegian  | Finnish | 593124  | 1.6503e-3      | 1.6503e-3          | 1.5358e-4        | 10.745850997260929 |\n| F4        | Mbuti | Russia_Bolshoy | French     | Finnish | 542789  | 1.8785e-3      | 1.8785e-3          | 1.2646e-4        | 14.854487416366263 |\n| F4        | Mbuti | Russia_Bolshoy | Icelandic  | Finnish | 542789  | 1.0829e-3      | 1.0828e-3          | 1.4963e-4        | 7.236818881873822  |\n| F4        | Mbuti | Russia_Bolshoy | Lithuanian | Finnish | 542789  | 5.4902e-4      | 5.4907e-4          | 1.4601e-4        | 3.7605973064589096 |\n| F4        | Mbuti | Russia_Bolshoy | Norwegian  | Finnish | 542789  | 9.3473e-4      | 9.3475e-4          | 1.5302e-4        | 6.108881868125652  |\n'-----------'-------'----------------'------------'---------'---------'----------------'--------------------'------------------'--------------------'\nAs you can see, in all cases, the Z score is positive and larger than 3, indicating a significant deviation from zero, and implying gene flow between Nganasan and Finnish, and BolshoyOleniOstrov and Finnish, when compared to French, Icelandic, Lithuanian or Norwegian."
  },
  {
    "objectID": "fstats.html#outgroup-f3-statistics",
    "href": "fstats.html#outgroup-f3-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.3 Outgroup-F3-Statistics",
    "text": "3.3 Outgroup-F3-Statistics\nOutgroup F3 statistics are a special case how to use F3 statistics. The definition is the same as for Admixture F3 statistics, but instead of a target C and two source populations A and B, one now gives an outgroup C and two test populations A and B.\nTo get an intuition for this statistics, consider the following tree:\n\nIn this scenario, the statistic F3(A, B; C) measures the branch length from C to the common ancestor of A and B, coloured red. So this statistic is simply a measure of how closely two population A and B are related with each other, as measured from a distant outgroup. It is thus a similarity measure: The higher the statistic, the more genetically similar A and B are to one another.\nHere is again a SNP table to illustrate, using haploid individuals:\n\n\n\nSNP\nA\nB\nC\n\\((c-a)(c-b)\\)\n\n\n\n\n1\n1\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n\n\n3\n0\n0\n1\n1\n\n\n4\n1\n0\n1\n0\n\n\n5\n1\n1\n0\n1\n\n\n6\n0\n0\n1\n1\n\n\n\\(F_3(A, B;C)\\)\n\n\n\n0.5\n\n\n\nYou can see that each position which is similar between A and B, but different to C contributes 1, all other SNPs 0. So it directly measures similarity between A and B on alleles that differ from the outgroup C.\n\n\n\n\n\n\nNote\n\n\n\nNote that the averaging-relation shown for F4 statistics above is also true for Outgroup-F3 statistics, but only for populations A and B, not for C. So if you have multiple samples in A and B, you may think of this statistic being the average over all pairwise nucleotide similarities between individuals in A and B with respect to the same outgroup C.\n\n\nWe can use this statistic to measure for example the genetic affinity to East Asia, by performing the statistic F3(Han, X; Mbuti), where Mbuti is a distant African population and acts as outgroup here, Han denote Han Chinese, and X denotes various European populations that we want to test.\nYou can again define a configuration file that performs looping over various populations X for you:\nfstats:\n- type: F3\n  a: [\"Han\"]\n  b: [\"Chuvash\", \"Albanian\", \"Armenian\", \"Bulgarian\", \"Czech\", \"Druze\", \"English\",\n      \"Estonian\", \"Finnish\", \"French\", \"Georgian\", \"Greek\", \"Hungarian\", \"Icelandic\",\n      \"Italian_North\", \"Italian_South\", \"Lithuanian\", \"Maltese\", \"Mordovian\", \"Norwegian\",\n      \"Orcadian\", \"Russian\", \"Sardinian\", \"Scottish\", \"Sicilian\", \"Spanish_North\",\n      \"Spanish\", \"Ukrainian\", \"Finland_Levanluhta\", \"Russia_Bolshoy\", \"Russia_Chalmny_Varre\", \"Saami.DG\"]\n  c: [\"Mbuti\"]\nwhich cycles through many populations from Europe, including the ancient individuals from Chalmny Varre, Bolshoy Oleni Ostrov and Levänluhta (described in (Lamnidis et al. 2018)). We store this file in a file called fstats_working/OutgroupF3_europe.config and run via:\n\nxerxes fstats --statConfig fstats_working/OutgroupF3_europe.config -d ~/dev/poseidon-framework/community-archive -f fstats_working/outgroupf3_europe.tsv\n\n\n\n\n\n\n\nWarning\n\n\n\nOften in Outgroup-F3-statistics you use single genomes for population C, sometimes even single haploid genomes. In this case, F3 and F3star will get undefined results, because ordinary F3 and F3star statistics require population C to have non-zero average heterozygosity, so you will need at least one diploid sample, or multiple haploid or diploid samples.\nUse F3vanilla if your third population C is a single pseudo-haploid sample.\n\n\nHere is the output of this run (but note that a tab-separated version was also stored in fstats_working/outgroupf3_europe.tsv using the option -f):\n.-----------.-----.----------------------.-------.---.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |  a  |          b           |   c   | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=====:======================:=======:===:=========:================:====================:==================:====================:\n| F3        | Han | Chuvash              | Mbuti |   | 593124  | 5.3967e-2      | 5.3967e-2          | 5.0668e-4        | 106.51180329550319 |\n| F3        | Han | Albanian             | Mbuti |   | 593124  | 4.9972e-2      | 4.9973e-2          | 4.9520e-4        | 100.91326321202445 |\n| F3        | Han | Armenian             | Mbuti |   | 593124  | 4.9531e-2      | 4.9531e-2          | 4.7771e-4        | 103.68366652942314 |\n| F3        | Han | Bulgarian            | Mbuti |   | 593124  | 5.0103e-2      | 5.0103e-2          | 4.8624e-4        | 103.04188532686614 |\n| F3        | Han | Czech                | Mbuti |   | 593124  | 5.0536e-2      | 5.0536e-2          | 4.9261e-4        | 102.58792370749681 |\n| F3        | Han | Druze                | Mbuti |   | 593124  | 4.8564e-2      | 4.8564e-2          | 4.6788e-4        | 103.79674299622445 |\n| F3        | Han | English              | Mbuti |   | 593124  | 5.0280e-2      | 5.0281e-2          | 4.9183e-4        | 102.23198323949656 |\n| F3        | Han | Estonian             | Mbuti |   | 593124  | 5.1154e-2      | 5.1155e-2          | 5.0350e-4        | 101.59882496016485 |\n| F3        | Han | Finnish              | Mbuti |   | 593124  | 5.1784e-2      | 5.1784e-2          | 5.0603e-4        | 102.33488758899031 |\n| F3        | Han | French               | Mbuti |   | 593124  | 5.0207e-2      | 5.0208e-2          | 4.8552e-4        | 103.40976592749682 |\n| F3        | Han | Georgian             | Mbuti |   | 593124  | 4.9711e-2      | 4.9711e-2          | 4.8100e-4        | 103.34881140790415 |\n| F3        | Han | Greek                | Mbuti |   | 593124  | 4.9874e-2      | 4.9874e-2          | 4.8994e-4        | 101.79554640756365 |\n| F3        | Han | Hungarian            | Mbuti |   | 593124  | 5.0497e-2      | 5.0498e-2          | 4.9878e-4        | 101.24215699276706 |\n| F3        | Han | Icelandic            | Mbuti |   | 593124  | 5.0680e-2      | 5.0680e-2          | 4.9729e-4        | 101.91303336514295 |\n| F3        | Han | Italian_North        | Mbuti |   | 593124  | 4.9903e-2      | 4.9904e-2          | 4.8436e-4        | 103.03094306099203 |\n| F3        | Han | Italian_South        | Mbuti |   | 592980  | 4.9201e-2      | 4.9201e-2          | 5.1170e-4        | 96.15239597674244  |\n| F3        | Han | Lithuanian           | Mbuti |   | 593124  | 5.0896e-2      | 5.0896e-2          | 5.0638e-4        | 100.50984037418753 |\n| F3        | Han | Maltese              | Mbuti |   | 593124  | 4.8751e-2      | 4.8751e-2          | 4.7500e-4        | 102.63442479673623 |\n| F3        | Han | Mordovian            | Mbuti |   | 593124  | 5.1820e-2      | 5.1820e-2          | 4.8853e-4        | 106.07409963190884 |\n| F3        | Han | Norwegian            | Mbuti |   | 593124  | 5.0724e-2      | 5.0724e-2          | 4.9514e-4        | 102.4454387098217  |\n| F3        | Han | Orcadian             | Mbuti |   | 593124  | 5.0469e-2      | 5.0469e-2          | 4.9485e-4        | 101.98814656611475 |\n| F3        | Han | Russian              | Mbuti |   | 593124  | 5.1277e-2      | 5.1277e-2          | 4.8613e-4        | 105.48070801791317 |\n| F3        | Han | Sardinian            | Mbuti |   | 593124  | 4.9416e-2      | 4.9417e-2          | 4.8908e-4        | 101.04049389691913 |\n| F3        | Han | Scottish             | Mbuti |   | 593124  | 5.0635e-2      | 5.0635e-2          | 5.0565e-4        | 100.13962104744425 |\n| F3        | Han | Sicilian             | Mbuti |   | 593124  | 4.9194e-2      | 4.9194e-2          | 4.8157e-4        | 102.15353663091187 |\n| F3        | Han | Spanish_North        | Mbuti |   | 593124  | 5.0032e-2      | 5.0032e-2          | 4.9377e-4        | 101.32594226555439 |\n| F3        | Han | Spanish              | Mbuti |   | 593124  | 4.9693e-2      | 4.9693e-2          | 4.8551e-4        | 102.35200847948641 |\n| F3        | Han | Ukrainian            | Mbuti |   | 593124  | 5.0731e-2      | 5.0731e-2          | 4.9506e-4        | 102.47529111692852 |\n| F3        | Han | Finland_Levanluhta   | Mbuti |   | 303033  | 5.4488e-2      | 5.4488e-2          | 5.7681e-4        | 94.46487653920919  |\n| F3        | Han | Russia_Bolshoy       | Mbuti |   | 542789  | 5.7273e-2      | 5.7273e-2          | 5.2875e-4        | 108.31739594898687 |\n| F3        | Han | Russia_Chalmny_Varre | Mbuti |   | 428215  | 5.4000e-2      | 5.4000e-2          | 5.6936e-4        | 94.84371082564112  |\n| F3        | Han | Saami.DG             | Mbuti |   | 585193  | 5.4727e-2      | 5.4728e-2          | 5.5546e-4        | 98.5265149143263   |\n'-----------'-----'----------------------'-------'---'---------'----------------'--------------------'------------------'--------------------'\nNow it’s time to plot these results using R. Let’s first read in the table:\n\nd &lt;- read.csv(\"fstats_working/outgroupf3_europe.tsv\", sep = \"\\t\")\n\nWe can check that it worked:\n\nhead(d)\n\n  Statistic   a         b     c  d NrSites Estimate_Total Estimate_Jackknife\n1        F3 Han   Chuvash Mbuti NA  593124       0.053967           0.053967\n2        F3 Han  Albanian Mbuti NA  593124       0.049972           0.049973\n3        F3 Han  Armenian Mbuti NA  593124       0.049531           0.049531\n4        F3 Han Bulgarian Mbuti NA  593124       0.050103           0.050103\n5        F3 Han     Czech Mbuti NA  593124       0.050536           0.050536\n6        F3 Han     Druze Mbuti NA  593124       0.048564           0.048564\n  StdErr_Jackknife Z_score_Jackknife\n1       0.00050668          106.5118\n2       0.00049520          100.9133\n3       0.00047771          103.6837\n4       0.00048624          103.0419\n5       0.00049261          102.5879\n6       0.00046788          103.7967\n\n\nNice, now on to plotting (here I’m using Base R for zero-dependency pain, you’re welcome!):\n\norder &lt;- order(d$Estimate_Total) # order the estimates for visual effect\nx &lt;- d$Estimate_Jackknife[order]\nxErr &lt;- d$StdErr_Jackknife[order]\ny &lt;- seq_along(d$b)\nplot(x, y, xlab = \"Z Score\", ylab = NA, yaxt = \"n\", # plot no y-axis ticks\n     xlim = c(0.048,0.06),\n     main = \"F3(Han, X; Mbuti)\")\n# plot the labels\ntext(x + 0.001, y, labels = d$b, adj=0)\n# plot the error bars\narrows(x - xErr, y, x + xErr, y, length=0.05, angle=90, code=3)\n\n\n\n\n\n\n\n\nAs expected, the ancient samples and modern Saami are the ones with the highest allele sharing with present-day East Asians (as represented by Han) compared to many other Europeans.\n\n\n\n\nGreen, Richard E, Johannes Krause, Adrian W Briggs, Tomislav Maricic, Udo Stenzel, Martin Kircher, Nick Patterson, et al. 2010. “A Draft Sequence of the Neandertal Genome.” Science 328 (5979): 710–22. http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=20448178&retmode=ref&cmd=prlinks.\n\n\nLamnidis, Thiseas C, Kerttu Majander, Choongwon Jeong, Elina Salmela, Anna Wessman, Vyacheslav Moiseyev, Valery Khartanovich, et al. 2018. “Ancient Fennoscandian Genomes Reveal Origin and Spread of Siberian Ancestry in Europe.” Nature Communications 9 (1): 5018. https://doi.org/10.1038/s41467-018-07483-5.\n\n\nMartin, Simon H, John W Davey, and Chris D Jiggins. 2015. “Evaluating the Use of ABBA-BABA Statistics to Locate Introgressed Loci.” Molecular Biology and Evolution 32 (1): 244–57. https://doi.org/10.1093/molbev/msu269.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\n\n\nPeter, Benjamin M. 2016. “Admixture, Population Structure, and F-Statistics.” Genetics 202 (4): 1485–1501. https://doi.org/10.1534/genetics.115.183913."
  },
  {
    "objectID": "eager.html#why-do-we-need-nf-coreeager",
    "href": "eager.html#why-do-we-need-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.1 Why do we need nf-core/eager?",
    "text": "4.1 Why do we need nf-core/eager?\n\nCompared to other Next-Generation Sequencing data, the chemical structure and increased risk of present-day contamination require methods specialized for ancient DNA in both the wet and the dry lab.\nEnsuring the authenticity of ancient genomic data is one of the main focuses of bioinformatic tools developed for the study of ancient DNA and underlines the necessity of reproducibility of results.\nWith an increasing number of laboratories contributing to the field, the available computational resources and previous bioinformatic experience varies greatly. To increase accessibility, newly developed tools should be adaptable to different environments, efficient, consistently maintained and well-documented."
  },
  {
    "objectID": "eager.html#what-can-nf-coreeager-do",
    "href": "eager.html#what-can-nf-coreeager-do",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.2 What can nf-core/eager do?",
    "text": "4.2 What can nf-core/eager do?\nnf-core/eager streamlines the initial steps of ancient DNA analysis from FASTQ files after sequencing to variant calling (Fellows Yates et al. 2021).\n\nPreprocessing:\n\nFastQC (sequencing quality control)\nAdapterRemoval2/fastp (sequencing artifact clean-up)\n\nMapping:\n\nBWA aln/BWA mem/CircularMapper/Bowtie2 (alignment)\nSAMtools (mapping quality filtering)\nPicard MarkDuplicates/DeDup (PCR duplicate removal)\nSAMtools/PreSeq/Qualimap2/BEDtools/Sex.DetERRmine/EndorSpy/MtNucRatio (mapping statistics)\n\naDNA evaluation:\n\nDamageProfiler/mapDamage2 (damage assessment)\nPMDtools (aDNA read selection)\nmapDamage2/Bamutils (damage removal)\nANGSD (human contamination estimation)\nBBduk/HOPS/Kraken & Kraken Parse/MALT & MaltExtract (metagenomic screening)\n\nVariant calling: GATK UnifiedGenotyper & HaplotypeCaller/sequenceTools pileupCaller/VCF2Genome/MultiVCFAnalyzer/freebayes/ANGSD\nReport generation: MultiQC (summarize all generated statistics)"
  },
  {
    "objectID": "eager.html#how-do-i-use-nf-coreeager",
    "href": "eager.html#how-do-i-use-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.3 How do I use nf-core/eager?",
    "text": "4.3 How do I use nf-core/eager?\n\n4.3.1 Installation\nYou need:\n\na Unix machine (HPC-cluster or a computer running Linux or MacOS)\nan installation of docker or apptainer (formerly known as singularity) or conda, java and nextflow (e.g. via conda)\ninternet connection\n\nDownload the latest version of nf-core/eager\nnextflow pull nf-core/eager \n# for a specific version\nnextflow pull nf-core/eager -r 2.5.0\nRun a test specifying your choice of conda, docker or singularity\nnextflow run nf-core/eager -r 2.5.0 -profile test_tsv,docker\nTo optimize the use of available clusters, queues and resources, check if a Nextflow pipeline configuration is already available for your institution or computing environment. If not, prepare a custom profile tailored to your computational resources and setup. These are then added as a profile.\nnextflow run nf-core/eager -r 2.5.0 -profile test_tsv,eva #for MPI-EVA\n\n\n4.3.2 Input preparation\nYou can run nf-core/eager by providing either a path to fastq or bam files or a path to a tab-separated table of input data to --input. For a large number of samples and convenience, a tsv is usually the preferred option. Using a tsv input also allows for merging of different files (e.g. different libraries, different UDG treatments, etc.) at different stages of the pipeline.\n\n\n\nPipeline stages and merging steps performed for a single sample with different libraries and different UDG treatments.\n\n\nA tsv input file contains the following columns, detailing the name of the sample, library, sequencing lane, colour chemistry depending on the sequencer used, target organism, library strandedness, UDG treatment, path to fastq with forward reads (SE and PE), path to reverse reads (only PE), path to bam (optional). nf-core/eager will treat the data according to the provided information, e.g. only trim UDG half data and genotype single-stranded libraries using single-stranded mode.\nSample_Name Library_ID Lane Colour_Chemistry SeqType Organism Strandedness UDG_Treatment R1                                                                                                                                  R2                                                                                                                                  BAM\nJK2782      JK2782     1    4                PE      Mammoth  single       half          https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2782_TGGCCGATCAACGA_L008_R1_001.fastq.gz.tengrand.fq.gz https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2782_TGGCCGATCAACGA_L008_R2_001.fastq.gz.tengrand.fq.gz NA\nJK2802      JK2802     2    2                SE      Mammoth  double       full          NA                                                                                                                                  NA                                                                                                                                  https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2802_AGAATAACCTACCA_L008_R1_001.fastq.gz.tengrand.bam\nCollecting and double-checking this information is time consuming, but crucial!\nIf you realize you have different libraries from same individual, you should enter the same Sample_Name for all respective libraries. nf-core/eager will then produce all steps for the independent libraries (e.g. endogenous DNA, sequencing quality control, contamination estimation, etc.), but merge the deduplicated bam files before genotyping, genetic sex estimation and coverage calculation. To avoid re-mapping the whole dataset and conserve computing resources, also consider providing the mapped bam files to nf-core/eager directly.\n\n\n\n\n\n\nTip\n\n\n\nAt DAG, we can take advantage of all the information entered in Pandora to produce a eager-ready tsv with pandora2eager.\n\n\n\n\n4.3.3 Parameter customization\nBy default nf-core/eager runs the following, when you only provide input data and a reference genome:\nnextflow run nf-core/eager --input &lt;INPUT&gt;.tsv --fasta '&lt;REFERENCE&gt;.fasta' -profile eva\n\nPreprocessing:\n\nFastQC (sequencing quality control)\nAdapterRemoval2 (sequencing artifact clean-up)\n\nMapping:\n\nBWA aln (alignment)\nPicard MarkDuplicates (PCR duplicate removal)\nSAMtools/PreSeq/Qualimap2/EndorSpy (mapping statistics)\n\naDNA evaluation: DamageProfiler (damage assessment)\nReport generation: MultiQC (summarize all generated statistics)\n\nThe most direct way to add analysis steps (e.g. turn on genotyping) or change settings (e.g. shorter read length cut-off) is to add more parameters to the command line, in this case --run_genotyping --genotyping_tool pileupcaller and --clip_readlength 25, respectively. However, this gets cumbersome for the rather extensive workflows we usually employ for human aDNA analysis, including read trimming based on UDG treatment, genetic sex estimation, human nuclear contamination estimation, mitochondrial to nuclear ratio estimation and genotyping.\nBut the power of nf-core/eager lies in its adaptability to your specific analysis needs and the possibility to ‘remember’ your favorite settings with a personal configuration file. This separate file can contain all parameters for your required tools, as well as custom computational resource requests. For 1240K capture data, a profile mapping to the hs37d5 reference genome with genotyping could look like this:\nprofiles{\n  TF_hs37 { #name of the profile\n    params {\n        config_profile_description = \"human 1240K data hs37d5 + genotyping\"\n        config_profile_contact = \"Selina Carlhoff (@scarlhoff)\"\n        email = \"selina_carlhoff@eva.mpg.de\"\n        snpcapture_bed = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        fasta = \"/PATH/hs37d5/hs37d5.fa\"\n        fasta_index = \"/PATH/hs37d5/hs37d5.fa.fai\"\n        bwa_index = \"/PATH/hs37d5/\"\n        skip_preseq = true\n        clip_readlength = 30\n        preserve5p = true\n        bwaalnn = 0.01\n        bwaalnl = 16500\n        run_bam_filtering = true\n        bam_mapping_quality_threshold = 30\n        bam_filter_minreadlength = 30\n        bam_unmapped_type = \"discard\"\n        run_trim_bam = true\n        bamutils_clip_double_stranded_half_udg_left = 2\n        bamutils_clip_double_stranded_half_udg_right = 2\n        bamutils_clip_single_stranded_none_udg_left = 0\n        bamutils_clip_single_stranded_none_udg_right = 0\n        run_genotyping = true\n        genotyping_tool = \"pileupcaller\"\n        genotyping_source = \"trimmed\"\n        pileupcaller_bedfile = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        pileupcaller_snpfile = \"/PATH/1240K.snp\"\n        run_mtnucratio = true\n        mtnucratio_header = \"MT\"\n        run_sexdeterrmine = true\n        sexdeterrmine_bedfile = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        run_nuclear_contamination = true\n        contamination_chrom_name = \"X\"\n    }\n    process {\n    maxRetries = 2\n        withName:bwa {\n        time = { task.attempt == 3 ? 1440.h : task.attempt == 2 ? 72.h : 48.h }\n        }\n        withName:markduplicates {\n        memory = { task.attempt == 3 ? 16.GB : task.attempt == 2 ? 8.GB : 4.GB }\n        }\n        withName: mtnucratio {\n            memory = '10.G'\n            time = '24.h'\n        }\n    }\n  }\n}\nThe configuration file is then provided to nf-core/eager via the -profile and -c flag.\nnextflow run nf-core/eager -–input &lt;INPUT&gt;.tsv -profile TF_hs37,eva,archgen -c /&lt;PATH&gt;/eager2.config\nFull documentation of all parameters is available on the nf-core/eager website.\n\n\n\n\n\n\nTip\n\n\n\nThe standardised parameters for the DAG automated pipeline can be found at /mnt/archgen/Autorun_eager/conf/Autorun.config.\n\n\n\n\n4.3.4 Run submission\nOnce all input files and parameters are prepared, you are ready for submission. To make sure that the workflow continues running when you disconnect from the cluster or shut down your computer, nf-core/eager should be run in a screen session.\n# create a screen session\nscreen -R eager\n# submit nf-core/eager run\nnextflow run nf-core/eager –input &lt;INPUT&gt;.tsv -profile &lt;YOUR_PROFILE&gt; -c /&lt;PATH&gt;/&lt;YOUR_CONFIG&gt;.config\n# disconnect from screen session by pressing Ctrl+A+D\n# reconnect to screen session\nscreen -r eager\n# end screen session after successful pipeline execution\nexit\nAfter submitting a command specifying all the parameters you would like to use, Nextflow generates the corresponding shell scripts and submits each job to your scheduler according to your requested computational resources. You can track the execution status live in the terminal or on Nextflow Tower. For use with tower, you should assign the run an identifiable name with -name and activate tracking using -with-tower.\n\n\n4.3.5 Output\nDuring the progression of the run, the results of each pipeline steps are collected in separate output directories. These contain the raw outputs of each tool, including any generated files (e.g. deduplicated bam files or genotypes).\n&lt;RUNNAME&gt;/\n- results/\n  - adapterremoval/\n  - damageprofiler/\n  - deduplication/\n  - documentation/\n  - endorspy/\n  - fastqc/\n  - genotyping/\n  - lanemerging/\n  - mapping/\n  - merged_bams/\n  - multiqc/\n  - nuclear_contamination/\n  - pipeline_info/\n  - qualimap/\n  - reference_genome/\n  - samtools/\n  - sex_determination/\n  - trimmed_bam/\n- work/\nBut as a first overview, we want to look at the summary of all statistics aggregated in multiqc/multiqc_report.html.\n\n\n\nScreenshot of the top section of a MultiQC report\n\n\nThis table collects the output from all tools, so you can get an overview of sequenced reads per sequencing run, endogenous DNA per library, covered SNPs per sample and much more. You can also inspect and export crucial plots, such as read length distribution and damage profile. The end of the report also contains a list of all software versions and an overview of which profiles were used.\n\n\n\nAncient DNA damage plot as generated by MultiQC\n\n\n\n\n4.3.6 Trouble shooting\nA common issue with nf-core/eager, especially when used in combination with the SGE scheduler, are memory issues with java-driven tools, e.g. MarkDuplicates. Sometimes the pipeline does not catch cases properly, where the allocated memory is exceeded, and the job keeps running instead of being re-submitted with larger memory allocation. Therefore, if you notice jobs running much longer than expected, it is worth checking the work/ directory, where all information about each submitted job is recorded. Each job is assigned a randomly generated name using numbers and letter which you can identify from the log printed in to your screen, the &lt;RUNNAME&gt;/.nextflow.log or Nextflow tower. Each work directory contains files tracing the execution of the job.\n&lt;RUNNAME&gt;/\n- work/\n  - &lt;WORKDIRECTORY&gt;/\n    - .command.sh # exact command run for this tool\n    - .command.run # exact command submitted to the scheduler\n    - .command.log # any messages during the execution\n    - .command.err # any error messages\n    - .command.out # any output messages\n    - .command.trace # assigned computational resources\nIf you spot java.lang.OutOfMemoryError: unable to create new native thread in .command.log or command.err, you can delete the individual job from the scheduling queue. It will be re-submitted automatically with larger memory allocation.\nIf you’ve had an issue with a run or want to restart the pipeline, you can do so using -resume. Nextflow will used cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. You can also supply a run name to resume a specific run: -resume [run-name]. Use the nextflow log command to show previous run names."
  },
  {
    "objectID": "eager.html#how-do-i-report-the-usage-of-nf-coreeager",
    "href": "eager.html#how-do-i-report-the-usage-of-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.4 How do I report the usage of nf-core/eager?",
    "text": "4.4 How do I report the usage of nf-core/eager?\nIf you use nf-core/eager for your analysis, please cite Fellows Yates et al. (2021) and the release of nf-core/eager on zenodo, as well as the nf-core publication (Ewels et al. 2020). As nf-core/eager is only the pipeline connecting multiple tools, please also cite the version of each used tool, the respective individual publications and add the full command for easy reproducibility.\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\nEwels, Philip A., Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso, and Sven Nahnsen. 2020. “The Nf-Core Framework for Community-Curated Bioinformatics Pipelines.” Nature Biotechnology 38 (3): 276–78. https://doi.org/10.1038/s41587-020-0439-x.\nnextflow run nf-core/eager\n        -profile eva,archgen\n        -r 2.4.0\n        --input &lt;INPUT&gt;.tsv\n        --min_adap_overlap 1\n        --clip_readlength 30\n        --clip_min_read_quality 20\n        --preserve5p\n        --mapper bwaaln\n        --bwaalnnn 0.01\n        --bwaalno 2\n        --run_bam_filtering true\n        --bam_mapping_quality_threshold 30\n        --bam_filter_minreadlength 30\n        --bam_unmapped_type discard\n        --dedupper markduplicates\n        --damageprofiler_length 100\n        --damageprofiler_threshold 15\n        --damageprofiler_yaxis 0.3"
  },
  {
    "objectID": "eager.html#how-do-i-update-nf-coreeager",
    "href": "eager.html#how-do-i-update-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.5 How do I update nf-core/eager?",
    "text": "4.5 How do I update nf-core/eager?\nnextflow pull nf-core/eager\n#or for a specific version\nnextflow pull nf-core/eager -r 2.5.0"
  },
  {
    "objectID": "eager.html#whats-next-for-nf-coreeager",
    "href": "eager.html#whats-next-for-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.6 What’s next for nf-core/eager?",
    "text": "4.6 What’s next for nf-core/eager?\nWhile nf-core/eager 2.5.0 has only been released recently, behind the scenes the development team has been very busy re-writing nf-core/eager to be more efficient and include even more functionality. So look out for nf-core/eager 3.0 release sometime soon!\n\n\n\nOverview of the development status of nf-core/eager 3.0 as of October 2023, new functionality marked in purple."
  },
  {
    "objectID": "eager.html#how-can-i-get-help-with-problems-or-questions",
    "href": "eager.html#how-can-i-get-help-with-problems-or-questions",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.7 How can I get help with problems or questions?",
    "text": "4.7 How can I get help with problems or questions?\nCheck the website for in-depth documentation of nf-core/eager.\n Raise your issue on GitHub.\nJoin the #eager channel on the nf-core Slack."
  },
  {
    "objectID": "fst.html#theory-primer",
    "href": "fst.html#theory-primer",
    "title": "5  Measuring population structure using Fst",
    "section": "5.1 Theory primer",
    "text": "5.1 Theory primer\n\\(F_\\text{ST}\\) is a measure for how differentiated two populations are, taking into account internal genetic variation. It sometimes called the “Fixation Index”. It is in spirit very similar to \\(F_2\\), in that it becomes 0 if two populations are not differentiated at all (meaning they are effectively the same), and the more differentiated, the more positive is the measure. However, \\(F_\\text{ST}\\) is scaled very differently from \\(F_2\\).\nWhile there are various mathematical definitions for both the theoretical definition and estimation for \\(F_\\text{ST}\\), which differ in subtle ways, we here follow the excellent paper by (Bhatia et al. 2013), which proposes the following estimator, termed Hudson-estimator:\n\\[F_\\text{ST}=1-\\frac{H_w}{H_b}\\]\nHere, \\(H_w\\) is the average heterozygosity within each population, and \\(H_b\\) is the average heterozygosity between two populations. We can easily read off the two boundaries of the definition: At the lower end, we have \\(F_\\text{ST}=0\\) if and only if \\(H_w=H_b\\), so there is no difference between heterozygosity measured within or between groups, which is equivalent to saying that the two populations are the same. On the upper end we have \\(F_\\text{ST}=1\\) if and only if \\(H_w=0\\), so all observed variants are fully fixed in both populations (but not necessarily different between the populations).\n\n\n\n\n\n\nNote\n\n\n\n(Bhatia et al. 2013) also give a more formal evolutionary definition of \\(F_\\text{ST}\\), developed in turn by (Weir and Hill 2002), in terms of covariance between derived and ancestral populations. Specifically, for a given SNP, the definition involves the conditional probability of allele frequency \\(p_i\\) in population \\(i\\), given an ancestral allele frequency \\(p_\\text{anc}\\), which is defined as a random process with the expectation\n\\[E(p_i|p_\\text{anc}) = p_\\text{anc}\\]\nand variance \\[Var(p_i|p_\\text{anc}) = F_\\text{ST}^i p_\\text{anc}(1-p_\\text{anc}).\\]\nThis form of the conditional variance can be understood by analysing the equation for the two boundary cases: For \\(F_\\text{ST}^i=0\\), there is no variance, so the conditional probability of the derived frequency will be completely determined by the ancestral frequency with no random change. In contrast \\(F_\\text{ST}^i=1\\) means that the variance in the derived allele frequency is that of a binomial distribution with variance \\(p_\\text{anc}(1-p_\\text{anc}\\), indicating random but complete fixation of the frequency to 0 or 1.\n\\(F_\\text{ST}\\) between two populations A and B is then defined as \\[F_\\text{ST}(A,B) = \\frac{F_\\text{ST}^A+F_\\text{ST}^B}{2}\\],\nso the average of the two drift processes.\n\n\n\nWeir, B S, and W G Hill. 2002. “Estimating f-Statistics.” Annual Review of Genetics 36: 721–50. https://doi.org/10.1146/annurev.genet.36.050802.093940.\n\nBhatia, G, N Patterson, S Sankararaman, and A L Price. 2013. “Estimating and Interpreting FST: The Impact of Rare Variants.” Genome Research 23 (9): 1514–21. http://genome.cshlp.org/cgi/doi/10.1101/gr.154831.113.\nIt is fairly straight forward to see (and shown in (Bhatia et al. 2013)) that the Hudson-estimator above can be recast as\n\\[F_\\text{ST}(A,B)=\\frac{(a-b)^2}{a(1-b)+b(1-a)}\\]\nHere, \\(a\\) and \\(b\\) denote population allele frequencies, which are in principle unobserved, but can be approximated by sample allele frequencies. This approximation is biased, and (Patterson et al. 2012) gives additional formulae for an (asympotically) unbiased estimator.\nFrom this definition, you can see that \\(F_\\text{ST}(A,B)\\) is closely related to F2-statistics, introduced in (Patterson et al. 2012):\n\\[F_2(A,B)=(a-b)^2\\].\nIn some sense, \\(F_\\text{ST}(A,B)\\) can be considered a normalised version of \\(F_2(A,B)\\). While both statistics range mathematically from 0 to 1, the upper bound 1 has very different meanings in both. A theoretical value of \\(F_2=1\\) would mean that both populations are fixed at different alleles in all studied SNPs, which is practically not possible (even completely random fixations would suggest that 1/4 of them would agree given that there are only four nucleotides, let alone the fact that such deeply diverged populations/species would not be alignable anymore). One can say that the time-scale on which \\(F_2\\) approaches 1, for non-ascertained SNPs, so the entire genome, is the time scale of nucleotide substitutions (i.e. mutations plus fixation) along species branches, which in neutral evolution is given by the inverse mutation rate \\(1/\\mu\\). This would mean something on the order of \\(10^8\\) generations, which is arguably as deep as the tree of live itself. In contrast, \\(F_\\text{ST}\\) approaches 1 on the time-scale of fixation of standing variation, which is \\(2N\\) generations, which for humans is on the order of 10000 generations, so around the depth of modern-human diversity from its origins in Africa several hundred thousand years ago. Arguably, this time scale is much more useful for data analyses and thus easier to interpret.\nOf course, in practice, one uses some ascertained SNP set, as also here in our examples below, in which case values are much higher because we consider only variants that are segregating in human populations within a relatively high allele frequency.\nIf you’ve gone through our chapter on F3 and F4 statistics, you will have already encountered our software xerxes. You can compute both the biased and the approximately unbiased estimators for \\(F_\\text{ST}(A,B)\\), using the FST or FSTvanilla statistics, as defined in the whitepaper.\nFor what follows, we will use the approximately unbiased form FST.\n\\(F_\\text{ST}(A,B)\\) has a convenient and untuitive scale: It ranges from 0 to 1, where \\(F_\\text{ST}(A,B)=0\\) denotes that \\(A\\) and \\(B\\) are the same population, with no differentiation whatsoever. On the other hand of the spectrum we have \\(F_\\text{ST}(A,B)=1\\), which would mean that two populations are fully separated.\nAnother way to see this measure is to consider it as relative shared variance: If you consider genetic variation between \\(A\\) and \\(B\\), and within each of \\(A\\) and \\(B\\), then \\(F_\\text{ST}(A,B)\\) can be considered to measure the average variance between populations relative to the average variance within populations, again with intuitive boundaries 0 and 1."
  },
  {
    "objectID": "fst.html#computing-fst-using-xerxes",
    "href": "fst.html#computing-fst-using-xerxes",
    "title": "5  Measuring population structure using Fst",
    "section": "5.2 Computing FST using xerxes",
    "text": "5.2 Computing FST using xerxes\nFor human present-day populations, we can compute pairwise FSt using xerxes.\nWe here chose a number of populations from (Patterson et al. 2012) with more than 10 samples per population, and prepare the following config file for xerxes:\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\nfstats:\n- type: FST\n  a: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n  b: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n- type: F2\n  a: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n  b: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\nThis will then produce all combinations of \\(FST(A, B)\\) and \\(F_2(A, B)\\) as indicated in the population lists.\n\n\n\n\n\n\nNote\n\n\n\nNote that the config-file engine in xerxes always computes all the combinations of populations, even for cases of \\(A=B\\). It also doesn’t know about symmetry, so will happily compute the redundant statistics \\(FST(\\text{Adygei}, \\text{Adygei})\\) and \\(FST(\\text{Adygei}, \\text{Adygei})\\). While this could be possibly improved, there is no big harm done, as this runs fairly quickly.\n\n\nWe run this config file using the command line\nREPO=/path/to/community-archive/2012_PattersonGenetics\n\nxerxes fstats -d $REPO --statConfig fstat_world_config.yaml -f fstat_world_output.tsv &gt; fstat_world_table.txt\nThe standard output, is a nicely layouted ASCII Table, which looks like this in the beginning:\n.-----------.-----------------.-----------------.---.---.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |        a        |        b        | c | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=================:=================:===:===:=========:================:====================:==================:====================:\n| FST       | Adygei          | Adygei          |   |   | 593124  | 0.0000         | 0.0000             | 0.0000           | NaN                |\n| FST       | Adygei          | Balochi         |   |   | 593124  | 1.2789e-2      | 1.2789e-2          | 3.3572e-4        | 38.09517110646904  |\n| FST       | Adygei          | Basque          |   |   | 593124  | 1.8790e-2      | 1.8790e-2          | 4.0141e-4        | 46.810358341103225 |\n| FST       | Adygei          | BedouinA        |   |   | 593124  | 1.3017e-2      | 1.3017e-2          | 2.9647e-4        | 43.90737238689979  |\n| FST       | Adygei          | BedouinB        |   |   | 593124  | 3.3455e-2      | 3.3454e-2          | 5.7648e-4        | 58.03217592610529  |\n| FST       | Adygei          | Biaka           |   |   | 593124  | 0.1716         | 0.1716             | 1.2185e-3        | 140.85275693678508 |\n| FST       | Adygei          | Brahui          |   |   | 593124  | 1.4644e-2      | 1.4644e-2          | 3.4481e-4        | 42.46989237781921  |\n| FST       | Adygei          | Burusho         |   |   | 593124  | 1.8566e-2      | 1.8566e-2          | 3.8156e-4        | 48.6573908240317   |\n| FST       | Adygei          | Druze           |   |   | 593124  | 1.2173e-2      | 1.2173e-2          | 2.6659e-4        | 45.65975464203526  |\n| FST       | Adygei          | French          |   |   | 593124  | 9.7730e-3      | 9.7730e-3          | 3.1627e-4        | 30.9006924987833   |\n| FST       | Adygei          | Han             |   |   | 593124  | 9.8759e-2      | 9.8759e-2          | 1.1973e-3        | 82.48660429503893  |\n| FST       | Adygei          | Hazara          |   |   | 593124  | 3.0725e-2      | 3.0726e-2          | 7.1478e-4        | 42.98629834431124  |\n| FST       | Adygei          | Italian_North   |   |   | 593124  | 8.6600e-3      | 8.6601e-3          | 2.7883e-4        | 31.058813893781032 |\n\nbut of course has many more lines (&gt;1800 in this case). We also used the -f flag to output a tab-separated file, here named fstat_world_output.tsv, which is easier to read into R."
  },
  {
    "objectID": "fst.html#plotting-results-in-r",
    "href": "fst.html#plotting-results-in-r",
    "title": "5  Measuring population structure using Fst",
    "section": "5.3 Plotting results in R",
    "text": "5.3 Plotting results in R\nAll of the following code uses strictly only base-R for maximum compatibility. The code should run on any R installation.\nWe first load the data\n\ndat &lt;- dat &lt;- subset(read.table(\"fst_working/fstat_world_output.tsv\", sep=\"\\t\", header = TRUE),\n                     select=-c(c, d, Z_score_Jackknife))\ndatFST &lt;- dat[dat$Statistic == \"FST\",]\ndatF2 &lt;- dat[dat$Statistic == \"F2\",]\nhead(datFST)\n\n  Statistic      a        b NrSites Estimate_Total Estimate_Jackknife\n1       FST Adygei   Adygei  593124       0.000000           0.000000\n2       FST Adygei  Balochi  593124       0.012789           0.012789\n3       FST Adygei   Basque  593124       0.018790           0.018790\n4       FST Adygei BedouinA  593124       0.013017           0.013017\n5       FST Adygei BedouinB  593124       0.033455           0.033454\n6       FST Adygei    Biaka  593124       0.171600           0.171600\n  StdErr_Jackknife\n1       0.00000000\n2       0.00033572\n3       0.00040141\n4       0.00029647\n5       0.00057648\n6       0.00121850\n\n\nOk, this looks good. Let’s check out the largest values\n\nhead(dat[order(-dat$Estimate_Total),])\n\n    Statistic         a         b NrSites Estimate_Total Estimate_Jackknife\n166       FST     Biaka Karitiana  593124         0.3021             0.3021\n456       FST Karitiana     Biaka  593124         0.3021             0.3021\n473       FST Karitiana    Papuan  593124         0.3011             0.3011\n676       FST    Papuan Karitiana  593124         0.3011             0.3011\n468       FST Karitiana  Mandenka  593124         0.2798             0.2798\n526       FST  Mandenka Karitiana  593124         0.2798             0.2798\n    StdErr_Jackknife\n166        0.0016933\n456        0.0016933\n473        0.0026493\n676        0.0026493\n468        0.0017116\n526        0.0017116\n\n\nwhich shows that the largest FST values of around 0.3 are observed between Karitiana, from South America, and Biaka from Papua Neu Guinea (but note that these values are dependent on the ascertainment of SNPs, which here causes inflation)\nHere is a histogram of the values\n\nhist(datFST$Estimate_Total)\n\n\n\n\nSo most values are in the range of a few percent and 20 percent, with a mean of\n\nmean(datFST$Estimate_Total)\n\n[1] 0.09015616\n\n\nWe can compare that to F2:\n\nhist(datF2$Estimate_Total)\n\n\n\n\nwhich is an order of magnitude smaller.\nSo one of the key things to visualise is the pairwise matrix of FST, which we can quickly compute using the xtabs function from the stats package (part of base R):\n\nfstMat &lt;- xtabs(Estimate_Total ~ a + b, datFST)\nf2Mat &lt;- xtabs(Estimate_Total ~ a + b, datF2)\n\nand plot a simple heatmap using the powerful heatmap function from the stats package:\n\nheatmap(fstMat, symm = TRUE, hclustfun = function(m) hclust(m, method=\"ward.D2\"))\n\n\n\n\nwhich we can compare to the output using F2, which looks almost the same:\n\nheatmap(f2Mat, symm = TRUE, hclustfun = function(m) hclust(m, method=\"ward.D2\"))\n\n\n\n\nOK, let’s look at the dendrogram a bit closer:\n\nfstDist &lt;- as.dist(fstMat)\ndendro &lt;- hclust(fstDist, method=\"ward.D2\")\nplot(dendro, hang = -1)\n\n\n\n\nwhich again shows the strong drift that Native American populations (Karitiana) and Mayans experienced in their ancestral past.\nThis nicely shows how FST is affected by total drift, which is inversely proportional to population size, and proportional to total divergence time. A long branch can be caused by either low population size (as in the ancestral population of indigenous Americans) or long divergence time (as between populations from Africa and those outside of Africa)."
  },
  {
    "objectID": "pmrread.html#background",
    "href": "pmrread.html#background",
    "title": "6  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "6.1 Background",
    "text": "6.1 Background\n\n6.1.0.1 Measures of relatedness\nCoefficient of relationship, denoted \\(r_{ij}\\) , defined by Sewall Wright in 1922 (Wright 1922), is a measure of the degree of biological relationship between two individuals, commonly used in genetics and genealogy. It calculates the proportion of genes that two individuals have in common as a result of their genetic relationship. The coefficient of relationship is a derivative of the coefficient of inbreeding (\\(f_k\\) or \\(C_{I_k}\\)) defined by Wright a year earlier. A coefficient of inbreeding for an individual is typically one-half the coefficient of relationship between the parents.\nCoefficient of relationship between the parents approaches a value of 1 as the level of inbreeding increases and approaches 0 the more remote the common ancestors are.\nIn human relationships, coefficient of relationship is often calculated based on the knowledge of the family tree, typically extending to up to three or four generations, using a formula:\n\\[\nr_{ij} = \\sum (^1/_2)^{L_{ij}}\n\\] where \\(L\\) is the numbers of generation links between two individuals (\\(i\\) and \\(j\\)). E.g. full siblings are linked by two links through the mother (siblingA - mother - siblingB) and two links through the father (siblingA - father - siblingB), therefore the coefficient of relationship between them is \\(r = (^1/_2)^2 + (^1/_2)^2 = (^1/_4) + (^1/_4) = (^1/_2)\\) , while e.g. a person with their aunt are linked by three links through the shared grandmother/mother and three through the shared grandfather/father, so \\(r=(^1/_2)^3 + (^1/_2)^3 = (^1/_8) + (^1/_8) = (^1/_4)\\) .\nNote that under such definition, the coefficient of relationship is a lower bound and an actual value that may be up to a few percent higher due to unaccounted for consanguinity within the pedigree. The value is accurate to within 1% if the full family tree of both individuals is known to a depth of seven generations.\nKinship coefficient, denoted \\(\\phi_{ij}\\) [fa:i], is the probability that one allele sampled from individual \\(i\\) and one allele sampled from the same locus from individual \\(j\\) are identical by descent.\n\\(1 - \\phi_{ij}\\) can thus be interpreted as the probability that a randomly sampled allele from each individual is not identical by descent. Assuming that alleles are not under linkage disequilibrium, this value can be estimated from genome-wide data for a pair of individuals.\nKinship coefficient \\(\\phi_{ij}\\) , under some assumptions such as limited inbreeding, is related to Wright’s coefficient of relationship, denoted \\(r_{ij}\\) , via:\n\\[\n\\phi_{ij} = (^1/_2)r_{ij}\n\\] and hence provides a direct relationship between the degrees of relatedness from the pedigree and the expected kinship coefficient \\(\\phi_{ij}\\) .\n\n\n\nTable 1. Values of the coefficient of relatedness and the kinship coefficient for different pedigree relationships up to the second-degree, assuming that \\(C_I\\) = 0. From Rohrlach et al. (2023)."
  },
  {
    "objectID": "pmrread.html#pairwise-mismatch-rate-pmr",
    "href": "pmrread.html#pairwise-mismatch-rate-pmr",
    "title": "6  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "6.2 Pairwise Mismatch Rate (PMR)",
    "text": "6.2 Pairwise Mismatch Rate (PMR)\nMethods of relatedness estimation applied routinely to modern data are not applicable to ancient genomes due to small numbers of individuals sampled and high rate of data missingness (ie. low coverage), as well as due to lack of diploid phased genomic data available for majority of such samples. Thus, estimation of the coefficient of relatedness for ancient individuals using these methods is prone to biases and generally unreliable.\nPairwise mismatch rate (PMR) was introduced by (Kennett et al. 2017) as a means to estimate relatedness between ancient individuals (Figure 1). For each pair of individuals they computed the average mismatch rate across all autosomal SNPs covered by at least one sequence read for both of the two compared individuals (when &gt;1 sequence read was present for one individual at a given site, a random read was sampled for the analysis) and computed standard errors using a weighted block jackknife. Mismatch rates significantly lower (Z&gt;3) than the highest observed value, provided putative evidence of relatedness.\nThe PMR can be used to estimate the kinship coefficient, which, assuming that we can account for the inbreeding coefficient \\(C_I\\), can be used to estimate the degree of relatedness. Hence, we may gain insights into the pedigree joining many individuals (to a certain resolution). Kennett et al’s (2017) \\(PMR\\) estimation, however, did not include a hard-classification method nor was wrapped into any particular software piece.\n\n\n\nFig 1. Pairwise mismatch rate calculation (PMR). A) Pseudohaploid genomes are compared within each pair of individuals. B) Only sites called in both individuals are considered (filled circles) and classified as match (green) or mismatch (red). Accounting for linkage disequilibrium: C) Estimation of PMR using sliding window (as implemented in READ) as well as genome-wide (optional in READv2). D) Estimation of PMR on thinned SNP data (as implemented in BREADR)\n\n\nAccounting for pseudohaploidization:\nDue to pseudohaploidisation (ie. drawing one allele randomly for each position) identical individuals will have an expected PMR of half of this between unrelated individuals.\nThe estimate of relatedness coefficient \\(r\\) needs therefore be corrected using the expected mismatch rate in non-related individuals. In (Kennett et al. 2017) they chose correction based on the approximate maximum mismatch rates observed: \\(b = max(PMR_{observed})/2\\) . The estimator they used is thus:\n\\[\nr = 1 - ((PMR_{ij} - b)/b) .\n\\]\nPMR estimation in ancient-DNA-based inference of relatedness has first been implemented as separate software with READ (Monroy Kuhn, Jakobsson, and Günther 2018) and then by its successor - READv2 (Alaçamlı et al. 2024). Other software used in relatedness estimation among ancient individuals, such as BREADR (Rohrlach et al. 2023), also build on PMR.\nAccounting for linkage disequilibrium:\nLinkage disequilibrium (LD, non-independent co-inheritance) of the loci included in the PMR estimation will bias the results towards falsely positive relatedness detection. To minimize this effect, different approaches can be employed. This is particularly crucial, when analyzing genome-wide (shotgun) data. In the 1240k SNP panel widely used in ancient genomics the analysed loci have already been selected taking LD into account. The approaches to minimize the LD bias that have been employed in PMR estimation comprise PMR estimation separately over consecutive genome fragments (sliding window) and obtaining median estimate among them (e.g., implemented in READv1, and as an option in READv2; Figure 1C), and decreasing the number of SNPs included in the analysis using a threshold of physical proximity of SNPs along the genome (thinning; e.g., implemented in BREADR; Figure 1D).\nGood practice: Only pairs with at least 10,000 overlapping SNPs of the 1240k SNP panel should be included in PMR estimation (Furtwängler et al. 2020)"
  },
  {
    "objectID": "pmrread.html#read-version-1",
    "href": "pmrread.html#read-version-1",
    "title": "6  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "6.3 READ (version 1)",
    "text": "6.3 READ (version 1)\nRelationship Estimation from Ancient DNA (Monroy Kuhn, Jakobsson, and Günther 2018) - formalized implementation of PMR for ancient genomic data.\n\nPseudohaploid input data (TPED/TFAM format)\nDivision of the genome into non-overlapping windows of 1 Mbps each and calculation of the proportion of non-matching alleles inside each window (P0) for each pair of individuals.\nNormalization of P0 using the value expected for a randomly chosen pair of unrelated individuals from the population (in order to make the classification independent of within population diversity, SNP ascertainment and marker density).\nThis value is estimated by calculation of the median of all average pairwise P0 per window across all pairs of individuals, which, under sufficient sample size, can be treated as a proxy for an expected P0 in a pair of unrelated individuals. Normalization can also be performed using parameters other than median, e.g., maximum observed P0 value among the pairs.\nClassification of each pair of individuals as unrelated, second-degree (i.e. nephew/niece-uncle/aunt, grandparent-grand- child or half-siblings), first-degree (parent-offspring or siblings) or identical individuals/identical twins from the average across the per-window proportions of non-matching alleles (P0).\n\n\n\n\nFigure 2. Outline of the general READ workflow to estimate the degree of relationship between two individuals. From (Monroy Kuhn, Jakobsson, and Günther 2018)\n\n\nPluses: The method has been shown to work quite well with as little as 0.1x shotgun coverage per genome. It has very simple assumptions estimating the expected pairwise mismatch rate from the data without the need for population allele frequencies. It can thus be used as part of initial QC procedures (e.g. identifying duplicated individuals) or in populations (or species) for which little additional information is available.\nMinuses: READ had been implemented as a Python 2 script. The last version of Python 2 was released in 2020 and some systems have already stopped supporting the language. Furthermore, READ wrote a large number of temporary files to the hard disk which were then analyzed by a separate R script called from the Python script."
  },
  {
    "objectID": "pmrread.html#readv2",
    "href": "pmrread.html#readv2",
    "title": "6  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "6.4 READv2",
    "text": "6.4 READv2\nNew implementation of READ (Alaçamlı et al. 2024)\n\n6.4.0.0.1 Improvements over version 1\n\nAll analyses are carried out within a single Python3 script using NumPy (Harris et al. 2020) and pandas (McKinney 2010) libraries.\nMajor analysis speed improvement (although more memory-intensive)\nAvoids excessive use of temporary files and the calling of a separate R script.\nMinor gain in accuracy due to changes in some default values (based on performance tests results).\nIntroduction of the “effective number of overlapping SNPs” (number of overlapping SNPs times the pairwise mismatch rate expected for unrelated individuals) representing a measure of the amount of information available for kinship estimation in a given pair of individuals. Provides benchmarking and increases comparability between studies.\nAbility to classify up to third-degree relatives (requires at least 5000 effectively overlapping SNPs).\nAbility to differentiate between different types of first-degree relationships (ie. full siblings vs parent-offspring; requires at least 10,000 effectively overlapping SNPs).\nFeasible for as much as 696 individuals (241 860 pairs) provided enough available memory.\nGenome-wide P0 calculation as default; with the uncertainty estimated using a block-jackknife approach with block sizes of 5 Mb (as commonly employed in human population genomic studies; (Patterson et al. 2012)). Option to use window-based approach of chosen window size also available.\n\n\n\n\nFigure 3. READv2 flowchart. From (Alaçamlı et al. 2024)\n\n\n\n\n6.4.0.0.2 Noteworthy properties\nOverall, READv2 performs well down to at least 0.1X sequence data in the simulated dataset. This corresponded to on average about 1,878 overlapping SNPs for each pair of individuals at an expected mismatch for unrelated individuals of ∼0.247.\nIn default settings, READv2 performs a genome-wide estimate of the pairwise mismatch rate, based on which it will assess the degree of relationship in each pair of individuals. This is followed by a separate round of classification for first-degree relatives. Here, the genome is divided into 20Mb windows and the proportion of windows that are classified as either “identical/twin” or “unrelated” is estimated. These proportions correspond to Cotterman coefficients \\(k_0\\) – windows classified as unrelated (i.e. no shared chromosome), and \\(k_2\\) – windows classified as identical (i.e. both chromosomes shared). Expected \\(k_0 + k_2\\) is low for parent-offspring and around 0.5 for full siblings when sufficient data are available.\nIf that proportion is less than 0.3, the pair is classified as “parent-offspring”; if it is between 0.35 and 0.6, the pair is classified as “siblings”. For other proportions, or when the number of effectively overlapping SNPs is below 10,000, the pair remains classified as “first-degree” without further specification.\nThe two types of relations are well separated down to 0.5X coverage in the simulated dataset (or ∼8,000 “effectively overlapping SNPs”), but they overlap at 0.2X and below. Hence, to avoid wrongly classifying parent-offspring pairs as siblings, READv2 applies a default cutoff of 10,000 effectively overlapping SNPs, below which classification is not performed. (In (Rivollat et al. 2020) a cutoff of 7000 effective SNP number cutoff was used.)\nWarnings: At low coverage (0.05x and 0.1x) there are high false positive rates for second- and third-degree relatedness; many unrelated pairs are classified into these categories. At 0.01X, unrelated individuals are even classified as first-degree or identical twins, resulting in a reduced false positive rate for second- and third-degree but an increased false positive rate for first-degree. To avoid false classifications in empirical data, READv2 applies a conservative threshold of 5000 “effectively overlapping SNPs”, below which no attempt to classify third-degree relatives is taken.\nAccording to the authors READv2 alone can lead to very similar results as the combination of READv1 and lcMLkin. output table by including information such as the number of overlapping SNPs, the number of effectively overlapping SNPs, and the kinship coefficient \\(μ\\) (= 1 - normalized P0)\n\n\n6.4.0.0.3 Comparison with other methods\nREADv2 is very similar in its approach to BREADR (Rohrlach et al. 2023) and TKGWV2 (Fernandes et al. 2021), with each tool having its own unique features. READv2 has the functionality to separate the different first-degree relationships, BREADR has a better quantification of uncertainty, and TKGWV2 works well with lower amounts of input data.\n\n\n\nTable 2. Comparison of methods available for relatedness reconstruction in ancient genomic data. From Alaçamlı et al. (2024).\n\n\nFor further comparison of some of the methods available, you can also refer to the paper by Akturk et al. (2023)"
  },
  {
    "objectID": "pmrread.html#usage",
    "href": "pmrread.html#usage",
    "title": "6  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "6.5 Usage",
    "text": "6.5 Usage\n\n6.5.1 Running READ (version 1)\nREAD version 1 is implemented in Python 2.\nThe input for READ is a pair of files in Plink’s TPED/TFAM.\n## Get READ off Bitbucket\ngit clone https://bitbucket.org/tguenther/read.git\nIf starting from eigenstrat files you will need to use for example:\n\nconvertf (Eigensoft) and plink transpose:\n\n## Create a parameter file for Eigensoft convertf:\necho \"\ngenotypename:    myGenotypeFile.geno\nsnpname:         myGenotypeFile.snp\nindivname:       myGenotypeFile.ind\noutputformat:    PACKEDPED\ngenotypeoutname: myGenotypeFile.bed\nsnpoutname:      myGenotypeFile.bim\nindivoutname:    myGenotypeFile.fam\n\" &gt; scratch/myGenotypeFile.GenotToBed.convertf.param\n\n## Convert eigenstrat to packedped\nPathTo_EigensoftConvertf -p myGenotypeFile.GenotToBed.convertf.param\n\n## Transpose packedped to tped\nplink --bfile myGenotypeFile --recode transpose --out myGenotypeFile \n## The output is myGenotypeFile.tped, myGenotypeFile.tfam, myGenotypeFile.nosex, myGenotypeFile.log\nor\n\ntrident genoconvert, if you are working with Poseidon packages:\n\n## Convert eigenstrat-formatted package(s) to plink-formatted\ntrident genoconvert -d ... -d ... --outFormat PLINK\nWith the generated PLINK files you can no run READ:\n## Run READ\npython pathTo_Read.py myGenotypeFile &lt;normalization_method&gt;\n\n6.5.1.1 Options\nnormalization_method:\n\nmedian (default) - assuming that most pairs of compared individuals are unrelated, READ uses the median across all pairs for normalization.\nmean - READ uses the mean across all pairs for normalization, this would be more sensitive to outliers in the data (e.g. recent migrants or identical twins)\nmax - READ uses the maximum across all pairs for normalization. This should be used to test trios where both parents are supposed to be unrelated but the offspring is a first degree relative to both others.\nvalue &lt;val&gt; - READ uses a user-defined value for normalization. This can be used if the value from another population should be used for normalization. That would be useful if only two individuals are available for the test population. A value can be obtained from the NonNormalizedP0 column of the file meansP0_AncientDNA_normalized from a previous run of READ.\n\nOptionally, one can add --window_size &lt;value&gt; at the end in order to modify the window size used (default: 1000000).\n\n\n6.5.1.2 Output:\n“READ_results”:\nPairIndividuals Relationship Z_upper Z_lower I0232I0358 Unrelated NA -17.0238 ... I0354I0360 First Degree 3.8987 -9.6782 I0354I0361 Unrelated NA -15.4353 ... I0421I0430 First Degree 7.5325 -12.1995 I0421I0431 Unrelated NA -3.9037 ...\n““meansP0_AncientDNA_normalized”:\nPairIndividuals Normalized2AlleleDifference StandardError NonNormalizedP0 NonNormalizedStandardError I0232I0358 1.0069 0.0059 0.2531 0.00149 ... I0354I0360 0.7587 0.0138 0.1907 0.0035 I0354I0361 1.0092 0.0067 0.2537 0.0017 ... I0421I0430 0.7409 0.0095 0.1862 0.0024 I0421I0431 1.0115 0.0270 0.2543 0.0068 ...\n“READ_results_plot.pdf”:\n\n\n\n\n\n\n\n\n6.5.2 Running READ v2\nREADv2 is recommended to be ran as conda environment.\n## Preparing the environment\ngit clone https://github.com/GuntherLab/READv2.git\n\nconda create -n readv2 python=3.7 pandas=1.1.1 numpy=1.18.3 pip=22.3.1 matplotlib=3.5.3\n\nconda activate readv2\n\npip install plinkio  ## Used for format conversion of the inpt data.\n## Personally I couldn't make plinkio work and am using Poseidon/trident or eigensoft convertf (as described below).\nThe input for READv2 is a trio of files in Plink’s BED/BIM/FAM format.\nIf starting with files in EIGENSTRAT format, they can be converted to PLINK format using for example:\n\nconvertf (Eigensoft) and plink transpose:\n\n## Create convertf parameter file for converting .geno to .bed files (this can probably also be done using PLINKIO, as implemented in READv2 documentation):\necho \"\ngenotypename:    myGenotypeFile.geno\nsnpname:         myGenotypeFile.snp\nindivname:       myGenotypeFile.ind\noutputformat:    PACKEDPED\ngenotypeoutname: myGenotypeFile.bed\nsnpoutname:      myGenotypeFile.bim\nindivoutname:    myGenotypeFile.fam\n\" &gt; scratch/myGenotypeFile.GenoToBed.convertf.param\n    \n## Convert eigenstrat to packedped\nPathTo_EigensoftConvertf -p myGenotypeFile.GenotToBed.convertf.param\n## The output is myGenotypeFile.bed, myGenotypeFile.fam, myGenotypeFile.bim\nor\n\ntrident genoconvert, if you are working with Poseidon packages:\n\n## Convert eigenstrat-formatted package(s) to plink-formatted       \ntrident genoconvert -d ... -d ... --outFormat PLINK\nWith the PLINK format files you can run READv2:\n## Activate READv2 conda environment\nconda activate readv2\n\n## Run READv2\npython pathTo_Readv2.py -i myGenotypeFile\nREADv2 options:\n\n-i, --input_file val – Input file prefix (required). The current READ version only supports Plink bed/bim/fam files.\n-n, --norm_method val – Normalization method (either ‘mean’, ‘median’, ‘max’ or ‘value’).\n\nmedian (default) – assuming that most pairs of compared individuals are unrelated, READ uses the median across all pairs for normalization.\nmean – READ uses the mean across all pairs for normalization, this would be more sensitive to outliers in the data (e.g. recent migrants or identical twins)\nmax – READ uses the maximum across all pairs for normalization. This should be used to test trios where both parents are supposed to be unrelated but the offspring is a first-degree relative to both others.\nvalue – READ uses a user-defined value for normalization. This can be used if the value from another population should be used for normalization. That would be useful if only two individuals are available for the test population. Normalization value needs to be provided through --norm_value\n\n--norm_value val – Provide a user-defined normalization value\n--window_size val – Change window size for block jackknife or for window-based P0 estimates (as in READv1), default: 5000000\n--window_est – Window based estimate of P0 (as opposed to the genome-wide estimate, default in READv2)\n-h, --help – Print help message\n-v, --version – Print version\n\n\n6.5.2.1 Output:\n“meansP0_AncientDNA_normalized_READv2”\nPairIndividuals Norm2AlleleDiff StError_2Allele_Norm    Nonnormalized_P0    Nonnormalized_P0_serr   OverlapNSNPs\nI0232I0358  1.0052  0.0076    0.2553  0.0019    261263\n...\nI0234I0423  0.9659  0.0083  0.2453  0.0021    195053\nI0234I0424  0.9985  0.0071    0.2536 0.0018   224327\n...\nI0354I0360  0.7464  0.0137    0.1896 0.0035    21056\nI0354I0361  1.0046  0.0089    0.2552  0.0023    147438\n...\n“Read_Results.tsv”\nPairIndividuals Rel Zup Zdown   P0_mean Nonnormalized_P0    Nonnormalized_P0_serr   1st_Type    Perc_Win_1stdeg_P0  OverlapNSNPs    NSNPsXNorm  KinshipCoefficient\nI0232I0358  Unrelated   NA  12.9726  1.0052  0.2553  0.0019    N/A 0.9150  261263  66362.1298    -0.0052\n...\nI0234I0423  Third Degree    0.3487  7.1920   0.9659  0.2453  0.0021    N/A 0.7974  195053  49544.4533   0.0341\nI0234I0424  Unrelated   NA  12.9201  0.9985  0.2536 0.0018   N/A 0.9020  224327  56980.1981    0.0015\n...\nI0354I0360  First Degree    4.7682   8.7575   0.7464  0.1896 0.0035   N/A 0.3816  21056   5348.3310   0.2536\nI0354I0361  Unrelated   NA  11.0926 1.0046  0.2552  0.0023    N/A 0.9281  147438  37450.0013  -0.0046\n...\n“READ.pdf”:\n\n\n\n\n\n\n\n\n\nAktürk, Şevval, Igor Mapelli, Merve Nur Güler, Kanat Gürün, Büşra Katırcıoğlu, Kıvılcım Başak Vural, Ekin Sağlıcan, et al. 2023. “Benchmarking Kinship Estimation Tools for Ancient Genomes Using Pedigree Simulations.” bioRxiv.\n\n\nAlaçamlı, Erkin, Thijessen Naidoo, Şevval Aktürk, Merve N Güler, Igor Mapelli, Kıvılcım Başak Vural, Mehmet Somel, Helena Malmström, and Torsten Günther. 2024. “READv2: Advanced and User-Friendly Detection of Biological Relatedness in Archaeogenomics.” bioRxiv.\n\n\nFernandes, Daniel M, Olivia Cheronet, Pere Gelabert, and Ron Pinhasi. 2021. “TKGWV2: An Ancient DNA Relatedness Pipeline for Ultra-Low Coverage Whole Genome Shotgun Data.” Sci. Rep. 11 (1): 21262.\n\n\nFurtwängler, Anja, A B Rohrlach, Thiseas C Lamnidis, Luka Papac, Gunnar U Neumann, Inga Siebke, Ella Reiter, et al. 2020. “Ancient Genomes Reveal Social and Genetic Structure of Late Neolithic Switzerland.” Nat. Commun. 11 (1): 1915.\n\n\nHarris, Charles R, K Jarrod Millman, Stéfan J van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” Nature 585 (7825): 357–62.\n\n\nKennett, Douglas J, Stephen Plog, Richard J George, Brendan J Culleton, Adam S Watson, Pontus Skoglund, Nadin Rohland, et al. 2017. “Archaeogenomic Evidence Reveals Prehistoric Matrilineal Dynasty.” Nat. Commun. 8 (February): 14115.\n\n\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in Python,” 56–61.\n\n\nMonroy Kuhn, Jose Manuel, Mattias Jakobsson, and Torsten Günther. 2018. “Estimating Genetic Kin Relationships in Prehistoric Populations.” PLoS One 13 (4): e0195491.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93.\n\n\nRivollat, Maı̈té, Choongwon Jeong, Stephan Schiffels, İşil Küçükkalıpçı, Marie-Hélène Pemonge, Adam Benjamin Rohrlach, Kurt W Alt, et al. 2020. “Ancient Genome-Wide DNA from France Highlights the Complexity of Interactions Between Mesolithic Hunter-Gatherers and Neolithic Farmers.” Sci Adv 6 (22): eaaz5344.\n\n\nRohrlach, Adam B, Jonathan Tuke, Divyaratan Popli, and Wolfgang Haak. 2023. “BREADR: An r Package for the Bayesian Estimation of Genetic Relatedness from Low-Coverage Genotype Data.” bioRxiv. https://doi.org/10.1101/2023.04.17.537144.\n\n\nWright, Sewall. 1922. “Coefficients of Inbreeding and Relationship.” Am. Nat. 56 (645): 330–38."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aktürk, Şevval, Igor Mapelli, Merve Nur Güler, Kanat Gürün, Büşra\nKatırcıoğlu, Kıvılcım Başak Vural, Ekin Sağlıcan, et al. 2023.\n“Benchmarking Kinship Estimation Tools for Ancient Genomes Using\nPedigree Simulations.” bioRxiv.\n\n\nAlaçamlı, Erkin, Thijessen Naidoo, Şevval Aktürk, Merve N Güler, Igor\nMapelli, Kıvılcım Başak Vural, Mehmet Somel, Helena Malmström, and\nTorsten Günther. 2024. “READv2: Advanced and\nUser-Friendly Detection of Biological Relatedness in\nArchaeogenomics.” bioRxiv.\n\n\nBhatia, G, N Patterson, S Sankararaman, and A L Price. 2013.\n“Estimating and Interpreting FST: The Impact of Rare\nVariants.” Genome Research 23 (9): 1514–21. http://genome.cshlp.org/cgi/doi/10.1101/gr.154831.113.\n\n\nEwels, Philip A., Alexander Peltzer, Sven Fillinger, Harshil Patel,\nJohannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso,\nand Sven Nahnsen. 2020. “The Nf-Core Framework for\nCommunity-Curated Bioinformatics Pipelines.” Nature\nBiotechnology 38 (3): 276–78. https://doi.org/10.1038/s41587-020-0439-x.\n\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida\nAndrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia,\nJudith Neukamm, and Alexander Peltzer. 2021. “Reproducible,\nPortable, and Efficient Ancient Genome Reconstruction with\nNf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\n\nFernandes, Daniel M, Olivia Cheronet, Pere Gelabert, and Ron Pinhasi.\n2021. “TKGWV2: An Ancient DNA\nRelatedness Pipeline for Ultra-Low Coverage Whole Genome Shotgun\nData.” Sci. Rep. 11 (1): 21262.\n\n\nFurtwängler, Anja, A B Rohrlach, Thiseas C Lamnidis, Luka Papac, Gunnar\nU Neumann, Inga Siebke, Ella Reiter, et al. 2020. “Ancient Genomes\nReveal Social and Genetic Structure of Late Neolithic\nSwitzerland.” Nat. Commun. 11 (1): 1915.\n\n\nGreen, Richard E, Johannes Krause, Adrian W Briggs, Tomislav Maricic,\nUdo Stenzel, Martin Kircher, Nick Patterson, et al. 2010. “A Draft\nSequence of the Neandertal Genome.” Science 328 (5979):\n710–22. http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=20448178&retmode=ref&cmd=prlinks.\n\n\nHarris, Charles R, K Jarrod Millman, Stéfan J van der Walt, Ralf\nGommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020.\n“Array Programming with NumPy.”\nNature 585 (7825): 357–62.\n\n\nKennett, Douglas J, Stephen Plog, Richard J George, Brendan J Culleton,\nAdam S Watson, Pontus Skoglund, Nadin Rohland, et al. 2017.\n“Archaeogenomic Evidence Reveals Prehistoric Matrilineal\nDynasty.” Nat. Commun. 8 (February): 14115.\n\n\nLamnidis, Thiseas C, Kerttu Majander, Choongwon Jeong, Elina Salmela,\nAnna Wessman, Vyacheslav Moiseyev, Valery Khartanovich, et al. 2018.\n“Ancient Fennoscandian Genomes Reveal Origin and Spread of\nSiberian Ancestry in Europe.” Nature Communications 9\n(1): 5018. https://doi.org/10.1038/s41467-018-07483-5.\n\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif\nLazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023.\n“The Allen Ancient DNA Resource (AADR):\nA Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\n\n\nMartin, Simon H, John W Davey, and Chris D Jiggins. 2015.\n“Evaluating the Use of ABBA-BABA Statistics to Locate\nIntrogressed Loci.” Molecular Biology and Evolution 32\n(1): 244–57. https://doi.org/10.1093/molbev/msu269.\n\n\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in\nPython,” 56–61.\n\n\nMonroy Kuhn, Jose Manuel, Mattias Jakobsson, and Torsten Günther. 2018.\n“Estimating Genetic Kin Relationships in Prehistoric\nPopulations.” PLoS One 13 (4): e0195491.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin\nRohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich.\n2012a. “Ancient Admixture in Human History.”\nGenetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\n\n\n———. 2012b. “Ancient Admixture in Human History.”\nGenetics 192 (3): 1065–93.\n\n\nPeter, Benjamin M. 2016. “Admixture, Population Structure, and\nF-Statistics.” Genetics 202 (4): 1485–1501.\nhttps://doi.org/10.1534/genetics.115.183913.\n\n\nRivollat, Maı̈té, Choongwon Jeong, Stephan Schiffels, İşil Küçükkalıpçı,\nMarie-Hélène Pemonge, Adam Benjamin Rohrlach, Kurt W Alt, et al. 2020.\n“Ancient Genome-Wide DNA from France Highlights the\nComplexity of Interactions Between Mesolithic Hunter-Gatherers and\nNeolithic Farmers.” Sci Adv 6 (22): eaaz5344.\n\n\nRohrlach, Adam B, Jonathan Tuke, Divyaratan Popli, and Wolfgang Haak.\n2023. “BREADR: An r Package for the Bayesian Estimation of Genetic\nRelatedness from Low-Coverage Genotype Data.” bioRxiv.\nhttps://doi.org/10.1101/2023.04.17.537144.\n\n\nWeir, B S, and W G Hill. 2002. “Estimating f-Statistics.”\nAnnual Review of Genetics 36: 721–50. https://doi.org/10.1146/annurev.genet.36.050802.093940.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific\nData Management and Stewardship.” Scientific Data 3 (1).\nhttps://doi.org/10.1038/sdata.2016.18.\n\n\nWright, Sewall. 1922. “Coefficients of Inbreeding and\nRelationship.” Am. Nat. 56 (645): 330–38."
  }
]