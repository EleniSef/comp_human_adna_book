[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Methods for human population genetics and ancient DNA",
    "section": "",
    "text": "This book summarises prepared mini-courses for various computational tools and methods in the field of human archaeogenetic data analysis, with a particular emphasis on population genetics.\nThe chapters are contributed by different authors, as indicated in the Yaml-frontmatter of each chapter’s .qmd source file."
  },
  {
    "objectID": "Quarto_intro.html",
    "href": "Quarto_intro.html",
    "title": "1  Introduction to Quarto",
    "section": "",
    "text": "In this introduction to quarto, you will be shown the basics in how to set up a website and a book. For more detailed information, I highly recommend checking out the website and also watch the introduction tutorial."
  },
  {
    "objectID": "Quarto_intro.html#setting-up",
    "href": "Quarto_intro.html#setting-up",
    "title": "1  Introduction to Quarto",
    "section": "1.1 Setting up",
    "text": "1.1 Setting up\nQuarto is the “next generation” of R Markdown and is usable on different tools.\n\nHere, we will describe how to set up your environment to use Quarto in RStudio, and VSCode.\nQuarto in general is set up to be very intuitive and user friendly. And while it is possible to set up different documents simultaneously, those can also easily be set up to in just the way you need for whatever occasion. So, for either communicating your results to collaborators, discuss code with your supervisor, setting up a website or writing your paper, to just name some scenarios, quarto comes in quite handy. So, let’s begin:"
  },
  {
    "objectID": "Quarto_intro.html#rstudio",
    "href": "Quarto_intro.html#rstudio",
    "title": "1  Introduction to Quarto",
    "section": "1.2 RStudio",
    "text": "1.2 RStudio\nFor this, you have to download RStudio first. If you have done this already, we can get started.\n\n1.2.1 Getting started\nFirst, we have to install the quarto package using the following command in our console:\ninstall.packages(\"quarto\")\nNow we are ready to set up a quarto document.\nFor this we open a new project and select the quarto document we want to create. You can choose to set up a git repository as well. For practicality, I usually also tick the box visual markdown editor.\n\nThe new project will look like this:\n\n\n\n1.2.2 Universal instructions\nWhen setting up your document, quarto will always provide you with some presets. So first, we will have a look into the .qmd files, for they are handled the same way, regardless the format you are setting up (website, book, presentation, etc.).\n\n1.2.2.1 Render & save\nIf we now click on render, we will be provided with the preview of our final version if the project in the viewer.\nImportant: Do not mistake “save” with “render”. Just by saving, your document did not get rendered automatically, unless you tick the box “Render on Save”. You have to tick that box on each .qmd file individually though.\n\n\n\n1.2.2.2 Visual and Source\nIf you have chosen the Visual option on your toolbar, the preview will mostly resemble your .qmd files. If you are more comfortable with the R markdown optics, you can switch to Source.\nIn the Source version, you can write up your document in LaTeX.\n\n\n\n1.2.2.3 .qmd files\nQuarto will automatically provide you with two .qmd files, as well as a .yml file.\nThe .qmd files respond to the individual pages of your website or chapter of your book or pages of your presentation, etc. You can shape them individually or define the layout for all of them in the .yml file, to which we will get later.\nIn your .qmd files you also find a yaml at the top of your document, separated by\n---\n---\nWithin these, you can define the outline of .qmd file individually, starting with the page header. Other options, you might be using in the future are:\nauthor: Jessi Doe\n-> will add an author underneath the header\nexecute:    echo: true\n-> if >true<, code will be visible\ntoc:true\n-> if >true<, a table of content will be automatically added\nbibliography: your_references.bib\n-> file or list of files for your references\nIt is crucial to stick to the spacing. Otherwise, an error will occur.\n\n1.2.2.3.1 Insert\nIf you click Insert, you will realize, quarto provides you with a lot of options and shortcuts as well. So by just selecting on your chosen item to insert, it will be added to the document, while you are also provided with options on the appearance (in the case of figure/images or tables, etc). We will here have a brief look into how to work with R code and how to use the reference option.\n\n\n1.2.2.3.2 R code\nTo add R code to your file, you select Insert, select Code Cell and choose the kind of code you want to insert. In this case, R. There are some things to keep in mind though.\nDepending on how you have set up your .qmd file (or your .yml), your code will be visible or not on your website. To check your output, you can click the green arrow for the latest bit of code or the grey arrow above a bar to run the previous code.\n\nBut in case there are some chunks of code, you do not want to show all the time, there are some nice sets.\nSo if we just load the library tidyverse, for example, the additional information regardless and it will be also visible on our website.\n\nTo avoid this, we can set up a code chunk, looking like this:\n\nThis will prevent the output of this code chunk to be depicted on your website, while the package is otherwise active and can be used in the following R code. This is, by the way, true for all R code and data sets you will use: they will be active in your document and can be used in different code chunks, once provided.\nA code block included in your document could look like the following. Here I used the option\ncode-fold: true so the code can be extended. This option is only available in html though.\n\n\n\n1.2.2.3.3 References\nDepending on which citation program you are using, quarto is able to connect to it (Zotero works, for example). So, when selecting to insert a Citation, you can choose to simply add a reference from your program.\nAlternatively, you are provided with some options to choose from:\n\nIn your source code and your website, a citation will be depicted as follows:\n\nThe citation will also automatically be added to a references.bib as well as to a references.qmd and is therefore available on your website on the page “References”, which also will be created automatically.\n\n\n\n\n1.2.3 Render your document\nWhen done with setting up your documents, you would like to have the actual output. Depending on the format you set in your .yml file, your output can be a html, pdf, MS Word, OpenOffice, or ePUB file. To create those, the terminal in your RStudio Project is used.\nBy using the command:\nquarto render\nall formats you predefined in your .yml file will be rendered.\nIn case you are only interested in one format to be rendered, you can specify your command:\nquarto render -to pdf\nYour rendered document should now look like this:\n\n\n\n1.2.4 Quarto website\nIn the provided .qmd files, the index is also the first page of your website. As you might have noticed, quarto already sets up a navbar as well as a search function on your website.\n\n\n1.2.4.1 .yml files\nWhile your .qmd files contain information about one page, the .yml file defines the overall looks of the website.\nHere, you can define the type of your project (in this case a website), you can change the name of the website (Title), define your navbar (which shall be your landing page and in what order your pages shall be set up) or the overall appearance of your website in general (theme, css, toc, backgroundcolor, etc.). For different styles and layouts, check out the quarto website again.\n\n\n\n\n1.2.5 Quarto book\nThe setup of the .yml file in a quarto book is slightly different than that of the website. So here are some general remarks about them.\n\n1.2.5.1 .yml files\nWe see, for example, that instead of a navbar, we find chapters. Those will appear in the listed order in your book.\nWe also already get provided with a bibliography and the responding .bib file. If you have other .bib files, those can be included in your references, by just adding them to bibliography.\n\n_________________________________________________________________________\nWith this, you should at least have some ideas on how you can use quarto in your daily work routine. Happy coding and please feel free to contact me for any remarks or questions. I am happy to try and help."
  },
  {
    "objectID": "Quarto_intro.html#vscode",
    "href": "Quarto_intro.html#vscode",
    "title": "1  Introduction to Quarto",
    "section": "1.3 VSCode",
    "text": "1.3 VSCode\nMuch of the concepts as described in the RStudio tutorial above apply equally well to using Quarto in VSCode, just with a different interface to execute them.\nHere we will describe how to set up VSCode and Quarto, and how to preview and render Quarto objects within the VSCode interface.\nTo understand about more about the details of which files to edit etc., please see the RStudio description above.\n\n1.3.1 Getting Started\n\nInstall the Quarto CLI tool for your operating system from the Quarto Website\nInstall the VSCode Quarto extension\n\n\n\n1.3.2 Using Quarto\nThe basic workflow is as follows:\n\nCreate or modify .qmd objects etc as described above in the Rstudio section about Quarto markdown files\nWithin VSCode, make sure you’ve opened it in the repository containing all the files\nPress ctrl + shift + p to bring up your command palette\n\nTo preview a local ‘live’ version of HTML or website versions, you can type Quarto: preview. To close the live preview, press ctrl+c in the VSCode terminal.\nTo render all the files e.g. final HTML and/or PDF versions, you can type Quarto: Render Project, where you will be given different options depending on the formats defined in the _quarto.yml file.\n\n\nFor further VSCode integrations, just type Quarto: into your command palette and explore all the options."
  },
  {
    "objectID": "poseidon.html",
    "href": "poseidon.html",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "",
    "text": "Poseidon is an open computational framework to enable standardised and FAIR (Wilkinson et al. (2016)) handling of genotypes with their highly relevant context information.\nIt includes a well-specified data format, advanced software tools, and public, community-maintained archives to support the entire archaeogenetic research cycle, from data acquisition to management, analysis and publication.\nPoseidon and all its components is well documented at https://www.poseidon-adna.org. In this tutorial we will give a brief overview and highlight two workflows in the context of Poseidon."
  },
  {
    "objectID": "poseidon.html#the-components-of-poseidon",
    "href": "poseidon.html#the-components-of-poseidon",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.1 The components of Poseidon",
    "text": "2.1 The components of Poseidon\n\n\n\nOverview of the Poseidon framework\n\n\nPoseidon is an entire ecosystem build on top of the data format specification of the Poseidon package.\n\n2.1.1 The Poseidon package\nA Poseidon package bundles genotype data in EIGENSTRAT/PLINK format with human- and machine-readable meta-data.\n\n\n\nThe files in a Poseidon package\n\n\nIt includes sample-wise context information like spatio-temporal origin and genetic data quality in the .janno-file, literature in the .bib, and pointers to sequencing data in the .ssf file.\n.janno and .ssf have many predefined and specified columns, but can store arbitrary additional variables.\n\n\n2.1.2 The software tools\ntrident is a command line application to create, download, inspect and merge Poseidon packages – and therefore the central tool of the Poseidon framework. The init subcommand creates new packages from genotype data, fetch downloads them from the public archives through the Web-API, and forge merges and subsets them as specified. list gives an overview over entities in a set of packages and validate confirms their structural integrity.\nxerxes is derived from trident and allows to directly perform various basic and experimental genomic data analyses on Poseidon packages. It implements allele sharing statistics (\\(F_2\\), \\(F_3\\), \\(F_4\\), \\(F_{ST}\\)) with a flexible permutation interface.\njanno is an R package to simplify reading .janno files into R and the popular tidyverse ecosystem (Wickham et al. (2019)). It provides an S3 class janno that inherits from tibble.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\nqjanno is another command line tool to perform SQL queries on .janno files. On start-up it creates an SQLite database in memory and reads .janno files into it. It then sends any user-provided SQL query to the database server and forwards its output.\n\n\n2.1.3 The public archives\nThe Poseidon community maintains public archives for Poseidon packages to establish a central, open point of access for published, archaeogenetic genotype data.\n\nThe Community Archive: Author supplied, per-paper packages with the genotype data published in the respective papers. Partially pre-populated from various versions of the AADR.\nThe AADR Archive: Complete and structurally unified releases of the Allen Ancient DNA Resource (Mallick et al. (2023)) repackaged in the Poseidon package format.\nThe Minotaur Archive: Per-paper packages with genotype data reprocessed by the Minotaur workflow (see below).\n\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif Lazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023. “The Allen Ancient DNA Resource (AADR): A Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\nThe data is versioned with Git and hosted on GitHub for easy co-editing and automatic structural validation.\nIt can be accessed through a Web-API with various endpoints at server.poseidon-adna.org, e.g. /packages for a JSON list of packages in the community archive.\nThis API enables a little Archive explorer web app on the Poseidon website.\n\n\n2.1.4 The Minotaur workflow\nThe Minotaur workflow is a semi-automatic workflow to reproducibly process published sequencing data from the International Nucleotide Sequence Database Collaboration (INSDC) archives into Poseidon packages.\nCommunity members can request new packages by submitting a build recipe as a Pull Request against a dedicated submission GitHub repository. This recipe is derived from a Sequencing Source File (.ssf), describing the sequencing data for the package and where it can be downloaded.\nUsing the recipe, the sequencing data gets processed through nf-core/eager (Fellows Yates et al. (2021)) on computational infrastructure of MPI-EVA, using a standardised, yet flexible, set of parameters.\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\nThe generated genotypes, together with descriptive statistics of the sequencing data (Endogenous, Damage, Nr_SNPs, Contamination), are compiled into a Poseidon package, and made available to users in the Minotaur archive."
  },
  {
    "objectID": "poseidon.html#forging-a-dataset-with-trident",
    "href": "poseidon.html#forging-a-dataset-with-trident",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.2 Forging a dataset with trident",
    "text": "2.2 Forging a dataset with trident\nforge creates new Poseidon packages by extracting and merging packages, populations and individuals/samples from your Poseidon repositories. It can also work directly with your genotype data. In addition, forge allows merging of multiple data sets (packages), in contrast to mergeit which merges only two data sets at a time.\n(-f/--forgeString) can be used to query entire packages, groups or individuals. In general --forgeString query consists of multiple entities, inside \"\" separated by , .\n\nTo include all individuals in a Poseidon package, use * to surround the package title.*2019_Jeong_InnerEurasia* . In cases where only genotype files are available, use the file name prefix.\nTo include certain group(s) from a Poseidon package, simply add them to the -f query. No specific markers are required. Russia_HG_Karelia\nTo extract individuals only, surround them by < and >. <ALA026> . To exclude individuals add package name *package* and <individual> with a dash sign. \"*2021_Saag_EastEuropean-3.2.0*,-<NIK003>\"\n\n\ntrident forge \\\n  -p pileupcaller.double.geno \\\n  -d 2021_Saag_EastEuropean-3.2.0 \\\n  -d 2016_FuNature-2.1.1 \\\n  -f \"*pileupcaller.double*,Russia_AfontovaGora3,<NIK003>\" \\\n  -o testpackage \\\n  --outFormat EIGENSTRAT \\\n  /\nForge selection language\nforge has a an optional flag --intersect, that defines, if the genotype data from different packages should be merged with an intersect instead of the default union operation. The default is to output the union of all SNPs, by setting the additional SNPs from the other merged package as missing in the samples that did not have them originally. This option is useful for making a data set based on Human Origins (HO) SNPs for analysis like PCA and ADMIXTURE.\ntrident forge \\\n  --intersect \\\n  -p pileupcaller.double.geno \\\n  -d 2012_PattersonGenetics-2.1.3 \\\n  -o testpackage_HO \\\n  --outFormat EIGENSTRAT \\\n  /\nIn case of PCA, --forgeFile can be used to merge necessary populations/groups from the available packages in the community archive to create specific PCA configurations.\ntrident forge \\\n  -d /path/to/community/archive \\\n  --forgeFile WestEurasia_poplist.txt \\\n  -o WE_PCA \\\n  /\nIn addition, --selectSnps allows to provide forge with a SNP file in EIGENSTRAT (.snp) or PLINK (.bim) format to create a package with a specific selection. This option generates a package with exactly the SNPs listed in this file."
  },
  {
    "objectID": "poseidon.html#contributing-to-the-community-archive",
    "href": "poseidon.html#contributing-to-the-community-archive",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.3 Contributing to the community archive",
    "text": "2.3 Contributing to the community archive\n\n\n\nPoseidon needs your data as soon as it is published\n\n\nTo maintain the public data archives, specifically the community archive and the minotaur archive, Poseidon depends on work donations by an interested community.\nMany practitioners of archaeogenetics both produce genotype data from archaological contexts and require the reference data from other publications, provided in public archives, to contextualize it.\nIf authors themselves provide high-quality, easily accessible versions of their data beyond the raw data available at the INSDC databases, they gain at least three important advantages:\n\nTheir work will be easily findable and potentially cited more often.\nThey have primacy over how their data is communicated. That is, which genotypes, dates or group names they consider correct.\nTheir results for derived, genotype based analyses (PCA, F-Statistics, etc.) can be reproduced exactly.\n\nAnd the whole community wins, because sharing the tedious data preparation tasks empowers all researchers to achieve more in shorter time.\n\n\n\n\nThis tutorial explains the main cornerstones of a workflow to add a new Poseidon package to the community archive after publishing the corresponding dataset. The process is documented in more detail in a Submission guide on the website.\n\nMake yourself familiar with a number of core technologies. This is less daunting than it sounds, because: Superficial knowledge is sufficient and knowing them is useful beyond this particular task.\n\n\nCreating and validating Poseidon packages with the trident tool.\nFree and open source distributed version control with Git.\nCollaborative working on Git projects with GitHub.\nHandling large files in Git using Git LFS.\n\n\nCreate a package from your genotype data and fill it with a suitable set of meta and context information.\n\n\ntrident init allows to wrap genotype data in a dummy Poseidon package. Imagine we had genotype data for a number of individuals in EIGENSTRAT format:\n\n\nmyData.ind\n\nSample1  M       ExamplePop1\nSample2  F       ExamplePop1\nSample3  M       ExamplePop2\n\n\n\nmyData.snp\n\n           rs3094315     1        0.020130          752566 G A\n          rs12124819     1        0.020242          776546 A G\n          rs28765502     1        0.022137          832918 T C\n\n\n\nmyData.geno\n\n099\n922\n999\n\nWith trident init -p myData.geno -o myPackage we can create a basic package around this data.\n$ ls myPackage\nmyData.geno  myData.snp     myPackage.janno\nmyData.ind   myPackage.bib  POSEIDON.yml\nIn a next step we modify POSEIDON.yml, .janno and .bib to include the context information we consider relevant. All of these files are well specified and documented, so we only demonstrate a minimal change for this example:\nWe replace the main contributor for the package.\n\n\nmyPackage/POSEIDON.yml\n\nposeidonVersion: 2.7.1\ntitle: myPackage\ndescription: Empty package template. Please add a description\ncontributor:\n- name: Clemens Schmid               #- name: Josiah Carberry\n  email: clemens_schmid@eva.mpg.de   #  email: carberry@brown.edu\n  orcid: 0000-0003-3448-5715         #  orcid: 0000-0002-1825-0097\npackageVersion: 0.1.0\nlastModified: 2023-10-18\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: myData.geno\n  snpFile: myData.snp\n  indFile: myData.ind\n  snpSet: Other\njannoFile: myPackage.janno\nbibFile: myPackage.bib\n\nWhen we applied all necessary modifications we can confirm that the package is still valid with trident validate -d myPackage.\n\n\nSubmit the package to the community archive.\n\n\nTo submit the package we have to create a fork of the community archive repository on GitHub. This requires a GitHub account.\n\n\n\n\nPress the fork button in the top right corner to fork a repository on GitHub\n\n\n\nAnd then clone the fork to our computer, while omitting the large genotype data files. Note that this requires several setup steps to work correctly:\n\nGit has be installed for your computer (see here)\nYou must have created an ssh key pair to connect to GitHub via ssh (see here)\nGit LFS has to be installed (see here) and and configured for your user with git lfs install\n\nGIT_LFS_SKIP_SMUDGE=1 git clone git@github.com:<yourGitHubUserName>/community-archive.git\nWith the cloned repository on our system we can copy the files into the repositories directory and commit the changes.\n\n\nin the community-archive directory\n\ncp -r ../myPackage myPackage\ngit add myPackage\ngit commit -m \"added a first draft of myPackage\"\ngit push\n\nIn a last step we can open a Pull Request on GitHub from our fork to the original archive repository. Poseidon core members will take it from here.\n\n\n\n\nWhen you pushed to your fork, GitHub will automatically offer to “contribute” to the source repository"
  },
  {
    "objectID": "fstats.html",
    "href": "fstats.html",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "",
    "text": "F3 statistics are a useful analytical tool to understand population relationships. F3 statistics, just as F4 and F2 statistics measure allele frequency correlations between populations and were introduced by Nick Patterson (Patterson et al. 2012), but see also (Peter 2016) for another introduction.\nF3 statistics are used for two purposes: i) as a test whether a target population (C) is admixed between two source populations (A and B), and ii) to measure shared drift between two test populations (A and B) from an outgroup (C).\nF3 statistics are in both cases defined as the product of allele frequency differences between population C to A and B, respectively:\n\\[F3(A,B;C)=\\langle(c−a)(c−b)\\rangle\\]\nHere, \\(\\langle\\cdot\\rangle\\) denotes the average over all genotyped sites, and a, b and c denote the allele frequency for a given site in the three populations A, B and C.\nIt can be shown that if \\(F3(A, B; C)\\) is negative, it provides unambiguous proof that population C is admixed between populations A and B, as in the following phylogeny (taken from Figure 1 from (Patterson et al. 2012):\n\nIntuitively, an F3 statistics becomes negative if the allele frequency of the target population C is on average intermediate between the allele frequencies of A and B. Consider as an extreme example a genomic site where \\(a=0\\), \\(b=1\\) and \\(c=0.5\\). Then we have \\((c−a)(c−b)=−0.25\\), which is negative. So if the entire statistics is negative, it suggests that in many positions, the allele frequency c is indeed intermediate, suggesting admixture between the two sources.\nOne way to understand this is by looking what happens to a list of SNPs and allele frequencies for groups A, B and C:\n\n\n\nSNP\nA\nB\nC\n\\((c-a)(c-b)\\)\n\n\n\n\n1\n1\n0\n0.5\n-0.25\n\n\n2\n0.8\n0\n0\n0\n\n\n3\n0\n0.7\n0\n0\n\n\n4\n0.1\n0.5\n0.3\n-0.04\n\n\n5\n0\n0.1\n0.2\n0.02\n\n\n6\n1\n0.2\n0.9\n-0.07\n\n\n\\(F_3(A, B;C)\\)\n\n\n\n-0.057\n\n\n\nEvery SNP where C has an allele frequency intermediate between A and B contributes negatively. Here, the average is also negative, providing evidence for admixture. For statistical certainty, an error bar for this estimate is needed, which is typically computed via Jackknife (see for example the xerxes whitepaper).\n\n\n\n\n\n\nDanger\n\n\n\nIf an F3 statistics is not negative, it does not proof that there is no admixture!\n\n\n\n\nWe will use this statistics to test if Finnish are admixed between East and West, using different Eastern and Western sources. In the West, we use French, Icelandic, Lithuanian and Norwegian as source, and in the East we use Nganasan and one of the ancient individuals analysed in (Lamnidis et al. 2018), from the site of Bolshoy Oleni Ostrov from the Northern Russian Kola-peninsula, and dating to 3500 years before present.\n\n\n\n\n\n\nTip\n\n\n\nIf you happen to have downloaded a copy of the Poseidon Community Archive already, then just use the path to that archive in the following commands. Otherwise you download the entire archive via\n\ntrident fetch -d /somewhere/to/store/the/archive --downloadAll \n\nor just the relevant packages for the examples in this chapter:\n\ntrident fetch -d /somewhere/to/store/the/archive -f \"*2012_PattersonGenetics*,*2014_RaghavanNature*,*2014_LazaridisNature*,*2018_Lamnidis_Fennoscandia*\"\n\n\n\nWe use the software xerxes fstats from the Poseidon Framework. Here is a command line that computes 8 statistics for us:\n\nxerxes fstats -d ~/dev/poseidon-framework/community-archive \\\n    --stat \"F3(Nganasan,French,Finnish)\" \\\n    --stat \"F3(Nganasan, Icelandic, Finnish)\" \\\n    --stat \"F3(Nganasan, Lithuanian, Finnish)\" \\\n    --stat \"F3(Nganasan, Norwegian, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, French, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Icelandic, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Lithuanian, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Norwegian, Finnish)\" \\\n\n\n\n\n\n\n\nNote\n\n\n\nNote that xerxes fstats will automatically find the right packages from your local archive that contain these groups. You can see in the output of the program which packages contribute:\n[Info]    5 relevant packages for chosen statistics identified:\n[Info]    *2012_PattersonGenetics-2.1.3*\n[Info]    *2014_LazaridisNature-4.0.2*\n[Info]    *2016_LazaridisNature-2.1.3*\n[Info]    *2018_Lamnidis_Fennoscandia-2.1.0*\n[Info]    *2019_Flegontov_PalaeoEskimo-2.2.1*\nSo these five packages contain the samples requested in these statistics. You can inquire about this also more manually using trident list\n\n\nHere is the result that you should get, nicely layouted in a Text-table:\n.-----------.----------------.------------.---------.---.---------.----------------.--------------------.------------------.---------------------.\n| Statistic |       a        |     b      |    c    | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife |  Z_score_Jackknife  |\n:===========:================:============:=========:===:=========:================:====================:==================:=====================:\n| F3        | Nganasan       | French     | Finnish |   | 593124  | -1.0450e-3     | -1.0451e-3         | 1.2669e-4        | -8.249133659451905  |\n| F3        | Nganasan       | Icelandic  | Finnish |   | 593124  | -1.1920e-3     | -1.1920e-3         | 1.3381e-4        | -8.908381869946188  |\n| F3        | Nganasan       | Lithuanian | Finnish |   | 593124  | -1.1605e-3     | -1.1605e-3         | 1.5540e-4        | -7.4680182465607245 |\n| F3        | Nganasan       | Norwegian  | Finnish |   | 593124  | -1.0913e-3     | -1.0914e-3         | 1.3921e-4        | -7.83945981272796   |\n| F3        | Russia_Bolshoy | French     | Finnish |   | 542789  | -6.1807e-4     | -6.1809e-4         | 1.0200e-4        | -6.059872900228102  |\n| F3        | Russia_Bolshoy | Icelandic  | Finnish |   | 542789  | -6.2801e-4     | -6.2802e-4         | 1.1792e-4        | -5.325695961373772  |\n| F3        | Russia_Bolshoy | Lithuanian | Finnish |   | 542789  | -3.7310e-4     | -3.7310e-4         | 1.2973e-4        | -2.8760029685791637 |\n| F3        | Russia_Bolshoy | Norwegian  | Finnish |   | 542789  | -3.7646e-4     | -3.7653e-4         | 1.1630e-4        | -3.2375830440434323 |\n'-----------'----------------'------------'---------'---'---------'----------------'--------------------'------------------'---------------------'\n\n\n\n\n\n\nTip\n\n\n\nUse the option -f <FILE> to output the results additionally to a tab-separated file, or --raw if you prefer the standard output to be tab-separated\n\n\nYou can see that in all cases this statistic is negative (Estimate_Total). The next two columns ( Estimate_Jackknife and StdErr_Jackknife) are computed using Jackknifing (see the xerxes whitepaper for details). The last column is the Z score, and it is important here: It gives the deviation of the f3 statistic from zero in units of the standard error. As general rule, a Z score of -3 or more suggests a significant rejection of the Null hypothesis that the statistic is not negative. In this case, all of the statistics are significantly negative (with one borderline exception), proving that Finnish have ancestral admixture of East and West Eurasian ancestry. Here, Eastern ancestry is represented by Nganasan or Russia_Bolshoy, respectively, while Western ancestry by French, Icelandic, Lithuanian and Norwegian, respectively. Note that the statistics does not suggest when this admixture happened!\n\n\n\nAs you will have noticed, the command line above is getting quite long, since a separate --stat option has to be entered for every statistic to be computed. There is a more powerful and elegant interface to xerxes, which uses a configuration file in YAML format. To illustrate it, let us consider the configuration file (which can be found in fstats_working/F3_finnish.config in the git-repository of this book) needed to compute the same statistic as above:\nfstats:\n- type: F3\n  a: [\"Nganasan\", \"Russia_Bolshoy\"]\n  b: [\"French\", \"Icelandic\", \"Lithuanian\", \"Norwegian\"]\n  c: [\"Finnish\"]\nYou can then run xerxes as\n\nxerxes fstats -d ~/dev/poseidon-framework/community-archive --statConfig fstats_working/F3_finnish.config\n\nSo xerxes then automatically creates all combinations of populations listed in slots a, b and c.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are actually three types of F3-statistics supported by xerxes:\n\nF3vanilla: The purest form, defined literally as \\(\\langle(c−a)(c−b)\\rangle\\)\nF3: A bias-corrected version, which is only valid for groups in C that have non-zero heterozygosity\nF3star: This one is normalised by the heterozgygosity of the third population, C, as suggested in (Patterson et al. 2012) and implemented in the Admixtools package.\n\nThe white-paper explains this in detail.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe configuration file format has a lot more options. Here is a bit more complex example, but see also the documentation:\n# You can define groups right within the configuration file.\n# here we use negative selection to remove individuals from the\n# newly defined groups\ngroupDefs:\n  CEU2: [\"CEU.SG\", \"-<NA12889.SG>\", \"-<NA12890.SG>\"]\n  FIN2: [\"FIN.SG\", \"-<HG00383.SG>\", \"-<HG00384.SG>\"]\n  GBR2: [\"GBR.SG\", \"-<HG01791.SG>\", \"-<HG02215.SG>\"]\n  IBS2: [\"IBS.SG\", \"-<HG02238.SG>\", \"-<HG02239.SG>\"]\nfstats:\n- type: F2 # this will create 2x2 = 4 F2-Statistics\n  a: [\"French\", \"Spanish\"]\n  b: [\"Han\", \"CEU2\"]\n- type: F3vanilla # This will create 3x2x1 = 6 Statistics\n  a: [\"French\", \"Spanish\", \"Mbuti\"]\n  b: [\"Han\", \"CEU2\"]\n  c: [\"<Chimp.REF>\"]\n- type: F4 # This will create 5x5x4x1 = 100 Statistics\n  a: [\"<I0156.SG>\", \"<I0157.SG>\", \"<I0159.SG>\", \"<I0160.SG>\", \"<I0161.SG>\"]\n  b: [\"<I0156.SG>\", \"<I0157.SG>\", \"<I0159.SG>\", \"<I0160.SG>\", \"<I0161.SG>\"]\n  c: [\"CEU2\", \"FIN2\", \"GBR2\", \"IBS2\"]\n  d: [\"<Chimp.REF>\"]\n# Altogether: 110 statistics of different types\nwhich will not just create multiple statistic using row-combinations, as described, but also uses newly defined groups and combines multiple statistic types (F2, F3 and F4) in one run."
  },
  {
    "objectID": "fstats.html#f4-statistics",
    "href": "fstats.html#f4-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.2 F4 Statistics",
    "text": "3.2 F4 Statistics\nA different way to test for admixture is by “F4 statistics” (or “D statistics” which is very similar), also introduced in (Patterson et al. 2012).\nF4 statistics are also defined in terms of correlations of allele frequency differences, similarly to F3 statistics, but involving four different populations, not just three. Specifically we define\n\\[F4(A,B;C,D)=\\langle(a−b)(c−d)\\rangle.\\]\n\n3.2.1 Shaping intuition - the ABBA- and BABA-sites\nTo understand the statistics, consider the following tree:\n\nIn this tree, without any additional admixture, the allele frequency difference between A and B should be completely independent from the allele frequency difference between C and D. In that case, F4(A, B; C, D) should be zero, or at least not statistically different from zero. However, if there was gene flow from C or D into A or B, the statistic should be different from zero. Specifically, if the statistic is significantly negative, it implies gene flow between either C and B, or D and A. If it is significantly positive, it implies gene flow between A and C, or B and D.\nIt is helpful to again consider an example using a SNP list, this time assuming that every population is just a single (haploid) individual, so each allele frequency can just be 0 or 1. For example:\n\n\n\nSNP\nA\nB\nC\nD\n\\((a-b)(c-d)\\)\n\n\n\n\n1\n1\n0\n0\n0\n0\n\n\n2\n1\n0\n1\n1\n0\n\n\n3\n0\n1\n1\n0\n-1\n\n\n4\n0\n1\n0\n1\n1\n\n\n5\n1\n0\n0\n1\n-1\n\n\n6\n1\n0\n0\n0\n0\n\n\n\\(F_4(A, B;C, D)\\)\n\n\n\n\n-0.0167\n\n\n\nYou can see that the only SNPs that contribute positively to this statistics are SNPs where the alleles are distributed as 1010 and 0101, and the only SNPs that contribute negatively are 1001 and 0110. In the literature, the two patterns have been dubbed “ABBA” and “BABA”, which is why the statistical test behind this statistic (see below) was sometimes called the ABBA-BABA test (see for example (Martin, Davey, and Jiggins 2015)).\nThe intuition here is straight-forward: In positions that are polymorphic in both \\((A,B)\\) and \\((C,D)\\), this statistic asks whether B is genetically more similar to C than it is to D. This is most useful as a test for “treeness”: If A, B, C, D are related to each other as indicated in the above tree, then C should be equally closely related to C as to D. But if we actually find evidence that B is closer to C than to D, or vice versa, then this means that the tree above cannot be correct, but that there must be a closer connection between B and C or B and D, depending on the sign of the statistic.\n\n\n3.2.2 From single samples to allele frequencies\nSo the ABBA- and BABA-categories of SNPs help shape intuition for how this statistic behaves for single haploid genomes. But what about population allele frequencies? Looking back at the formula \\(\\langle(a−b)(c−d)\\rangle\\) this doesn’t help very much with intuition how this behaves with frequencies. Well, a nice feature of F4-Statistics is that averages factor out. This means, that if you have multiple samples in one or multiple slots A, B, C or D, the total F4-statistic of the groups is exactly equal to the average of F4-Statistics of the individuals. Here is a more mathematical definition.\nLet’s say we have 2 individuals in each of A and B, so we may perhaps write \\(A=\\{A_1,A_2\\}\\) and \\(B=\\{B_1,B_2\\}\\). Then one can show to have\n\\[F4(A, B; C, D) = \\text{Average of}[F4(A_1, B_1; C, D), F4(A_1, B_2; C, D), F4(A_2, B_1; C, D), F4(A_2, B_2; C, D)]\\]\nso just thte average over all individual-based F4-statistics. And this can be shown to be true for arbitrary numbers of samples. So in other words: An F4-Statistic always measures the average excess of pairwise BABA SNPs over ABBA SNPs. To me, this is a useful insight, as I find thinking in terms of ABBA-BABA somehow more helpful than thinking in terms of correlations of allele-frequency differences (which is really what the original formula is).\n\n\n\n\n\n\nNote\n\n\n\nF4-statistics have been famously used to show that Neanderthals are more closely related to Non-African populations than to Africans, suggesting gene-flow between Neanderthals and Non-Africans (shown in (Green et al. 2010)). You can reproduce this famous result with\n\nxerxes fstats -d ~/poseidon_repo --stat 'F4(<Chimp.REF>,<Altai_published.DG>,Yoruba,French)' \\\n  --stat 'F4(<Chimp.REF>,<Altai_published.DG>,Sardinian,French)'\n\nwhich shows that the first statistic is significantly positive with a Z-score of 7.99, while the second one is insignificantly different from zero (Z=1.01)\n\n\nThe way this statistic is often used, is to put a divergent outgroup as population A, for which we know for sure that there was no admixture into either C or D. With this setup, we can then test for gene flow between B and D (if the statistic is positive), or B and C (if it is negative).\n\n\n3.2.3 Running F4-Statistics with xerxes\nHere, we can use this statistic to test for East Asian admixture in Finns, similarly to the test using Admixture F3 statistics above. We will again use xerxes fstats. We again prepare a configuration file (in fstats_working/F4_finish.config in the git-repository of this book), this time with four populations (A, B, C, D):\nfstats:\n- type: F4\n  a: [\"Mbuti\"]\n  b: [\"Nganasan\", \"Russia_Bolshoy\"]\n  c: [\"French\", \"Icelandic\", \"Lithuanian\", \"Norwegian\"]\n  d: [\"Finnish\"]\nYou can again run via\n\nxerxes fstats -d ~/dev/poseidon-framework/community-archive --statConfig fstats_working/F4_finnish.config\n\nThe result is:\n.-----------.-------.----------------.------------.---------.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |   a   |       b        |     c      |    d    | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=======:================:============:=========:=========:================:====================:==================:====================:\n| F4        | Mbuti | Nganasan       | French     | Finnish | 593124  | 2.3114e-3      | 2.3115e-3          | 1.2676e-4        | 18.235604067907143 |\n| F4        | Mbuti | Nganasan       | Icelandic  | Finnish | 593124  | 1.6590e-3      | 1.6590e-3          | 1.4861e-4        | 11.163339072181776 |\n| F4        | Mbuti | Nganasan       | Lithuanian | Finnish | 593124  | 1.3290e-3      | 1.3290e-3          | 1.4681e-4        | 9.052979707622278  |\n| F4        | Mbuti | Nganasan       | Norwegian  | Finnish | 593124  | 1.6503e-3      | 1.6503e-3          | 1.5358e-4        | 10.745850997260929 |\n| F4        | Mbuti | Russia_Bolshoy | French     | Finnish | 542789  | 1.8785e-3      | 1.8785e-3          | 1.2646e-4        | 14.854487416366263 |\n| F4        | Mbuti | Russia_Bolshoy | Icelandic  | Finnish | 542789  | 1.0829e-3      | 1.0828e-3          | 1.4963e-4        | 7.236818881873822  |\n| F4        | Mbuti | Russia_Bolshoy | Lithuanian | Finnish | 542789  | 5.4902e-4      | 5.4907e-4          | 1.4601e-4        | 3.7605973064589096 |\n| F4        | Mbuti | Russia_Bolshoy | Norwegian  | Finnish | 542789  | 9.3473e-4      | 9.3475e-4          | 1.5302e-4        | 6.108881868125652  |\n'-----------'-------'----------------'------------'---------'---------'----------------'--------------------'------------------'--------------------'\nAs you can see, in all cases, the Z score is positive and larger than 3, indicating a significant deviation from zero, and implying gene flow between Nganasan and Finnish, and BolshoyOleniOstrov and Finnish, when compared to French, Icelandic, Lithuanian or Norwegian."
  },
  {
    "objectID": "fstats.html#outgroup-f3-statistics",
    "href": "fstats.html#outgroup-f3-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.3 Outgroup-F3-Statistics",
    "text": "3.3 Outgroup-F3-Statistics\nOutgroup F3 statistics are a special case how to use F3 statistics. The definition is the same as for Admixture F3 statistics, but instead of a target C and two source populations A and B, one now gives an outgroup C and two test populations A and B.\nTo get an intuition for this statistics, consider the following tree:\n\nIn this scenario, the statistic F3(A, B; C) measures the branch length from C to the common ancestor of A and B, coloured red. So this statistic is simply a measure of how closely two population A and B are related with each other, as measured from a distant outgroup. It is thus a similarity measure: The higher the statistic, the more genetically similar A and B are to one another.\nHere is again a SNP table to illustrate, using haploid individuals:\n\n\n\nSNP\nA\nB\nC\n\\((c-a)(c-b)\\)\n\n\n\n\n1\n1\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n\n\n3\n0\n0\n1\n1\n\n\n4\n1\n0\n1\n0\n\n\n5\n1\n1\n0\n1\n\n\n6\n0\n0\n1\n1\n\n\n\\(F_3(A, B;C)\\)\n\n\n\n0.5\n\n\n\nYou can see that each position which is similar between A and B, but different to C contributes 1, all other SNPs 0. So it directly measures similarity between A and B on alleles that differ from the outgroup C.\n\n\n\n\n\n\nNote\n\n\n\nNote that the averaging-relation shown for F4 statistics above is also true for Outgroup-F3 statistics, but only for populations A and B, not for C. So if you have multiple samples in A and B, you may think of this statistic being the average over all pairwise nucleotide similarities between individuals in A and B with respect to the same outgroup C.\n\n\nWe can use this statistic to measure for example the genetic affinity to East Asia, by performing the statistic F3(Han, X; Mbuti), where Mbuti is a distant African population and acts as outgroup here, Han denote Han Chinese, and X denotes various European populations that we want to test.\nYou can again define a configuration file that performs looping over various populations X for you:\nfstats:\n- type: F3\n  a: [\"Han\"]\n  b: [\"Chuvash\", \"Albanian\", \"Armenian\", \"Bulgarian\", \"Czech\", \"Druze\", \"English\",\n      \"Estonian\", \"Finnish\", \"French\", \"Georgian\", \"Greek\", \"Hungarian\", \"Icelandic\",\n      \"Italian_North\", \"Italian_South\", \"Lithuanian\", \"Maltese\", \"Mordovian\", \"Norwegian\",\n      \"Orcadian\", \"Russian\", \"Sardinian\", \"Scottish\", \"Sicilian\", \"Spanish_North\",\n      \"Spanish\", \"Ukrainian\", \"Finland_Levanluhta\", \"Russia_Bolshoy\", \"Russia_Chalmny_Varre\", \"Saami.DG\"]\n  c: [\"Mbuti\"]\nwhich cycles through many populations from Europe, including the ancient individuals from Chalmny Varre, Bolshoy Oleni Ostrov and Levänluhta (described in (Lamnidis et al. 2018)). We store this file in a file called fstats_working/OutgroupF3_europe.config and run via:\n\nxerxes fstats --statConfig fstats_working/OutgroupF3_europe.config -d ~/dev/poseidon-framework/community-archive -f fstats_working/outgroupf3_europe.tsv\n\n\n\n\n\n\n\nWarning\n\n\n\nOften in Outgroup-F3-statistics you use single genomes for population C, sometimes even single haploid genomes. In this case, F3 and F3star will get undefined results, because ordinary F3 and F3star statistics require population C to have non-zero average heterozygosity, so you will need at least one diploid sample, or multiple haploid or diploid samples.\nUse F3vanilla if your third population C is a single pseudo-haploid sample.\n\n\nHere is the output of this run (but note that a tab-separated version was also stored in fstats_working/outgroupf3_europe.tsv using the option -f):\n.-----------.-----.----------------------.-------.---.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |  a  |          b           |   c   | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=====:======================:=======:===:=========:================:====================:==================:====================:\n| F3        | Han | Chuvash              | Mbuti |   | 593124  | 5.3967e-2      | 5.3967e-2          | 5.0668e-4        | 106.51180329550319 |\n| F3        | Han | Albanian             | Mbuti |   | 593124  | 4.9972e-2      | 4.9973e-2          | 4.9520e-4        | 100.91326321202445 |\n| F3        | Han | Armenian             | Mbuti |   | 593124  | 4.9531e-2      | 4.9531e-2          | 4.7771e-4        | 103.68366652942314 |\n| F3        | Han | Bulgarian            | Mbuti |   | 593124  | 5.0103e-2      | 5.0103e-2          | 4.8624e-4        | 103.04188532686614 |\n| F3        | Han | Czech                | Mbuti |   | 593124  | 5.0536e-2      | 5.0536e-2          | 4.9261e-4        | 102.58792370749681 |\n| F3        | Han | Druze                | Mbuti |   | 593124  | 4.8564e-2      | 4.8564e-2          | 4.6788e-4        | 103.79674299622445 |\n| F3        | Han | English              | Mbuti |   | 593124  | 5.0280e-2      | 5.0281e-2          | 4.9183e-4        | 102.23198323949656 |\n| F3        | Han | Estonian             | Mbuti |   | 593124  | 5.1154e-2      | 5.1155e-2          | 5.0350e-4        | 101.59882496016485 |\n| F3        | Han | Finnish              | Mbuti |   | 593124  | 5.1784e-2      | 5.1784e-2          | 5.0603e-4        | 102.33488758899031 |\n| F3        | Han | French               | Mbuti |   | 593124  | 5.0207e-2      | 5.0208e-2          | 4.8552e-4        | 103.40976592749682 |\n| F3        | Han | Georgian             | Mbuti |   | 593124  | 4.9711e-2      | 4.9711e-2          | 4.8100e-4        | 103.34881140790415 |\n| F3        | Han | Greek                | Mbuti |   | 593124  | 4.9874e-2      | 4.9874e-2          | 4.8994e-4        | 101.79554640756365 |\n| F3        | Han | Hungarian            | Mbuti |   | 593124  | 5.0497e-2      | 5.0498e-2          | 4.9878e-4        | 101.24215699276706 |\n| F3        | Han | Icelandic            | Mbuti |   | 593124  | 5.0680e-2      | 5.0680e-2          | 4.9729e-4        | 101.91303336514295 |\n| F3        | Han | Italian_North        | Mbuti |   | 593124  | 4.9903e-2      | 4.9904e-2          | 4.8436e-4        | 103.03094306099203 |\n| F3        | Han | Italian_South        | Mbuti |   | 592980  | 4.9201e-2      | 4.9201e-2          | 5.1170e-4        | 96.15239597674244  |\n| F3        | Han | Lithuanian           | Mbuti |   | 593124  | 5.0896e-2      | 5.0896e-2          | 5.0638e-4        | 100.50984037418753 |\n| F3        | Han | Maltese              | Mbuti |   | 593124  | 4.8751e-2      | 4.8751e-2          | 4.7500e-4        | 102.63442479673623 |\n| F3        | Han | Mordovian            | Mbuti |   | 593124  | 5.1820e-2      | 5.1820e-2          | 4.8853e-4        | 106.07409963190884 |\n| F3        | Han | Norwegian            | Mbuti |   | 593124  | 5.0724e-2      | 5.0724e-2          | 4.9514e-4        | 102.4454387098217  |\n| F3        | Han | Orcadian             | Mbuti |   | 593124  | 5.0469e-2      | 5.0469e-2          | 4.9485e-4        | 101.98814656611475 |\n| F3        | Han | Russian              | Mbuti |   | 593124  | 5.1277e-2      | 5.1277e-2          | 4.8613e-4        | 105.48070801791317 |\n| F3        | Han | Sardinian            | Mbuti |   | 593124  | 4.9416e-2      | 4.9417e-2          | 4.8908e-4        | 101.04049389691913 |\n| F3        | Han | Scottish             | Mbuti |   | 593124  | 5.0635e-2      | 5.0635e-2          | 5.0565e-4        | 100.13962104744425 |\n| F3        | Han | Sicilian             | Mbuti |   | 593124  | 4.9194e-2      | 4.9194e-2          | 4.8157e-4        | 102.15353663091187 |\n| F3        | Han | Spanish_North        | Mbuti |   | 593124  | 5.0032e-2      | 5.0032e-2          | 4.9377e-4        | 101.32594226555439 |\n| F3        | Han | Spanish              | Mbuti |   | 593124  | 4.9693e-2      | 4.9693e-2          | 4.8551e-4        | 102.35200847948641 |\n| F3        | Han | Ukrainian            | Mbuti |   | 593124  | 5.0731e-2      | 5.0731e-2          | 4.9506e-4        | 102.47529111692852 |\n| F3        | Han | Finland_Levanluhta   | Mbuti |   | 303033  | 5.4488e-2      | 5.4488e-2          | 5.7681e-4        | 94.46487653920919  |\n| F3        | Han | Russia_Bolshoy       | Mbuti |   | 542789  | 5.7273e-2      | 5.7273e-2          | 5.2875e-4        | 108.31739594898687 |\n| F3        | Han | Russia_Chalmny_Varre | Mbuti |   | 428215  | 5.4000e-2      | 5.4000e-2          | 5.6936e-4        | 94.84371082564112  |\n| F3        | Han | Saami.DG             | Mbuti |   | 585193  | 5.4727e-2      | 5.4728e-2          | 5.5546e-4        | 98.5265149143263   |\n'-----------'-----'----------------------'-------'---'---------'----------------'--------------------'------------------'--------------------'\nNow it’s time to plot these results using R. Let’s first read in the table:\n\nd <- read.csv(\"fstats_working/outgroupf3_europe.tsv\", sep = \"\\t\")\n\nWe can check that it worked:\n\nhead(d)\n\n  Statistic   a         b     c  d NrSites Estimate_Total Estimate_Jackknife\n1        F3 Han   Chuvash Mbuti NA  593124       0.053967           0.053967\n2        F3 Han  Albanian Mbuti NA  593124       0.049972           0.049973\n3        F3 Han  Armenian Mbuti NA  593124       0.049531           0.049531\n4        F3 Han Bulgarian Mbuti NA  593124       0.050103           0.050103\n5        F3 Han     Czech Mbuti NA  593124       0.050536           0.050536\n6        F3 Han     Druze Mbuti NA  593124       0.048564           0.048564\n  StdErr_Jackknife Z_score_Jackknife\n1       0.00050668          106.5118\n2       0.00049520          100.9133\n3       0.00047771          103.6837\n4       0.00048624          103.0419\n5       0.00049261          102.5879\n6       0.00046788          103.7967\n\n\nNice, now on to plotting (here I’m using Base R for zero-dependency pain, you’re welcome!):\n\norder <- order(d$Estimate_Total) # order the estimates for visual effect\nx <- d$Estimate_Jackknife[order]\nxErr <- d$StdErr_Jackknife[order]\ny <- seq_along(d$b)\nplot(x, y, xlab = \"Z Score\", ylab = NA, yaxt = \"n\", # plot no y-axis ticks\n     xlim = c(0.048,0.06),\n     main = \"F3(Han, X; Mbuti)\")\n# plot the labels\ntext(x + 0.001, y, labels = d$b, adj=0)\n# plot the error bars\narrows(x - xErr, y, x + xErr, y, length=0.05, angle=90, code=3)\n\n\n\n\n\n\n\n\nAs expected, the ancient samples and modern Saami are the ones with the highest allele sharing with present-day East Asians (as represented by Han) compared to many other Europeans.\n\n\n\n\nGreen, Richard E, Johannes Krause, Adrian W Briggs, Tomislav Maricic, Udo Stenzel, Martin Kircher, Nick Patterson, et al. 2010. “A Draft Sequence of the Neandertal Genome.” Science 328 (5979): 710–22. http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=20448178&retmode=ref&cmd=prlinks.\n\n\nLamnidis, Thiseas C, Kerttu Majander, Choongwon Jeong, Elina Salmela, Anna Wessman, Vyacheslav Moiseyev, Valery Khartanovich, et al. 2018. “Ancient Fennoscandian Genomes Reveal Origin and Spread of Siberian Ancestry in Europe.” Nature Communications 9 (1): 5018. https://doi.org/10.1038/s41467-018-07483-5.\n\n\nMartin, Simon H, John W Davey, and Chris D Jiggins. 2015. “Evaluating the Use of ABBA-BABA Statistics to Locate Introgressed Loci.” Molecular Biology and Evolution 32 (1): 244–57. https://doi.org/10.1093/molbev/msu269.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\n\n\nPeter, Benjamin M. 2016. “Admixture, Population Structure, and F-Statistics.” Genetics 202 (4): 1485–1501. https://doi.org/10.1534/genetics.115.183913."
  },
  {
    "objectID": "authentiCT.html",
    "href": "authentiCT.html",
    "title": "4  Contamination estimation with AuthentiCT",
    "section": "",
    "text": "A number of methods are available to estimate contamination which use a variety of signals and are appropriate for different types of data. Here, we will focus on the case of ancient human samples with present-day human DNA contamination.\nThree main signals are informative about the presence of contamination in aDNA datasets:\n1. Differences in the DNA Sequence: Sites that differ between the genome of interest and likely contaminants can be identified when their genome sequences are known in advance. Contamination can be estimated by measuring the proportion of sequences that present differences at specific sites (Box A).\n\nExample tool: contamMix\n2. Deviation from the expected ploidy: Contamination can cause a sample to show unusual patterns of ploidy. For instance, heterozygous sites on the X or Y-chromosomes in males, or Y-chromosome sequences in females, are signs of contamination (Box B). \nExample tools: X/Autosome coverage ratio, ANGSD, Schmutzi\n3. Ancient DNA degradation patterns: The degradation of DNA leaves characteristic patterns that can be used to distinguish aDNA sequences from those from present-day DNA contamination. The most common damage in aDNA originates from cytosine-deamination (Box C).\nExample tool: AuthentiCT\nThe figure below summarizes the classification of signals used to estimate contamination, as presented by Peyrégné (2020)\n\n\n\nBox A shows the genealogical relationship of present-day and ancient individuals, including two derived variants that are informative for either group. The presence of a red variant indicates a contaminant sequence, whereas a blue variant indicates endogenous sequences. Box B shows the expected ploidy for the autosomes (A), the X- and Y-chromosomes (X, Y), and the mitochondrial genome (MT). Deviations from these expectations indicate contamination from the opposite sex. The illustration below shows sequences aligned to a reference genome. These sequences carry two different alleles represented by dots. However, one allele (white) is rare compared to the other allele (blue). This observation is not compatible with the 50:50 ratio expected for a heterozygous site; therefore the discordant allele may originate from contamination. Box C illustrates aDNA damage. Left: aDNA fragments often contain U caused by aDNA damage whereas U are typically absent from present-day DNA. Right: When no repair enzymes are used, U will be misread as T and their presence will result in high rates of C-to-T substitutions that occur primarily at the ends of sequences. Note that this signal depends on the library preparation protocol, and that high rates of G-to-A toward the 3ʹ-end. Neither pattern is expected for present-day DNA sequences .\n\n\n\n\nAncient DNA is typically fragmented into pieces shorter than 100 bp and exhibits miscoding base modifications that accumulate over time.\nThe most common miscoding lesions observed in aDNA are the results of cytosine deamination that converts cytosine (C) into uracil (U), which is then misread as thymine (T), or 5- methylcytosine into thymine. This leads to C-to-T substitutions in the sequence data.\n\n\n\n\n\n\n\nThe deamination rate is higher for single-stranded DNA than for double-stranded DNA (Lindahl and Nyberg 1974). The prevalence of deamination-induced C-T substitutions increases with the age of the sample, and it is climate dependent (Kistler et al. 2017; Sawyer et al. 2012) .\n💡 To estimate present-day DNA contamination, these properties need to be formalised in a model of aDNA damage..\n\n\n\n1. Conditional substitution model (based on last positions)\nAssumes independence between C-to-T substitutions at both ends (no correlation).\nCytosine deamination is used to filter out present-day human contaminant DNA, as this type of deamination occurs less frequently in such DNA. To estimate contamination, patterns of ‘regular’ substitution are compared with ‘conditional’ substitution patterns. The frequency of conditional substitutions is calculated by identifying fragments that show a C-T change at either the 5’ or 3’ end and then determining the C-T substitution rate at the opposite end. Frequencies derived from conditional substitution are presumed to be higher than those from ‘regular’ substitution. This method is used as a ‘conservative’ proxy to estimate the expected deamination in endogenous DNA. It was introduced by Meyer et al. (2016)\nLet’s say we have sample X with damage rate of 30% at the 5’ end and 25% on the 3’ end, as illustrated below:\n\n\n\n\n\nTo calculate the conditional substitution rate at the 5’ end, we start by selecting only those reads that have a deamination signal at the 3’ end. hence maximizing it to a 100% rate. Similarly, to find the substitution rate at the 3’ end, we pick out reads that show a deamination signal at the 5’ end, aiming for the same 100% rate. The process is depicted in the figure below.\n\nThe table below summarizes the output of the calculation:\n\n\n\n\nregular substitution\nconditional substitution\n\n\n\n\n5’ end\n30\n30\n\n\n3’ end\n25\n28\n\n\n\nIf conditional substitution rate is larger than regular substitution rate at both ends, this could indicate presence of contamination. This is due to the background of present-day contamination that dilutes the deamination signal.\nExample\nIn Meyer et al. 2016, nuclear DNA sequences from femur XIII, a sample of the Sima de los Huesos (SH femur XIII) Middle Pleistocene hominins, showed regular C to T substitution frequencies of 12% and 17% at the 5’ and 3’ ends, respectively. When conditioned on C to T substitutions at the opposite ends of fragments, these frequencies dramatically increased to 55% and 62%, indicating the specimen contains a mixture of highly deaminated endogenous nuclear DNA and less deaminated human contamination. The output is outlined in the table below:\n\nA custom script to calculate the conditional substitution rate can be requested from Matthias Meyer.\n\n\n\nregular substitution\nconditional substitution\n\n\n\n\n5’ end\n17\n62\n\n\n3’ end\n12\n55\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe conditional substitution rate does not provide a robust estimate; instead, it offers an indication of contamination. Therefore, it is considered as qualitative rather than quantitative method.\n\n\n2. Non-conditional substitution model (based on pattern)\nUnlike the conditional substitution method, which looks at two point estimates, the non-conditional substitution model—on which the AuthentiCT tool is based—relies on a complex model that learns the patterns of post-mortem damage.\nAuthentiCT follows that model ⤵️"
  },
  {
    "objectID": "authentiCT.html#authentict-tool",
    "href": "authentiCT.html#authentict-tool",
    "title": "4  Contamination estimation with AuthentiCT",
    "section": "4.2 AuthentiCT tool",
    "text": "4.2 AuthentiCT tool\nAuthentiCT is based on model that learns the patterns of post-mortem damage. It models all C-T substitutions, irrespective of their position in the sequence, and accounts for clusters of C-T substitutions in the internal parts of sequences.\n\n4.2.1 Theory\nHow it works?\n- Modeling C-to-T Substitutions: AuthentiCT models all C-T substitutions across the DNA sequence. It does not limit this analysis to substitutions near the ends of DNA sequences, where such changes are most expected, but also considers them throughout the sequence.\n- Accounting for Clusters: While C-T substitutions occur predominantly at the ends of DNA fragments, they are also found in the internal parts. These internal clusters are not independent from each other.\n\n\n\nExcluding the first and last five bases to mask potential overhangs, C-to-Ts are found to particularly common in adjacent positions in many samples, with a significant deviation from the geometric distribution expected from independent events [Peyrégne and Peter 2020]\n\n\nAuthentiCT uses a a hidden Markov model (HMM)—a statistical model for predicting a sequence of unseen state changes based on observed sequences—to jointly model all C-T substitutions.\n- Observation: This refers to matches and mismatches to the reference, including deaminations, errors, and polymorphisms.\n- Unobserved states: These are the DNA conditions not directly observed, such as single-stranded areas (including 3’ or 5’ overhangs, haplotypes) and double-stranded regions.\n\n\n\n\n\nAfter evaluating each position within the DNA sequence, the model identifies three distinct single-stranded (ss) regions: inside the molecule, at the 5’ end, or at the 3’ end. It then calculates the probability for each model (contaminant vs. ancient) at the sequence’s end.\nThe model distinguishes between single-stranded and double-stranded parts of the DNA molecule using four hidden states to represent either double-stranded (ds) or single-stranded stretches. By compiling these probabilities across all sequences, AuthentiCT efficiently estimates the proportion of contamination.\n\n\n\nStates are depicted by nodes and transitions by edges. Each state emits a match to the reference M (blue) or a mismatch, which can either be compatible with cytosine deamination, D (red), or an error (or polymorphism), E (yellow). Single-stranded states (5’o, 3’o and ss) and the double-stranded state (ds) are in light and dark green, respectively. b The posterior probability for each state is shown with different colours [Peyrégne & Peter 2020]\n\n\n\n\n4.2.2 Applicability\n\n\n\n\n\n\nTip\n\n\n\n\nAuthentiCT is applicable to any species, if suitable reference genome is available for alignment.\nLimited to single-stranded libraries\nPerforms well for datasets of 10,000 or more sequences (less reliable for <1000) - bias decreases with longer sequences or higher GC contents.\nNot applicable to libraries generated after treatments that alter deamination patterns, e.g., UDG.\nIt is not possible to run it with PMD-filtered data, since such data lacks the non-deaminated reads.\n\n\n\n\n\n4.2.3 Method parameters\n\nAuthentiCT overestimates contamination for low contamination rates.\nConsistency between Shotgun (SG) vs capture (1240k) data.\n\n\n\nAssumes the absence of significant levels of deamination in the contaminating DNA, which can lead to underestimating the proportion of contamination.\nDNA fragments with different rates of damaged bases (differences in preservation, different treatments), may lead to an overestimate of present-day DNA contamination.\n\n\n\n4.2.4 Running & Output\nTo install AuthentiCT, follow the instructions available at AuthentiCT GitHub Repository.\n\nPython Version: Requires Python 3.6 or higher.\n\nEnsure the following dependencies are installed:\n\nnumpy (version 1.17.3)\nnumdifftools (version 0.9.39)\npandas (version 0.25.2)\nscipy (version 1.3.1)\n\nMain commands:\n\ndeam2cont: Estimates the level of contamination from deamination patterns.\ndeamination: Prints C-to-T substitution frequencies.\n\nScript\n#!/usr/bin/env bash\n\nBAM=LGJ001_ss.A0101_rmdup #file name (deduplicated bam)\n\nsamtools view $BAM.bam | AuthentiCT deam2cont -o $BAM.deam2cont.out -m 25 -b 30 -\n\n# -m  mapping quality cutoff\n# -b  base quality cutoff\nor for multiple bam files\n\n# Directory containing BAM files\nBAM_DIR=\"/path/to/your/deduplicated_bam/directory\"\n\n# Iterate over all BAM files in the directory\nfor BAM_FILE in \"$BAM_DIR\"/*.bam; do\n# Extract the file name without the extension\n    BAM_BASE=$(basename \"${BAM_FILE%.bam}\")\n\n# Run the samtools view and AuthentiCT commands for each BAM file\n  samtools view \"$BAM_FILE\" | AuthentiCT deam2cont -o \"$BAM_BASE.deam2cont.out\" -m 25 -b 30 -\ndone\n\n# -m    mapping quality cutoff\n# -b    base quality cutoff\nOutput example\n|       | Parameter    | Std.Err      | Output Explanation                                                                              |\n|------------------|------------------|------------------|-------------------|\n| e     | 0.003659     | 0.000079     | Error rate: we expect around 1 or 2 mismatches every 1000                                       |\n| rss   | 0.663170     | 0.016033     | Rate of C-to-T substitutions in single-stranded regions \\> damage rate                          |\n| lo    | 0.732971     | 0.003014     | Parameter of the geometric distribution modeling the length of single-stranded overhang         |\n| lss   | 0.727519     | 0.036484     | Parameter of the geometric distribution modeling the length of internal single-stranded regions |\n| lds   | 0.001000     | 0.000091     | Parameter of the geometric distribution modeling the length of double-stranded regions          |\n| contm | 0.098113    | 0.011459 | Contamination estimate (rate from 0 to 1)                                                   |\n| o     | 0.626460     | 0.018110     | Frequency of 5' single-stranded overhangs                                                       |\n| o2    | 0.657778     | 0.020543     | o2 is proportional to the frequency of 3' single-stranded overhangs                             |"
  },
  {
    "objectID": "authentiCT.html#practical-examples",
    "href": "authentiCT.html#practical-examples",
    "title": "4  Contamination estimation with AuthentiCT",
    "section": "4.3 Practical examples",
    "text": "4.3 Practical examples\n\n4.3.1 Consistency between Shotgun (SG) vs capture (1240k) data\nAim: We evaluated the consistency of AuthentiCT estimates between shotgun and 1240k-captured data derived from the same samples.\nDataset: consists of 1240k-captured sequences from individuals dating back to 3000 BP, showing low levels of contamination.\nOutcome: We did not observe high discrepancies when comparing samples with high number of input reads and C-T damage (>30%). However, we observed higher SE for samples with low number of input reads (less than 10.000), mainly for SG data. The table below outlines the coverage and the C-T substitution rates for each samples.\n\n\n\nPlot comparing AuthentiCT output for SG and capture (1240k) data, showing 2x standard error. We did not observe high discrepancies when comparing samples with high number of input reads and C-T damage (>30%). However, we observed higher SE for samples with low number of input reads (less than 10.000), mainly for SG data. Data used are shown in table below.\n\n\n\n| Seq ID | SG_Nr. Mapped Reads Passed Post-Filter | SG_Endogenous DNA Post (%) | SG_5 Prime C>T 1st base | TF_Nr. Mapped Reads Passed Post-Filter | TF_Endogenous DNA Post (%) | TF_5 Prime C>T 1st base |\n|--------|-----------------------------------------|----------------------------|-------------------------|-----------------------------------------|----------------------------|-------------------------|\n| 002.A  | 1,173,469                               | 21.38                       | 31.60%                  | 19,718,426                              | 65.53                      | 32.80%                  |\n| 003.A  | 85,840                                  | 1.58                        | 40.10%                  | 13,387,538                              | 40.04                      | 40.90%                  |\n| 005.B  | 67,131                                  | 1.51                        | 56.40%                  | 16,556,830                              | 3.52                       | 55.40%                  |\n| 006.B  | 1,896                                   | 0.05                        | 47.80%                  | 33,820                                  | 0.09                       | 51.00%                  |\n| 007.B  | 916,024                                 | 20.49                       | 41.40%                  | 13,232,270                              | 36.93                      | 39.90%                  |\n| 008.B  | 5,595                                   | 0.13                        | 57.00%                  | 74,134                                  | 0.24                       | 59.20%                  |\n| 010.A  | 36,098                                  | 1.06                        | 43.60%                  | 12,425,284                              | 34.83                      | 45.30%                  |\n| 012.A  | 522,067                                 | 10.01                       | 42.00%                  | 17,379,143                              | 52.79                      | 41.90%                  |\n| A001.A | 2,626                                   | 0.05                        | 39.40%                  | 43,414                                  | 0.11                       | 41.00%                  |\n| A005.B | 72,628                                  | 1.69                        | 38.50%                  | 3,624,986                               | 8.99                       | 38.50%                  |\n| A008.C | 80,062                                  | 1.74                        | 36.40%                  | 2,958,958                               | 7.87                       | 35.40%                  |\n| A010.A | 12,212                                  | 0.28                        | 52.00%                  | 198,887                                 | 0.63                       | 50.70%                  |\n| A015.A | 3,440                                   | 0.06                        | 38.00%                  | 64,418                                  | 0.16                       | 45.60%                  |\n\n\n4.3.2 Impact of sequence count used on AuthentiCT estimates\nAim: We evaluated whether lowering the sequence count for the deamination model in AuthentiCT’s -s option from the default 100,000 to 10,000 sequences—which speeds up the process—alters contamination estimates.\nDataset: Our analysis used 1240k-captured sequences from individuals dating back to the first millennium BC, varying in contamination levels.\nOutcome: Our results indicate that a reduced sequence count speeds up analysis but may broaden standard errors and slightly alter contamination estimates.\n\n\n\n\n\n\n\n4.3.3 Impact of sequencing setup used on AuthentiCT estimates\nAim and dataset: We explored AuthentiCT estimates in samples that were single-end (SE) sequenced and paired-end (PE) sequenced. The mean read length for all the samples was between 45 to 50 bp (insert size), and the sequencing read length was 75 bp in both the SE and PE setups. The adapters were trimmed using AdpaterRemoval tool.\nOutcome: The results indicate that AuthentiCT estimate ovaerall does not change with changing the sequencing setup.\n\n\n\n\n\n💡 Terminal end damage discrepancies in single-stranded libraries\nDuring the preparation of single-stranded libraries, an adapter is ligated to the 3’ ends of the molecules. This step tends to introduce a small to moderate bias against the ligation of uracils, leading to a common observation: the terminal 3’ C-to-T substitution rates are typically 10-20% lower compared to those at the 5’ terminal ends.This bias is amplified in the presence of inhibitory substances in the DNA extracts, or if the library prep is saturated with large amounts of input DNA.\n\n\n\n\n\n\nNote\n\n\n\nAuthentiCT compensates for these variances by accounting for the differences in the length of the single-stranded end and the frequency of this occurrence.\n\n\n\n\n\n\nKistler, Logan, Roselyn Ware, Oliver Smith, Matthew Collins, and Robin G. Allaby. 2017. “A New Model for Ancient DNA Decay Based on Paleogenomic Meta-Analysis.” Nucleic Acids Research 45 (11): 6310–20. https://doi.org/10.1093/nar/gkx361.\n\n\nLindahl, Tomas, and Barbro Nyberg. 1974. “Heat-Induced Deamination of Cytosine Residues in Deoxyribonucleic Acid.” Biochemistry 13 (16): 3405–10. https://doi.org/10.1021/bi00713a035.\n\n\nMeyer, Matthias, Juan-Luis Arsuaga, Cesare de Filippo, Sarah Nagel, Ayinuer Aximu-Petri, Birgit Nickel, Ignacio Martínez, et al. 2016. “Nuclear DNA Sequences from the Middle Pleistocene Sima de Los Huesos Hominins.” Nature 531 (7595): 504–7. https://doi.org/10.1038/nature17405.\n\n\nPeyrégné, Someone. 2020. “Title of the Article.” Journal Name 10: 100–110. https://doi.org/10.1000/j.journal.2020.01.001.\n\n\nSawyer, Susanna, Johannes Krause, Katerina Guschanski, Vincent Savolainen, and Svante Pääbo. 2012. “Temporal Patterns of Nucleotide Misincorporations and DNA Fragmentation in Ancient DNA.” Edited by Carles Lalueza-Fox. PLoS ONE 7 (3): e34131. https://doi.org/10.1371/journal.pone.0034131."
  },
  {
    "objectID": "eager.html#why-do-we-need-nf-coreeager",
    "href": "eager.html#why-do-we-need-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.1 Why do we need nf-core/eager?",
    "text": "5.1 Why do we need nf-core/eager?\n\nCompared to other Next-Generation Sequencing data, the chemical structure and increased risk of present-day contamination require methods specialized for ancient DNA in both the wet and the dry lab.\nEnsuring the authenticity of ancient genomic data is one of the main focuses of bioinformatic tools developed for the study of ancient DNA and underlines the necessity of reproducibility of results.\nWith an increasing number of laboratories contributing to the field, the available computational resources and previous bioinformatic experience varies greatly. To increase accessibility, newly developed tools should be adaptable to different environments, efficient, consistently maintained and well-documented."
  },
  {
    "objectID": "eager.html#what-can-nf-coreeager-do",
    "href": "eager.html#what-can-nf-coreeager-do",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.2 What can nf-core/eager do?",
    "text": "5.2 What can nf-core/eager do?\nnf-core/eager streamlines the initial steps of ancient DNA analysis from FASTQ files after sequencing to variant calling (Fellows Yates et al. 2021).\n\nPreprocessing:\n\nFastQC (sequencing quality control)\nAdapterRemoval2/fastp (sequencing artifact clean-up)\n\nMapping:\n\nBWA aln/BWA mem/CircularMapper/Bowtie2 (alignment)\nSAMtools (mapping quality filtering)\nPicard MarkDuplicates/DeDup (PCR duplicate removal)\nSAMtools/PreSeq/Qualimap2/BEDtools/Sex.DetERRmine/EndorSpy/MtNucRatio (mapping statistics)\n\naDNA evaluation:\n\nDamageProfiler/mapDamage2 (damage assessment)\nPMDtools (aDNA read selection)\nmapDamage2/Bamutils (damage removal)\nANGSD (human contamination estimation)\nBBduk/HOPS/Kraken & Kraken Parse/MALT & MaltExtract (metagenomic screening)\n\nVariant calling: GATK UnifiedGenotyper & HaplotypeCaller/sequenceTools pileupCaller/VCF2Genome/MultiVCFAnalyzer/freebayes/ANGSD\nReport generation: MultiQC (summarize all generated statistics)"
  },
  {
    "objectID": "eager.html#how-do-i-use-nf-coreeager",
    "href": "eager.html#how-do-i-use-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.3 How do I use nf-core/eager?",
    "text": "5.3 How do I use nf-core/eager?\n\n5.3.1 Installation\nYou need:\n\na Unix machine (HPC-cluster or a computer running Linux or MacOS)\nan installation of docker or apptainer (formerly known as singularity) or conda, java and nextflow (e.g. via conda)\ninternet connection\n\nDownload the latest version of nf-core/eager\nnextflow pull nf-core/eager \n# for a specific version\nnextflow pull nf-core/eager -r 2.5.0\nRun a test specifying your choice of conda, docker or singularity\nnextflow run nf-core/eager -r 2.5.0 -profile test_tsv,docker\nTo optimize the use of available clusters, queues and resources, check if a Nextflow pipeline configuration is already available for your institution or computing environment. If not, prepare a custom profile tailored to your computational resources and setup. These are then added as a profile.\nnextflow run nf-core/eager -r 2.5.0 -profile test_tsv,eva #for MPI-EVA\n\n\n5.3.2 Input preparation\nYou can run nf-core/eager by providing either a path to fastq or bam files or a path to a tab-separated table of input data to --input. For a large number of samples and convenience, a tsv is usually the preferred option. Using a tsv input also allows for merging of different files (e.g. different libraries, different UDG treatments, etc.) at different stages of the pipeline.\n\n\n\nPipeline stages and merging steps performed for a single sample with different libraries and different UDG treatments.\n\n\nA tsv input file contains the following columns, detailing the name of the sample, library, sequencing lane, colour chemistry depending on the sequencer used, target organism, library strandedness, UDG treatment, path to fastq with forward reads (SE and PE), path to reverse reads (only PE), path to bam (optional). nf-core/eager will treat the data according to the provided information, e.g. only trim UDG half data and genotype single-stranded libraries using single-stranded mode.\nSample_Name Library_ID Lane Colour_Chemistry SeqType Organism Strandedness UDG_Treatment R1                                                                                                                                  R2                                                                                                                                  BAM\nJK2782      JK2782     1    4                PE      Mammoth  single       half          https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2782_TGGCCGATCAACGA_L008_R1_001.fastq.gz.tengrand.fq.gz https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2782_TGGCCGATCAACGA_L008_R2_001.fastq.gz.tengrand.fq.gz NA\nJK2802      JK2802     2    2                SE      Mammoth  double       full          NA                                                                                                                                  NA                                                                                                                                  https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2802_AGAATAACCTACCA_L008_R1_001.fastq.gz.tengrand.bam\nCollecting and double-checking this information is time consuming, but crucial!\nIf you realize you have different libraries from same individual, you should enter the same Sample_Name for all respective libraries. nf-core/eager will then produce all steps for the independent libraries (e.g. endogenous DNA, sequencing quality control, contamination estimation, etc.), but merge the deduplicated bam files before genotyping, genetic sex estimation and coverage calculation. To avoid re-mapping the whole dataset and conserve computing resources, also consider providing the mapped bam files to nf-core/eager directly.\n\n\n\n\n\n\nTip\n\n\n\nAt DAG, we can take advantage of all the information entered in Pandora to produce a eager-ready tsv with pandora2eager.\n\n\n\n\n5.3.3 Parameter customization\nBy default nf-core/eager runs the following, when you only provide input data and a reference genome:\nnextflow run nf-core/eager --input <INPUT>.tsv --fasta '<REFERENCE>.fasta' -profile eva\n\nPreprocessing:\n\nFastQC (sequencing quality control)\nAdapterRemoval2 (sequencing artifact clean-up)\n\nMapping:\n\nBWA aln (alignment)\nPicard MarkDuplicates (PCR duplicate removal)\nSAMtools/PreSeq/Qualimap2/EndorSpy (mapping statistics)\n\naDNA evaluation: DamageProfiler (damage assessment)\nReport generation: MultiQC (summarize all generated statistics)\n\nThe most direct way to add analysis steps (e.g. turn on genotyping) or change settings (e.g. shorter read length cut-off) is to add more parameters to the command line, in this case --run_genotyping --genotyping_tool pileupcaller and --clip_readlength 25, respectively. However, this gets cumbersome for the rather extensive workflows we usually employ for human aDNA analysis, including read trimming based on UDG treatment, genetic sex estimation, human nuclear contamination estimation, mitochondrial to nuclear ratio estimation and genotyping.\nBut the power of nf-core/eager lies in its adaptability to your specific analysis needs and the possibility to ‘remember’ your favorite settings with a personal configuration file. This separate file can contain all parameters for your required tools, as well as custom computational resource requests. For 1240K capture data, a profile mapping to the hs37d5 reference genome with genotyping could look like this:\nprofiles{\n  TF_hs37 { #name of the profile\n    params {\n        config_profile_description = \"human 1240K data hs37d5 + genotyping\"\n        config_profile_contact = \"Selina Carlhoff (@scarlhoff)\"\n        email = \"selina_carlhoff@eva.mpg.de\"\n        snpcapture_bed = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        fasta = \"/PATH/hs37d5/hs37d5.fa\"\n        fasta_index = \"/PATH/hs37d5/hs37d5.fa.fai\"\n        bwa_index = \"/PATH/hs37d5/\"\n        skip_preseq = true\n        clip_readlength = 30\n        preserve5p = true\n        bwaalnn = 0.01\n        bwaalnl = 16500\n        run_bam_filtering = true\n        bam_mapping_quality_threshold = 30\n        bam_filter_minreadlength = 30\n        bam_unmapped_type = \"discard\"\n        run_trim_bam = true\n        bamutils_clip_double_stranded_half_udg_left = 2\n        bamutils_clip_double_stranded_half_udg_right = 2\n        bamutils_clip_single_stranded_none_udg_left = 0\n        bamutils_clip_single_stranded_none_udg_right = 0\n        run_genotyping = true\n        genotyping_tool = \"pileupcaller\"\n        genotyping_source = \"trimmed\"\n        pileupcaller_bedfile = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        pileupcaller_snpfile = \"/PATH/1240K.snp\"\n        run_mtnucratio = true\n        mtnucratio_header = \"MT\"\n        run_sexdeterrmine = true\n        sexdeterrmine_bedfile = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        run_nuclear_contamination = true\n        contamination_chrom_name = \"X\"\n    }\n    process {\n    maxRetries = 2\n        withName:bwa {\n        time = { task.attempt == 3 ? 1440.h : task.attempt == 2 ? 72.h : 48.h }\n        }\n        withName:markduplicates {\n        memory = { task.attempt == 3 ? 16.GB : task.attempt == 2 ? 8.GB : 4.GB }\n        }\n        withName: mtnucratio {\n            memory = '10.G'\n            time = '24.h'\n        }\n    }\n  }\n}\nThe configuration file is then provided to nf-core/eager via the -profile and -c flag.\nnextflow run nf-core/eager -–input <INPUT>.tsv -profile TF_hs37,eva,archgen -c /<PATH>/eager2.config\nFull documentation of all parameters is available on the nf-core/eager website.\n\n\n\n\n\n\nTip\n\n\n\nThe standardised parameters for the DAG automated pipeline can be found at /mnt/archgen/Autorun_eager/conf/Autorun.config.\n\n\n\n\n5.3.4 Run submission\nOnce all input files and parameters are prepared, you are ready for submission. To make sure that the workflow continues running when you disconnect from the cluster or shut down your computer, nf-core/eager should be run in a screen session.\n# create a screen session\nscreen -R eager\n# submit nf-core/eager run\nnextflow run nf-core/eager –input <INPUT>.tsv -profile <YOUR_PROFILE> -c /<PATH>/<YOUR_CONFIG>.config\n# disconnect from screen session by pressing Ctrl+A+D\n# reconnect to screen session\nscreen -r eager\n# end screen session after successful pipeline execution\nexit\nAfter submitting a command specifying all the parameters you would like to use, Nextflow generates the corresponding shell scripts and submits each job to your scheduler according to your requested computational resources. You can track the execution status live in the terminal or on Nextflow Tower. For use with tower, you should assign the run an identifiable name with -name and activate tracking using -with-tower.\n\n\n5.3.5 Output\nDuring the progression of the run, the results of each pipeline steps are collected in separate output directories. These contain the raw outputs of each tool, including any generated files (e.g. deduplicated bam files or genotypes).\n<RUNNAME>/\n- results/\n  - adapterremoval/\n  - damageprofiler/\n  - deduplication/\n  - documentation/\n  - endorspy/\n  - fastqc/\n  - genotyping/\n  - lanemerging/\n  - mapping/\n  - merged_bams/\n  - multiqc/\n  - nuclear_contamination/\n  - pipeline_info/\n  - qualimap/\n  - reference_genome/\n  - samtools/\n  - sex_determination/\n  - trimmed_bam/\n- work/\nBut as a first overview, we want to look at the summary of all statistics aggregated in multiqc/multiqc_report.html.\n\n\n\nScreenshot of the top section of a MultiQC report\n\n\nThis table collects the output from all tools, so you can get an overview of sequenced reads per sequencing run, endogenous DNA per library, covered SNPs per sample and much more. You can also inspect and export crucial plots, such as read length distribution and damage profile. The end of the report also contains a list of all software versions and an overview of which profiles were used.\n\n\n\nAncient DNA damage plot as generated by MultiQC\n\n\n\n\n5.3.6 Trouble shooting\nA common issue with nf-core/eager, especially when used in combination with the SGE scheduler, are memory issues with java-driven tools, e.g. MarkDuplicates. Sometimes the pipeline does not catch cases properly, where the allocated memory is exceeded, and the job keeps running instead of being re-submitted with larger memory allocation. Therefore, if you notice jobs running much longer than expected, it is worth checking the work/ directory, where all information about each submitted job is recorded. Each job is assigned a randomly generated name using numbers and letter which you can identify from the log printed in to your screen, the <RUNNAME>/.nextflow.log or Nextflow tower. Each work directory contains files tracing the execution of the job.\n<RUNNAME>/\n- work/\n  - <WORKDIRECTORY>/\n    - .command.sh # exact command run for this tool\n    - .command.run # exact command submitted to the scheduler\n    - .command.log # any messages during the execution\n    - .command.err # any error messages\n    - .command.out # any output messages\n    - .command.trace # assigned computational resources\nIf you spot java.lang.OutOfMemoryError: unable to create new native thread in .command.log or command.err, you can delete the individual job from the scheduling queue. It will be re-submitted automatically with larger memory allocation.\nIf you’ve had an issue with a run or want to restart the pipeline, you can do so using -resume. Nextflow will used cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. You can also supply a run name to resume a specific run: -resume [run-name]. Use the nextflow log command to show previous run names."
  },
  {
    "objectID": "eager.html#how-do-i-report-the-usage-of-nf-coreeager",
    "href": "eager.html#how-do-i-report-the-usage-of-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.4 How do I report the usage of nf-core/eager?",
    "text": "5.4 How do I report the usage of nf-core/eager?\nIf you use nf-core/eager for your analysis, please cite Fellows Yates et al. (2021) and the release of nf-core/eager on zenodo, as well as the nf-core publication (Ewels et al. 2020). As nf-core/eager is only the pipeline connecting multiple tools, please also cite the version of each used tool, the respective individual publications and add the full command for easy reproducibility.\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\nEwels, Philip A., Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso, and Sven Nahnsen. 2020. “The Nf-Core Framework for Community-Curated Bioinformatics Pipelines.” Nature Biotechnology 38 (3): 276–78. https://doi.org/10.1038/s41587-020-0439-x.\nnextflow run nf-core/eager\n        -profile eva,archgen\n        -r 2.4.0\n        --input <INPUT>.tsv\n        --min_adap_overlap 1\n        --clip_readlength 30\n        --clip_min_read_quality 20\n        --preserve5p\n        --mapper bwaaln\n        --bwaalnnn 0.01\n        --bwaalno 2\n        --run_bam_filtering true\n        --bam_mapping_quality_threshold 30\n        --bam_filter_minreadlength 30\n        --bam_unmapped_type discard\n        --dedupper markduplicates\n        --damageprofiler_length 100\n        --damageprofiler_threshold 15\n        --damageprofiler_yaxis 0.3"
  },
  {
    "objectID": "eager.html#how-do-i-update-nf-coreeager",
    "href": "eager.html#how-do-i-update-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.5 How do I update nf-core/eager?",
    "text": "5.5 How do I update nf-core/eager?\nnextflow pull nf-core/eager\n#or for a specific version\nnextflow pull nf-core/eager -r 2.5.0"
  },
  {
    "objectID": "eager.html#whats-next-for-nf-coreeager",
    "href": "eager.html#whats-next-for-nf-coreeager",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.6 What’s next for nf-core/eager?",
    "text": "5.6 What’s next for nf-core/eager?\nWhile nf-core/eager 2.5.0 has only been released recently, behind the scenes the development team has been very busy re-writing nf-core/eager to be more efficient and include even more functionality. So look out for nf-core/eager 3.0 release sometime soon!\n\n\n\nOverview of the development status of nf-core/eager 3.0 as of October 2023, new functionality marked in purple."
  },
  {
    "objectID": "eager.html#how-can-i-get-help-with-problems-or-questions",
    "href": "eager.html#how-can-i-get-help-with-problems-or-questions",
    "title": "5  Introduction to nf-core/eager",
    "section": "5.7 How can I get help with problems or questions?",
    "text": "5.7 How can I get help with problems or questions?\nCheck the website for in-depth documentation of nf-core/eager.\n Raise your issue on GitHub.\nJoin the #eager channel on the nf-core Slack."
  },
  {
    "objectID": "fst.html",
    "href": "fst.html",
    "title": "6  Measuring population structure using Fst",
    "section": "",
    "text": "Genetic drift is the process by which allele frequencies change randomly due to random fluctuations. Various models exist to model such fluctuations, but the most widely used one is the Wright-Fisher model. In that model, a parent generation of N individuals produces exactly N individuals as offspring, which make up the next generation. To model the fluctuations, every “child” gets assigned a random “parent” from the previous generation.\nExample: With \\(N=100\\) (haploid individuals), we might consider a genetic locus with two alleles \\(A\\) and \\(B\\), and in the parent generation, say, 50 individuals carried allele B, and 50 carried allele B. Then, the number of individuals carrying B in the next generation is the number of children who get assigned a parent with allele B. These will be close to 50, but not exactly, due to noise.\nAs a statistical process, this amounts to a binomial process, where the N offspring individuals are drawn, each carrying a 50% probability to have A vs. B. In the third generation, this probability may then have already shifted away from 50.\nHere is a simple function in R to model the allele frequency in a number of \\(g\\) successive generations, given a (haploid) population size \\(n\\) and a starting frequency \\(x0\\):\n\nwfsim <- function(n, g, x0) {\n  res <- numeric(g + 1)\n  res[1] <- x0\n  for (i in 2:(g + 1)) {\n    res[i] <- (rbinom(1, n, res[i - 1])) / n\n  }\n  return(res)\n}\n\nWe can test it:\n\nset.seed(1)\nwfsim(100, 10, 0.5)\n\n [1] 0.50 0.52 0.56 0.46 0.45 0.47 0.48 0.61 0.60 0.58 0.61\n\n\nSo indeed the allele frequency changes randomly. We can visualise it for more generations:\n\ntime_series <- wfsim(100, 100, 0.5)\nplot(time_series, type = \"l\", ylim = c(0, 1),\n     xlab = \"generation\", ylab = \"allele frequency\")\n\n\n\n\nWe can better understand this random process, by simulating it many times and plotting the results together:\n\ngens <- 100\nsims100 <- replicate(50, wfsim(100, gens, 0.5))\nmatplot(sims100, type = \"l\", lty = 1, col = \"black\",\n        ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\")\n\n\n\n\nThis shows how the variance increases with time, and eventually more and more of these curves get absorbed at either \\(x=0\\) or \\(x=1\\), a process called “fixation”.\nHow does this process depend on the population size? We can take a look. Here are three families of simulations, with three different population sizes:\n\ngens <- 1000\nsims100 <- replicate(50, wfsim(100, gens, 0.5))\npar(mfrow = c(1, 3))\nmatplot(sims100, type = \"l\", lty = 1, col = \"black\",\n        xlim = c(0, 100), ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\", main = \"N = 100\")\n\nsims1000 <- replicate(50, wfsim(1000, gens, 0.5))\nmatplot(sims1000, type = \"l\", lty = 1, col = \"black\",\n        xlim = c(0, 100), ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\", main = \"N = 1000\")\n\nsims10000 <- replicate(50, wfsim(10000, gens, 0.5))\nmatplot(sims10000, type = \"l\", lty = 1, col = \"black\",\n        xlim = c(0, 100), ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\", main = \"N = 10000\")\n\n\n\n\nThis shows that larger populations have weaker fluctuations than small populations.\n\n\n\nTo quantify genetic drift, we can measure the variance of this process over time. The following plot uses the same data as shown above and estimates the variance:\n\npar(mfrow = c(1, 3))\nplot(apply(sims100,   1, var), type = \"l\", ylim = c(0, 0.25),\n     xlab = \"generations\", ylab = \"Variance\", main = \"N = 100\")\nplot(apply(sims1000,  1, var), type = \"l\", ylim = c(0, 0.25),\n     xlab = \"generations\", ylab = \"Variance\", main = \"N = 1000\")\nplot(apply(sims10000, 1, var), type = \"l\", ylim = c(0, 0.25),\n     xlab = \"generations\", ylab = \"Variance\", main = \"N = 10000\")\n\n\n\n\nso an increasing variance. But it doesn’t go up forever, but reaches a plateau. This is because of fixation: Once all curves reach fixaton at either \\(x=0\\) or \\(x=1\\), variance does no longer increase. In fact, the maximum variance corresponds to the state where all curves have been fixed. The variance to that state corresponds to the variance of a Bernoulli-process, which is \\(x_0(1-x_0)\\), so it depends on the starting frequency.\nIt is this plateau of the variance that defines \\(F_\\text{ST}=1\\)! Here is an illustration using again three families of simulations, but this time with the same population size but different starting frequencies:\n\nsims_x05 <- replicate(1000, wfsim(100, gens, 0.5))\nsims_x03 <- replicate(1000, wfsim(100, gens, 0.3))\nsims_x02 <- replicate(1000, wfsim(100, gens, 0.2))\n\nplot_dat <- cbind(\n  apply(sims_x05, 1, var),\n  apply(sims_x03, 1, var),\n  apply(sims_x02, 1, var)\n)\n\npar(mfrow = c(1, 1))\n\ncols <- c(\"blue\", \"red\", \"green\")\nmatplot(plot_dat, type = \"l\", ylim = c(0, 0.25), lty = 1,\n             xlab = \"generations\", ylab = \"Variance\", col = cols)\nlegend(x = \"bottomright\",\n       legend = c(\"x = 0.5\", \"x = 0.3\", \"x = 0.2\", \"FST = 1\"),\n       lty = c(1, 1, 1, 2), col = c(cols, \"black\"))\n\ntheory_values <- sapply(c(0.5, 0.3, 0.2), function(x) x * (1 - x))\nabline(h = theory_values, lty = 2, col = cols)\n\n\n\n\n\n\n\n(Weir and Hill 2002) (explained and summarised in (Bhatia et al. 2013)) give a more formal evolutionary definition of \\(F_\\text{ST}\\), in terms of covariance between derived and ancestral populations. Specifically, for a given SNP, the definition involves the conditional probability of allele frequency \\(p_i\\) in population \\(i\\), given an ancestral allele frequency \\(p_\\text{anc}\\), which is defined as a random process with the expectation\n\nWeir, B S, and W G Hill. 2002. “Estimating f-Statistics.” Annual Review of Genetics 36: 721–50. https://doi.org/10.1146/annurev.genet.36.050802.093940.\n\\[E(p_i|p_\\text{anc}) = p_\\text{anc}\\]\nand variance \\[Var(p_i|p_\\text{anc}) = F_\\text{ST}^i p_\\text{anc}(1-p_\\text{anc}).\\]\nThis form of the conditional variance can be understood by analysing the equation for the two boundary cases: For \\(F_\\text{ST}^i=0\\), there is no variance, so the conditional probability of the derived frequency will be completely determined by the ancestral frequency with no random change. In contrast \\(F_\\text{ST}^i=1\\) means that the variance in the derived allele frequency is that of a binomial distribution with variance \\(p_\\text{anc}(1-p_\\text{anc})\\), indicating random but complete fixation of the frequency to 0 or 1.\n\\(F_\\text{ST}\\) between two populations A and B is then defined as \\[F_\\text{ST}(A,B) = \\frac{F_\\text{ST}^A+F_\\text{ST}^B}{2}\\].\n\n\n\nWhile there are various mathematical definitions for both the theoretical definition and estimation for \\(F_\\text{ST}\\), which differ in subtle ways, we here follow the excellent paper by (Bhatia et al. 2013), which proposes the following estimator, termed Hudson-estimator, which in turn is based on a proposal by (Hudson, Slatkin, and Maddison 1992) and has been implemented in the ADMIXTOOLS package (Patterson et al. 2012):\n\nHudson, R R, M Slatkin, and W P Maddison. 1992. “Estimation of Levels of Gene Flow from DNA Sequence Data.” Genetics 132 (2): 583–89. https://doi.org/10.1093/genetics/132.2.583.\n\\[F_\\text{ST}=1-\\frac{H_w}{H_b}\\]\nHere, \\(H_w\\) is the average heterozygosity within each population, and \\(H_b\\) is the average heterozygosity between two populations. We can easily read off the two boundaries of the definition: At the lower end, we have \\(F_\\text{ST}=0\\) if and only if \\(H_w=H_b\\), so there is no difference between heterozygosity measured within or between groups, which is equivalent to saying that the two populations are the same. On the upper end we have \\(F_\\text{ST}=1\\) if and only if \\(H_w=0\\), so all observed variants are fully fixed in both populations (but not necessarily different between the populations).\nIt is fairly straight forward to see (and shown in (Bhatia et al. 2013)) that the Hudson-estimator above can be recast as\n\nBhatia, G, N Patterson, S Sankararaman, and A L Price. 2013. “Estimating and Interpreting FST: The Impact of Rare Variants.” Genome Research 23 (9): 1514–21. http://genome.cshlp.org/cgi/doi/10.1101/gr.154831.113.\n\\[F_\\text{ST}(A,B)=\\frac{(a-b)^2}{a(1-b)+b(1-a)}\\]\nHere, \\(a\\) and \\(b\\) denote population allele frequencies, which are in principle unobserved, but can be approximated by sample allele frequencies. This approximation is biased, and (Patterson et al. 2012) gives additional formulae for an (asympotically) unbiased estimator (which is for example also used in Poseidon’s tool xerxes, as detailed in the whitepaper).\nFrom this definition, you can see that \\(F_\\text{ST}(A,B)\\) is closely related to F2-statistics, introduced in (Patterson et al. 2012):\n\\[F_2(A,B)=(a-b)^2\\].\nIn some sense, \\(F_\\text{ST}(A,B)\\) can be considered a normalised version of \\(F_2(A,B)\\). While both statistics range mathematically from 0 to 1, the upper bound 1 has very different meanings in both. A theoretical value of \\(F_2=1\\) would mean that both populations are fixed at different alleles in all studied SNPs, which is practically not possible (even completely random fixations would suggest that 1/4 of them would agree given that there are only four nucleotides, let alone the fact that such deeply diverged populations/species would not be alignable anymore). One can say that the time-scale on which \\(F_2\\) approaches 1, for non-ascertained SNPs, so the entire genome, is the time scale of nucleotide substitutions (i.e. mutations plus fixation) along species branches, which in neutral evolution is given by the inverse mutation rate \\(1/\\mu\\). This would mean something on the order of \\(10^8\\) generations, which is arguably of the same order of magnitude as the depth of the entire tree of life. In contrast, \\(F_\\text{ST}\\) approaches 1 on the time-scale of fixation of standing variation, which is \\(2N\\) generations, which for humans is on the order of 10000 generations, so around the depth of modern-human diversity from its origins in Africa several hundred thousand years ago. Arguably, this time scale is much more useful for data analyses and thus easier to interpret.\nOf course, in practice, one uses some ascertained SNP set, as also here in our examples below, in which case values are much higher because we consider only variants that are segregating in human populations within a relatively high allele frequency.\nIf you’ve gone through our chapter on F3 and F4 statistics, you will have already encountered our software xerxes. You can compute both the biased and the approximately unbiased estimators for \\(F_\\text{ST}(A,B)\\), using the FST or FSTvanilla statistics, as defined in the whitepaper.\nFor what follows, we will use the approximately unbiased form FST.\n\\(F_\\text{ST}(A,B)\\) has a convenient and untuitive scale: It ranges from 0 to 1, where \\(F_\\text{ST}(A,B)=0\\) denotes that \\(A\\) and \\(B\\) are the same population, with no differentiation whatsoever. On the other hand of the spectrum we have \\(F_\\text{ST}(A,B)=1\\), which would mean that two populations are fully separated.\nAnother way to see this measure is to consider it as relative shared variance: If you consider genetic variation between \\(A\\) and \\(B\\), and within each of \\(A\\) and \\(B\\), then \\(F_\\text{ST}(A,B)\\) can be considered to measure the average variance between populations relative to the average variance within populations, again with intuitive boundaries 0 and 1."
  },
  {
    "objectID": "fst.html#computing-fst-using-xerxes",
    "href": "fst.html#computing-fst-using-xerxes",
    "title": "6  Measuring population structure using Fst",
    "section": "6.2 Computing FST using xerxes",
    "text": "6.2 Computing FST using xerxes\nFor human present-day populations, we can compute pairwise FSt using xerxes.\nWe here chose a number of populations from (Patterson et al. 2012) with more than 10 samples per population, and prepare the following config file for xerxes:\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\nfstats:\n- type: FST\n  a: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n  b: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n- type: F2\n  a: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n  b: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\nThis will then produce all combinations of \\(FST(A, B)\\) and \\(F_2(A, B)\\) as indicated in the population lists.\n\n\n\n\n\n\nNote\n\n\n\nNote that the config-file engine in xerxes always computes all the combinations of populations, even for cases of \\(A=B\\). It also doesn’t know about symmetry, so will happily compute the redundant statistics \\(FST(\\text{Adygei}, \\text{Adygei})\\) and \\(FST(\\text{Adygei}, \\text{Adygei})\\). While this could be possibly improved, there is no big harm done, as this runs fairly quickly.\n\n\nWe run this config file using the command line\nREPO=/path/to/community-archive/2012_PattersonGenetics\n\nxerxes fstats -d $REPO --statConfig fstat_world_config.yaml -f fstat_world_output.tsv > fstat_world_table.txt\nThe standard output, is a nicely layouted ASCII Table, which looks like this in the beginning:\n.-----------.-----------------.-----------------.---.---.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |        a        |        b        | c | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=================:=================:===:===:=========:================:====================:==================:====================:\n| FST       | Adygei          | Adygei          |   |   | 593124  | 0.0000         | 0.0000             | 0.0000           | NaN                |\n| FST       | Adygei          | Balochi         |   |   | 593124  | 1.2789e-2      | 1.2789e-2          | 3.3572e-4        | 38.09517110646904  |\n| FST       | Adygei          | Basque          |   |   | 593124  | 1.8790e-2      | 1.8790e-2          | 4.0141e-4        | 46.810358341103225 |\n| FST       | Adygei          | BedouinA        |   |   | 593124  | 1.3017e-2      | 1.3017e-2          | 2.9647e-4        | 43.90737238689979  |\n| FST       | Adygei          | BedouinB        |   |   | 593124  | 3.3455e-2      | 3.3454e-2          | 5.7648e-4        | 58.03217592610529  |\n| FST       | Adygei          | Biaka           |   |   | 593124  | 0.1716         | 0.1716             | 1.2185e-3        | 140.85275693678508 |\n| FST       | Adygei          | Brahui          |   |   | 593124  | 1.4644e-2      | 1.4644e-2          | 3.4481e-4        | 42.46989237781921  |\n| FST       | Adygei          | Burusho         |   |   | 593124  | 1.8566e-2      | 1.8566e-2          | 3.8156e-4        | 48.6573908240317   |\n| FST       | Adygei          | Druze           |   |   | 593124  | 1.2173e-2      | 1.2173e-2          | 2.6659e-4        | 45.65975464203526  |\n| FST       | Adygei          | French          |   |   | 593124  | 9.7730e-3      | 9.7730e-3          | 3.1627e-4        | 30.9006924987833   |\n| FST       | Adygei          | Han             |   |   | 593124  | 9.8759e-2      | 9.8759e-2          | 1.1973e-3        | 82.48660429503893  |\n| FST       | Adygei          | Hazara          |   |   | 593124  | 3.0725e-2      | 3.0726e-2          | 7.1478e-4        | 42.98629834431124  |\n| FST       | Adygei          | Italian_North   |   |   | 593124  | 8.6600e-3      | 8.6601e-3          | 2.7883e-4        | 31.058813893781032 |\n\nbut of course has many more lines (>1800 in this case). We also used the -f flag to output a tab-separated file, here named fstat_world_output.tsv, which is easier to read into R."
  },
  {
    "objectID": "fst.html#plotting-results-in-r",
    "href": "fst.html#plotting-results-in-r",
    "title": "6  Measuring population structure using Fst",
    "section": "6.3 Plotting results in R",
    "text": "6.3 Plotting results in R\nAll of the following code uses strictly only base-R for maximum compatibility. The code should run on any R installation.\nWe first load the data\n\ndat <- dat <- subset(read.table(\"fst_working/fstat_world_output.tsv\", sep=\"\\t\", header = TRUE),\n                     select=-c(c, d, Z_score_Jackknife))\ndatFST <- dat[dat$Statistic == \"FST\",]\ndatF2 <- dat[dat$Statistic == \"F2\",]\nhead(datFST)\n\n  Statistic      a        b NrSites Estimate_Total Estimate_Jackknife\n1       FST Adygei   Adygei  593124       0.000000           0.000000\n2       FST Adygei  Balochi  593124       0.012789           0.012789\n3       FST Adygei   Basque  593124       0.018790           0.018790\n4       FST Adygei BedouinA  593124       0.013017           0.013017\n5       FST Adygei BedouinB  593124       0.033455           0.033454\n6       FST Adygei    Biaka  593124       0.171600           0.171600\n  StdErr_Jackknife\n1       0.00000000\n2       0.00033572\n3       0.00040141\n4       0.00029647\n5       0.00057648\n6       0.00121850\n\n\nOk, this looks good. Let’s check out the largest values\n\nhead(dat[order(-dat$Estimate_Total),])\n\n    Statistic         a         b NrSites Estimate_Total Estimate_Jackknife\n166       FST     Biaka Karitiana  593124         0.3021             0.3021\n456       FST Karitiana     Biaka  593124         0.3021             0.3021\n473       FST Karitiana    Papuan  593124         0.3011             0.3011\n676       FST    Papuan Karitiana  593124         0.3011             0.3011\n468       FST Karitiana  Mandenka  593124         0.2798             0.2798\n526       FST  Mandenka Karitiana  593124         0.2798             0.2798\n    StdErr_Jackknife\n166        0.0016933\n456        0.0016933\n473        0.0026493\n676        0.0026493\n468        0.0017116\n526        0.0017116\n\n\nwhich shows that the largest FST values of around 0.3 are observed between Karitiana, from South America, and Biaka from Papua Neu Guinea (but note that these values are dependent on the ascertainment of SNPs, which here causes inflation)\nHere is a histogram of the values\n\nhist(datFST$Estimate_Total, xlab = \"FST\", ylab = \"Nr of pairs\",\n     main = \"\")\n\n\n\n\nSo most values are in the range of a few percent and 20 percent, with a mean of\n\nmean(datFST$Estimate_Total)\n\n[1] 0.09015616\n\n\nWe can compare that to F2:\n\nhist(datF2$Estimate_Total, xlab = \"F2\", ylab = \"Nr of pairs\",\n     main = \"\")\n\n\n\n\nwhich is an order of magnitude smaller.\nSo one of the key things to visualise is the pairwise matrix of FST, which we can quickly compute using the xtabs function from the stats package (part of base R):\n\nfstMat <- xtabs(Estimate_Total ~ a + b, datFST)\nf2Mat <- xtabs(Estimate_Total ~ a + b, datF2)\n\nand plot a simple heatmap using the powerful heatmap function from the stats package:\n\nheatmap(fstMat, symm = TRUE, hclustfun = function(m) hclust(m, method=\"ward.D2\"))\n\n\n\n\nwhich we can compare to the output using F2, which looks almost the same:\n\nheatmap(f2Mat, symm = TRUE, hclustfun = function(m) hclust(m, method=\"ward.D2\"))\n\n\n\n\nOK, let’s look at the dendrogram a bit closer:\n\nfstDist <- as.dist(fstMat)\ndendro <- hclust(fstDist, method=\"ward.D2\")\nplot(dendro, hang = -1, ylab = \"FST\", xlab = \"\", main = \"\")\n\n\n\n\nwhich again shows the strong drift that Native American populations (Karitiana) and Mayans experienced in their ancestral past.\nThis nicely shows how FST is affected by total drift, which is inversely proportional to population size, and proportional to total divergence time. A long branch can be caused by either low population size (as in the ancestral population of indigenous Americans) or long divergence time (as between populations from Africa and those outside of Africa)."
  },
  {
    "objectID": "pca_mds.html",
    "href": "pca_mds.html",
    "title": "7  Dimensionality reduction using PCA and MDS",
    "section": "",
    "text": "Comparison of PCA and MDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA\nMDS\n\n\n\n\nInput\noriginal data matrix / similarity matrix\npairwise distance matrix\n\n\nFocus\ncaptures maximum variance in data\npreserves pairwise distances\n\n\nMissing data?\nFill-in OR projection\nNot an issue if using summary statistics, but this hides the uncertainty of the statistic\n\n\n\n\n\n\n\n\nThe datasets used in human ancient DNA analysis are often extremely multidimensional, often including data from thousands of individuals, across hundreds of thousands (or millions!) of single nucleotide polymrphisms (SNPs) (Mallick et al. 2023). Even when choosing to summarise this genome-wide information to single statistics of genetic similarity (e.g. with Outgroup F3), a similarity matrix across individuals can become very large when comparing across hundreds of individuals. As the name implies, dimensionality reduction methods can reduce the number of dimensions in the underlying data, while also aiming to minimise the loss of information. The two such methods we will focus on in this tutorial are Principal Component Analysis (PCA) and Multi-Dimensional Scaling (MDS). Both methods reveal structure within the dataset, and part of that structure is due to shared population history between individuals/populations. It is for that reason that both these methods are indispensible parts of an archaeogeneticist’s toolkit.\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif Lazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023. “The Allen Ancient DNA Resource (AADR): A Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\n\n\n\nWhen using either of these methods, we are essentially representing the data on a new set of orthogonal axes, with its origin in the center of the data. In PCA we typically use the original data for this transformation (i.e. the genotype matrix), and attempt to find the axes that capture the most variation among the samples. A covariance matrix (i.e. a similarity matrix) is often calculated and used as a useful intermediate step in PCA. Instead, in MDS we start with a pairwise distance matrix (typically a matrix of 1-F3), and attempt to find a spatial representation that best captures the distances between points.\n\n\n\nA visual representation of the transformation PCA applies to a cloud of points, the range of which is represented by the blue oval. First, the data is rescaled around its own mean value, effectively moving the origin to the center of the data cloud (here shown s a red point). Then, the axes of maximal variation are discovered using linear algebra. Finally, the data is transformed to represent it along the identified axes of variation.\n\n\nThe results of both of these methods are (usually) a 2 dimensional plot in which the distances between individual points roughly correlates to the genetic distance between these individuals. Therefore, genetically similar individuals will be plotted close to one another, and further away from individuals that are more genetically dissimilar.\n\n\n\nA recurring issue when analysing ancient DNA is the high degree of missing data (i.e. missingness). We often apply a minimum coverage filter to our datasets: A generally accepted rule-of-thumb for the 1240K dataset is a minimum of 15 000 covered (i.e. non-missing) SNPs. Another way to express this cutoff is to say that we will “happily” analyse data that is missing a genotype call in 98.8% of all SNPs in the dataset! So how does this high rate of missingness affect MDS and PCA?\n\n\nMissingness does not affect MDS as adversely as it does PCA, on account of the use of a pairwise distance matrix of 1-F3. This matrix will only have missing values in cases where there is no overlapping coverage between two individuals/populations used in an F3 statistic. Instead, the issue with MDS is that all F3 statistics are treated as equally reliable, regardless of their associated error bar.\n\n\n\nUnlike MDS, PCA is severely affected by missing data. During the rescaling of the data around its own mean values, missing data is “filled-in” to the mean value (mean imputation). This can cause points to shift towards the origin by a distance relative to the degree of missingness.\nBelow is a plot of the results of PCA on a dataset of differnt worldwide populations (Reich et al. 2012). In an attempt to limit the effects of colonial admixture on the studied Native American populations, the authors masked parts of the genomes of Native Americans that matched the European or African populations in their dataset, replacing those genotypes with missing data.\n\nReich, David, Nick Patterson, Desmond Campbell, Arti Tandon, Stéphane Mazieres, Nicolas Ray, Maria V Parra, et al. 2012. “Reconstructing Native American Population History.” Nature 488 (7411): 370–74.\n\n\n\nPCA results on the same dataset based on the raw data (left), and after masking parts of the genome in Native American populations that match European populations. Due to mean imputation, masked individuals are attracted to the Origin.\n\n\nAs you can see, individuals whose genotypes were masked are shifted towards the plot’s origin. So how can we use PCA with ancient samples that have high degrees of missingness? The answer is by using a Least Squares Projection, a.k.a. lsqproject!\n\n\n\n\nThe idea of projection is simple, and applies similarly to both PCA and MDS. In PCA, you use a subset of the dataset to calculate your axes of variation, and then apply the resulting transformation to additional data, thus projecting them onto those axes. The important detail is that the variation between projected individuals is not taken into account when deciding which the axes of maximal variation are. Similarly, in MDS you project points to the MDS space based on their distances to the points that constructed the space, disregarding the distances of the projected points to one another.\nBelow is a PCA plot calculated on present-day West Eurasian populations together with some Mesolithic hunter-gatherer individuals (in light brown). In the right side plot, the ancient individuals have been included in the calculation of the principal components, while in the left side they are projected on the principal components of the present-day West Eurasians.\n\n\n\nPCA plot of ancient Mesolithic hunter-gatherers and present-day West Eurasian populations. On the right, the hunter gatherers are included in the principal component calculation, while on the left, they are projected on principal components calculated on the present-day populations only. The hunter-gatherers are part of three different groups: Eastern European hunter-gatherers (brown right-facing triangles), Scandinavian hunter-gatherers (brown diamonds), and Western European hunter-gatherers (brown half-filled circles).\n\n\nThere are two things to note here:\n\nFirst, comparing the placement of Eastern European hunter-gatherers (brown right-facing triangles) between the two plots, you can see that projecting these individuals does indeed provide results that are not affected by mean imputation, and thus are not shifted towards the origin.\nSecondly, if you compare the positions of the Western European hunter-gatherers (brown half-filled circles), you will notice that projection causes these individuals to be plotted closer to present-day populations.\n\nThe degree of missingness in the Western European hunter-gatherers (WHG) is relatively low, and hence the shift in their placement between the two plots is not the result of mean imputation. Instead, when projected the WHG illustrate the effects of shrinkage.\n\n\n\nShrinkage comes in two flavours:\n\nThe kind of shrinkage you saw with the WHGs above, is pretty intuitive. When projecting populations on axes of variation that do not capture all the variation of the projected populations, they will appear as if they have less variation than reality. This translates to the points “shrinking” towards the origin slightly. that is to say, because the WHG individuals come from a population that harboured far more genetic variation than is present within present-day West Eurasian populations, much of their true variation is “hidden” when projecting them.\nThe second kind of shrinkage (a.k.a. projection bias) arises because “samples used to calculate the PC axes”stretch” the axes” (from the smartpca documentation). This problem is exacerbated in datasets where the number of markers far exceeds the number of samples used for PC calculation. This is often the case in human population genomics.\n\nWhile the first shrinkage flavour can be argued to be a feature of PCA, projection bias can be a problem when trying to compare present-day populations to projected ancient populations. A demonstration of the effects of shrinkage can be seen below:\n\n\n\nUsing 10 individuals of each of the three tested populations (Yoruba, French, Han) to calculate PCs, and then projected another 10 individuals of each population reveals the effects of shrinkage on the positions of the projected individuals. In the absence of shrinkage, all points originating from the same population would be overlapping.\n\n\nShrinkage can be corrected by scaling the eigenvectors of the projected and/or non-projected individuals to bring them more in line with one another. Below is the same dataset as above, but ran through smartpca with the parameter shrinkmode: YES:\n\n\n\nUsing 10 individuals of each of the three tested populations (Yoruba, French, Han) to calculate PCs, and then projected another 10 individuals of each population. Shrinkage correction was done using ‘shrinkmode: YES’\n\n\nAs a note of caution, shrinkmode: Yes increases the runtime greatly. An alternative would be to identify specific present-day populations that are of interest for the ancient-to-modern comparison, and project those as well. So, for example, if we were to compare Iron Age individuals from Germany with present-day individuals from Germany, then we could decide to take out some or all present-day Germans and project those as well. That would make them fully comparable."
  },
  {
    "objectID": "pca_mds.html#practice",
    "href": "pca_mds.html#practice",
    "title": "7  Dimensionality reduction using PCA and MDS",
    "section": "7.2 Practice",
    "text": "7.2 Practice\n\n7.2.1 Preparation\n\n7.2.1.1 Get trident\nTrident is a Poseidon framework data management tool. It enables downloading Poseidon packages (genomic datasets, usually including ancient individuals, comprising genome-wide SNP data) from the Poseidon server, as well as creating and manipulating such packages.\ntrident for Linux:\n\n# download Trident v1.4.0.3 binary\nwget https://github.com/poseidon-framework/poseidon-hs/releases/download/v1.4.0.3/trident-Linux\n# rename to trident\nmv trident-Linux trident\n\n# make it executable\nchmod +x trident\n# run it\n./trident -h\n\ntrident for MacOS:\n\n# download Trident v1.4.0.3 binary\ncurl -LO https://github.com/poseidon-framework/poseidon-hs/releases/download/v1.4.0.3/trident-macOS\n# rename to trident\nmv trident-macOS trident\n\n# make it executable\nchmod +x trident\n# run it\n./trident -h\n\ntrident for Windows:\n\nDownload trident-Windows.exe file\n\n\n7.2.1.2 Prepare practise dataset:\nHere, we are downloading packages (listed in pca_mds_working/exampleData.fetchFile.txt) file from the Poseidon server into the scratch/poseidon-repository directory. The datasets come from the following publications: Patterson et al. 2012, Lazaridis et al. 2014, Raghavan et al. 2014, and Jeong et al. 2019\n\nmkdir -p scratch/poseidon-repository\n# This will take a few seconds to pull the data from the server\n./trident fetch -d scratch/poseidon-repository --fetchFile \"pca_mds_working/exampleData.fetchFile.txt\"\n\n\n# Check composition of one of the downloaded packages\nls scratch/poseidon-repository/2014_LazaridisNature-4.0.2\n\n\n# List all groups (populations) comprised by the downloaded packages\n./trident list --groups -d scratch/poseidon-repository/\n\n\n# Summarize information about  the downloaded packages\n./trident summarise -d scratch/poseidon-repository\n\n\n# Choose populations for the analysis (list for this exercise in \"exampleData.forgeFile.txt\"\")\nhead pca_mds_working/exampleData.forgeFile.txt\n\n\n# Count the number of listed populations to include\nwc -l pca_mds_working/exampleData.forgeFile.txt\n\n\n# Create (forge) a new repository with chosen groups from the downloaded packages\n./trident forge \\\n  -d scratch/poseidon-repository \\\n  -o scratch/forged_package \\\n  -n PCA_package_1 \\\n  --forgeFile pca_mds_working/exampleData.forgeFile.txt \\\n  --outFormat EIGENSTRAT\n\nThe created repository comprises genomic data for: 111 modern Eurasian populations, 6 modern Native American populations, and 1 Upper Palaeolithic Siberian individual MA-1 (“Mal’ta”).\n\n\n\n7.2.2 Run PCA\n\n# Prepare parameter file for the smartpca run\nmkdir -p scratch/smartpca_runs/poplist1 scratch/smartpca_runs/poplist2/\n\ncat <<EOF > scratch/smartpca_runs/poplist1/parameters.par\ngenotypename:   scratch/forged_package/PCA_package_1.geno   ## Genotype data\nsnpname:    scratch/forged_package/PCA_package_1.snp        ## SNP information\nindivname:  scratch/forged_package/PCA_package_1.ind        ## Individual information\n\nevecoutname:    scratch/smartpca_runs/poplist1/PCA_poplist1.evec           ## Eigenvectors\nevaloutname:    scratch/smartpca_runs/poplist1/PCA_poplist1.eval           ## Eigenvalues\n\npoplistname:    pca_mds_working/PCA_poplists/PCA_poplist1.txt\n\nlsqproject: YES     ## Project individuals not included in PC calculation onto the PCs\noutliermode: 2      ## Turns off automatic outlier removal.\nnumoutevec:  4       ## The number of eigenvectors to print per sample. Default is 10.\nEOF\n\nSo prepared parameter file will cause smartpca to estimate PCs using only the individuals from the populations listed in PCA_poplist1.txt and project all the remaining individuals onto those estimated PCs.\n\n# Run smartpca\nsmartpca -p scratch/smartpca_runs/poplist1/parameters.par\n\n\n# Inspect the output files\nls scratch/smartpca_runs/poplist1/\nhead scratch/smartpca_runs/poplist1/PCA_poplist1.evec\n\nAdding populations (Native Americans)\n\n# Look into other provided poplists\nwc -l pca_mds_working/PCA_poplists/*\n\ndiff -y --suppress-common-lines pca_mds_working/PCA_poplists/PCA_poplist1.txt pca_mds_working/PCA_poplists/PCA_poplist2.txt\n\n\n# Replace poplist1 with poplist2 in the smartpca parameter file\nsed 's/poplist1/poplist2/g' scratch/smartpca_runs/poplist1/parameters.par > scratch/smartpca_runs/poplist2/parameters.par\ncat scratch/smartpca_runs/poplist2/parameters.par\n\n\n# Rerun smartpca using poplist2 (additional populations)\nsmartpca -p scratch/smartpca_runs/poplist2/parameters.par\n\n# Inspect smartpca output\nls scratch/smartpca_runs/poplist2/\n\nSkipping projection (running smartpca without a poplist)\n\nmkdir -p scratch/smartpca_runs/all_pops\nhead -n 7 scratch/smartpca_runs/poplist1/parameters.par | sed 's/poplist1/all_pops/g' > scratch/smartpca_runs/all_pops/parameters.par\ntail -n 2 scratch/smartpca_runs/poplist1/parameters.par >> scratch/smartpca_runs/all_pops/parameters.par\necho \"maxpops: 200\" >> scratch/smartpca_runs/all_pops/parameters.par\necho \"fastmode: YES\" >> scratch/smartpca_runs/all_pops/parameters.par\ncat scratch/smartpca_runs/all_pops/parameters.par\n\n\n## Runtime of about 2 minutes\nsmartpca -p scratch/smartpca_runs/all_pops/parameters.par\nls scratch/smartpca_runs/all_pops/\n\nAs a result we have three PCAs:\n\n\n\n\n\n\n\n\n\ndata used in PC estimation\ndata projected\n\n\n\n\nPCA_poplist1\nEurasians\nNative Americans, Mal’ta\n\n\nPCA_poplist2\nEurasians, Native Americans\nMal’ta\n\n\nPCA_all_pops\nEurasians, Native Americans, Mal’ta\n\n\n\n\n\n\n7.2.3 Plot PCA\n\nlibrary(tidyverse)\n\nif(!require('remotes')) install.packages('remotes')\nif (!require('janno')) remotes::install_github('poseidon-framework/janno')\n\n## Load in poplist data\n## poplist1 -- Eurasian populations\npoplist1 <- readr::read_tsv(\"pca_mds_working/PCA_poplists/PCA_poplist1.txt\", col_names = \"Pops\", col_types = 'c')\n## poplist2 -- Eurasian populations + 6 Native American populations\npoplist2 <- readr::read_tsv(\"pca_mds_working/PCA_poplists/PCA_poplist2.txt\", col_names = \"Pops\", col_types = 'c')\n\n## Load in eigenvector data\nPCA_poplist1_ev <- readr::read_fwf(\"scratch/smartpca_runs/poplist1/PCA_poplist1.evec\", col_positions=readr::fwf_widths(c(20,11,12,12,12,19), col_names = c(\"Ind\",\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"Pop\")), col_types = 'cnnnnc', comment=\"#\")\nPCA_poplist2_ev <- readr::read_fwf(\"scratch/smartpca_runs/poplist2/PCA_poplist2.evec\", col_positions=readr::fwf_widths(c(20,11,12,12,12,19), col_names = c(\"Ind\",\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"Pop\")), col_types = 'cnnnnc', comment=\"#\")\nPCA_all_pops_ev <- readr::read_fwf(\"scratch/smartpca_runs/all_pops/PCA_all_pops.evec\", col_positions=readr::fwf_widths(c(20,11,12,12,12,19), col_names = c(\"Ind\",\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"Pop\")), col_types = 'cnnnnc', comment=\"#\")\n\n## Finally, we load in the metadata from the forged package annotation file (janno). Here, we keep only the individual Ids, country and their Lat/Lon position.\nmetadata<-janno::read_janno(\"scratch/forged_package/PCA_package_1.janno\", to_janno=F)%>% select(Poseidon_ID, Latitude, Longitude, Country) %>% mutate(Longitude=as.double(Longitude), Latitude=as.double(Latitude))\n\n## Finally, we add the Lat/Lon information to our datasets\nPCA_poplist1_ev <- left_join(PCA_poplist1_ev, metadata, by=c(\"Ind\"=\"Poseidon_ID\")) %>% mutate(Country=as.factor(Country))\nPCA_poplist2_ev <- left_join(PCA_poplist2_ev, metadata, by=c(\"Ind\"=\"Poseidon_ID\")) %>% mutate(Country=as.factor(Country))\nPCA_all_pops_ev <- left_join(PCA_all_pops_ev, metadata, by=c(\"Ind\"=\"Poseidon_ID\")) %>% mutate(Country=as.factor(Country))\n\n\n## First we subset the dataset to only the populations in the poplist\nmoderns_pl1 <- PCA_poplist1_ev %>% filter(Pop %in% poplist1$Pops)\n\n\np <- ggplot() +\n     coord_equal(xlim=c(-0.05,0.05),ylim=c(-0.05,0.15)) +\n     theme_minimal()\n\np + geom_point(\n        data=moderns_pl1, ##The input data for plotting\n        aes(x=PC1, y=PC2) ## Define the x and y axis\n        )\n\n\n## We can see how genetic similarity depends on geographical location by colouring the poins by longitude or latitude\nLon_plot <- p +\n    geom_point(data=moderns_pl1, aes(x=PC1, y=PC2, col=Longitude)) ## Here we also define the colour of the points based on a variable\n\nLat_plot <- p +\n    geom_point(data=moderns_pl1, aes(x=PC1, y=PC2, col=Latitude))\n\ngridExtra::grid.arrange(Lon_plot, Lat_plot, ncol=2)\n\n\n## As the orientation (+/-) of PC coordinates sometimes can change between runs of PCA, we use this code to ensure the same \"orientation\" for all users and thus enable making comparisons.\ncorner_inds_pl1 <- moderns_pl1 %>% select(Ind, PC1, PC2) %>% filter(Ind %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_pl1$PC1[1] > corner_inds_pl1$PC1[2]) { PCA_poplist1_ev <- PCA_poplist1_ev %>% mutate(PC1=-PC1)}\nif (corner_inds_pl1$PC2[1] > corner_inds_pl1$PC2[2]) { PCA_poplist1_ev <- PCA_poplist1_ev %>% mutate(PC2=-PC2)}\nmoderns_pl1 <- PCA_poplist1_ev %>% filter(Pop %in% poplist1$Pops)\n\n\n## Let's now colour the points by country. \nPCA_plot_1 <- p +\n    geom_point(data=moderns_pl1, \n               aes(x=PC1, y=PC2, col=Country), \n               alpha=0.5    ## Makes points semi transparent (so that aggregations of points are visible).\n              )\nPCA_plot_1\n\n\n\n\nPractice plot A) PCA estimated using modern Eurasian genomic data with Mal’ta projected (“PCA_poplist1”)\n\n\n\n## Now, let's see where Mal'ta individual got projected\nPCA_plot_1 +\n    geom_point(\n        data=PCA_poplist1_ev %>% filter(Ind==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=PC1, y=PC2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n    )\nggsave(\"scratch/PCA_plot_1.png\")\n\nPCA with Native American populations added to the analysis\n\n## First we reorient the PCA\ncorner_inds_pl2 <- PCA_poplist2_ev %>% select(Ind, PC1, PC2) %>% filter(Ind %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_pl2$PC1[1] > corner_inds_pl2$PC1[2]) { PCA_poplist2_ev <- PCA_poplist2_ev %>% mutate(PC1=-PC1)}\nif (corner_inds_pl2$PC2[1] > corner_inds_pl2$PC2[2]) { PCA_poplist2_ev <- PCA_poplist2_ev %>% mutate(PC2=-PC2)}\nmoderns_pl2 <- PCA_poplist2_ev %>% filter(Pop %in% poplist2$Pops)\n\n## Then we plot the output\nPCA_plot_2 <-  ggplot() +\n     coord_equal(xlim=c(-0.05,0.05),ylim=c(-0.05,0.15)) +\n     theme_minimal() +\n     geom_point(data=moderns_pl2 %>% filter(Country!=\"Brazil\" & Country!=\"Mexico\"), \n                aes(x=PC1, y=PC2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=moderns_pl2 %>% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=PC1, y=PC2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n       geom_point(\n        data=PCA_poplist2_ev %>% filter(Ind==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=PC1, y=PC2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\nPCA_plot_2\nggsave(\"scratch/PCA_plot_2.png\")\n\n\n\n\nPractice plot B) PCA estimated using modern Eurasian and Native American genomic data with Mal’ta projected (“PCA_poplist2”)\n\n\nAnd a PCA with all populations, including Mal’ta, used or estimation (no projection)\n\n## First we reorient the PCA\ncorner_inds_ap <- PCA_all_pops_ev %>% select(Ind, PC1, PC2) %>% filter(Ind %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_ap$PC1[1] > corner_inds_ap$PC1[2]) { PCA_all_pops_ev <- PCA_all_pops_ev %>% mutate(PC1=-PC1)}\nif (corner_inds_ap$PC2[1] > corner_inds_ap$PC2[2]) { PCA_all_pops_ev <- PCA_all_pops_ev %>% mutate(PC2=-PC2)}\nmoderns_ap <- PCA_all_pops_ev #%>% filter(Pop %in% poplist2$Pops) ## Poplist 2 contains all the present-day populations.\n\n## Then we plot the output\nPCA_plot_ap <-  ggplot() +\n     coord_equal(xlim=c(-0.05,0.05),ylim=c(-0.05,0.15)) +\n     theme_minimal() +\n     geom_point(data=moderns_ap %>% filter(Country!=\"Brazil\" & Country!=\"Mexico\"), \n                aes(x=PC1, y=PC2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=moderns_ap %>% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=PC1, y=PC2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(\n        data=PCA_all_pops_ev %>% filter(Ind==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=PC1, y=PC2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\nPCA_plot_ap\nggsave(\"scratch/PCA_plot_ap.png\")\n\n\n\n\nPractice plot C) PCA estimated using modern Eurasian and Native American, and Mal’ta genomic data with no projection (“PCA_all_pops”\n\n\n\n\n7.2.4 PLINK MDS\nNow let’s get an MDS plot for pairwise distances for the same dataset\n\n##Convert package to PLINK format\n./trident genoconvert -d scratch/forged_package --outFormat PLINK\n\nls scratch/forged_package/\n\n# Compute pairwise distances of all individuals\nplink --bfile scratch/forged_package/PCA_package_1 --distance-matrix --out scratch/pairwise_distances\n\n\n## Read in individual IDs from MDS results\ninds <- readr::read_tsv(\"scratch/pairwise_distances.mdist.id\", col_types=\"cc\", col_names=c(\"Population\", \"Poseidon_ID\"))\ninds\n\n\nmetadata <- janno::read_janno(\"scratch/forged_package/PCA_package_1.janno\", to_janno=F)%>% select(Poseidon_ID, Latitude, Longitude, Country) %>% mutate(Longitude=as.double(Longitude), Latitude=as.double(Latitude))\n\n## Finally, we add the Lat/Lon information to our datasets\ninds <- left_join(inds, metadata, by=\"Poseidon_ID\")\n\ndist_mat <- matrix(scan(\"scratch/pairwise_distances.mdist\"), ncol=nrow(inds))\ndim(dist_mat)\n\n\n?heatmap\n\n\n# first try and filter for a few populations:\nunique(inds$Population)\n\nindices <- inds$Population %in% c('French', 'Greek', 'Nganasan')\nhead(indices, 40)\n\n## Generate a heatmap of pairwise distances\nheatmap(dist_mat[indices,indices], labRow = inds$Population[indices], labCol = inds$Population[indices])\n\n\nlibrary(ggplot2)\nlibrary(magrittr) # This is for the pipe operator %>%\nmds_coords <- cmdscale(dist_mat)\ncolnames(mds_coords) <- c(\"C1\", \"C2\")\nmds_coords <- tibble::as_tibble(mds_coords) %>%\n    dplyr::bind_cols(inds)\nmds_coords\n\n\ncorner_inds <- mds_coords %>% dplyr::select(Poseidon_ID, C1, C2) %>% dplyr::filter(Poseidon_ID %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds$C1[1] > corner_inds$C1[2]) { mds_coords <- mds_coords %>% mutate(C1=-C1)}\nif (corner_inds$C2[1] > corner_inds$C2[2]) { mds_coords <- mds_coords %>% mutate(C2=-C2)}\n\nggplot(mds_coords) + \n       geom_point(data=mds_coords %>% filter(Country!=\"Brazil\" & Country!=\"Mexico\"),                 aes(x=C1, y=C2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=mds_coords %>% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=C1, y=C2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n    theme_minimal() +\n    coord_equal() +\n     geom_point(\n        data=mds_coords%>% filter(Poseidon_ID==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=C1, y=C2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\nggsave(\"scratch/MDS_plot_ap.png\")\n\n\n\n\nPractice plot D) MDS estimated using modern Eurasian and Mal’ta genomic data (“MDS_plot_ap”)\n\n\n\n## How does MDS compare to PCA if we restrict to the populations in poplist1?\n## Read in the poplist\npoplist1 <- readr::read_tsv(\"pca_mds_working/PCA_poplists/PCA_poplist1.txt\", col_names = \"Pops\", col_types = 'c')\n\n## Filter distance matrix\nindices_pl1 <- inds$Population %in% poplist1$Pops\n\ndist_mat[indices_pl1, indices_pl1]\n\n\n## Do MDS\nmds_coords_pl1 <- cmdscale(dist_mat[indices_pl1,indices_pl1])\ncolnames(mds_coords_pl1) <- c(\"C1\", \"C2\")\nmds_coords_pl1 <- tibble::as_tibble(mds_coords_pl1) %>%\n    dplyr::bind_cols(inds %>% dplyr::filter(inds$Population %in% poplist1$Pops))\nmds_coords_pl1\n\n\n## Reorient\ncorner_inds_mds1 <- mds_coords_pl1 %>% dplyr::select(Poseidon_ID, C1, C2) %>% dplyr::filter(Poseidon_ID %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_mds1$C1[1] > corner_inds_mds1$C1[2]) { mds_coords_pl1 <- mds_coords_pl1 %>% mutate(C1=-C1)}\nif (corner_inds_mds1$C2[1] > corner_inds_mds1$C2[2]) { mds_coords_pl1 <- mds_coords_pl1 %>% mutate(C2=-C2)}\n\n## Plot\nggplot(mds_coords_pl1) + \n     geom_point(data=mds_coords_pl1 %>% filter(Country!=\"Brazil\" & Country!=\"Mexico\"), \n                aes(x=C1, y=C2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=mds_coords_pl1 %>% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=C1, y=C2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n    theme_minimal() +\n    coord_equal()+\n     geom_point(\n        data=mds_coords%>% filter(Poseidon_ID==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=C1, y=C2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\n\nggsave(\"scratch/MDS_poplist1.png\")\n\n\n\n\nPractice plot E) MDS estimated using modern Eurasian and Native American, and Mal’ta genomic data (“MDS_poplist1”)\n\n\n\n\n7.2.5 Compare plots\nLet’s now sum up, by directly comparing all the plots we have generated:\n\n\n\n\n\n\nPCA estimated using modern Eurasian genomic data with Mal’ta projected (“PCA_poplist1”)\nPCA estimated using modern Eurasian and Native American genomic data with Mal’ta projected (“PCA_poplist2”)\nPCA estimated using modern Eurasian and Native American, and Mal’ta genomic data with no projection (“PCA_all_pops”)\nMDS estimated using modern Eurasian, and Mal’ta genomic data (“MDS_plot_ap”)\nMDS estimated using modern Eurasian and Native American, and Mal’ta genomic data (“MDS_poplist1”)\n\nThe dataset excluding Native American data comprises much less variation that makes up PC2 and hence the PC2 variation of Eurasians in plot A is stretched up compared to these in plots B and C (including Native Americans). In plot B and C it is therefore more difficult to observe differences within Eurasians along PC2 than in plot A. The variation making PC1 is comparable between the three plots. Individual Mal’ta is more closely related to Native Americans than an average Eurasian, so without Native Americans in the dataset (A) it ends up within the Eurasian variation as there is no Native American genetic signal present that would “pull” him away from the Eurasian variation towards the American variation. When Mal’ta is included in the estimation in plot C (not only projected, like in A and B), we can observe the effects of missing data, inherent for ancient genomes, causing this individual to be pulled towards the plot’s origin.\nIn MDS the missingness in Mal’ta’s data does not affect its position as it does in PCA. Inclusion of the diverged Native American data in the estimation does cause the decrease of the distances with the Eurasian population along the C2, but the difference between plot E and plot D is not as pronounced as between B and A."
  },
  {
    "objectID": "pca_mds.html#conclusions",
    "href": "pca_mds.html#conclusions",
    "title": "7  Dimensionality reduction using PCA and MDS",
    "section": "7.3 Conclusions",
    "text": "7.3 Conclusions\nIn PCA it is important to estimate the Eigenvalues using high-coverage samples, hence usually modern datasets, such as the 1000 Genomes Project (1kGP) or Human Genome Diversty Project (HGDP), are used for the estimation and the ancient samples are then projected onto the estimated PCs. Also, as mentioned above in the “Shrinkage” section, it is a good approach to project also some modern data if they are to be directly compared to the ancient samples.\nWhile MDS will be less sensitive to the effects of missing data, it disregards the uncertainty of the underlying pairwise distance estimates.\nIt is thus the best practice to perform both analyses and compare them taking the shortcomings of each into account when interpreting the relative positions of the studied individuals/populations obtained using these methods."
  },
  {
    "objectID": "pmrread.html",
    "href": "pmrread.html",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "",
    "text": "Coefficient of relationship, denoted \\(r_{ij}\\) , defined by Sewall Wright in 1922 (Wright 1922), is a measure of the degree of biological relationship between two individuals, commonly used in genetics and genealogy. It calculates the proportion of genes that two individuals have in common as a result of their genetic relationship. The coefficient of relationship is a derivative of the coefficient of inbreeding (\\(f_k\\) or \\(C_{I_k}\\)) defined by Wright a year earlier. A coefficient of inbreeding for an individual is typically one-half the coefficient of relationship between the parents.\nCoefficient of relationship between the parents approaches a value of 1 as the level of inbreeding increases and approaches 0 the more remote the common ancestors are.\nIn human relationships, coefficient of relationship is often calculated based on the knowledge of the family tree, typically extending to up to three or four generations, using a formula:\n\\[\nr_{ij} = \\sum (^1/_2)^{L_{ij}}\n\\] where \\(L\\) is the numbers of generation links between two individuals (\\(i\\) and \\(j\\)). E.g. full siblings are linked by two links through the mother (siblingA - mother - siblingB) and two links through the father (siblingA - father - siblingB), therefore the coefficient of relationship between them is \\(r = (^1/_2)^2 + (^1/_2)^2 = (^1/_4) + (^1/_4) = (^1/_2)\\) , while e.g. a person with their aunt are linked by three links through the shared grandmother/mother and three through the shared grandfather/father, so \\(r=(^1/_2)^3 + (^1/_2)^3 = (^1/_8) + (^1/_8) = (^1/_4)\\) .\nNote that under such definition, the coefficient of relationship is a lower bound and an actual value that may be up to a few percent higher due to unaccounted for consanguinity within the pedigree. The value is accurate to within 1% if the full family tree of both individuals is known to a depth of seven generations.\n\n\n\nKinship coefficient, denoted \\(\\phi_{ij}\\) [fa:i], is the probability that one allele sampled from individual \\(i\\) and one allele sampled from the same locus from individual \\(j\\) are identical by descent.\n\\(1 - \\phi_{ij}\\) can thus be interpreted as the probability that a randomly sampled allele from each individual is not identical by descent. Assuming that alleles are not under linkage disequilibrium, this value can be estimated from genome-wide data for a pair of individuals.\nKinship coefficient \\(\\phi_{ij}\\) , under some assumptions such as limited inbreeding, is related to Wright’s coefficient of relationship, denoted \\(r_{ij}\\) , via:\n\\[\n\\phi_{ij} = (^1/_2)r_{ij}\n\\] and hence provides a direct relationship between the degrees of relatedness from the pedigree and the expected kinship coefficient \\(\\phi_{ij}\\) .\n\n\n\nTable 1. Values of the coefficient of relatedness and the kinship coefficient for different pedigree relationships up to the second-degree, assuming that \\(C_I\\) = 0. From Rohrlach et al. (2023)."
  },
  {
    "objectID": "pmrread.html#pairwise-mismatch-rate-pmr",
    "href": "pmrread.html#pairwise-mismatch-rate-pmr",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "8.2 Pairwise Mismatch Rate (PMR)",
    "text": "8.2 Pairwise Mismatch Rate (PMR)\nMethods of relatedness estimation applied routinely to modern data are not applicable to ancient genomes due to small numbers of individuals sampled and high rate of data missingness (ie. low coverage), as well as due to lack of diploid phased genomic data available for majority of such samples. Thus, estimation of the coefficient of relatedness for ancient individuals using these methods is prone to biases and generally unreliable.\nPairwise mismatch rate (PMR) was introduced by (Kennett et al. 2017) as a means to estimate relatedness between ancient individuals (Figure 1). For each pair of individuals they computed the average mismatch rate across all autosomal SNPs covered by at least one sequence read for both of the two compared individuals (when >1 sequence read was present for one individual at a given site, a random read was sampled for the analysis) and computed standard errors using a weighted block jackknife. Mismatch rates significantly lower (Z>3) than the highest observed value, provided putative evidence of relatedness.\nThe PMR can be used to estimate the kinship coefficient, which, assuming that we can account for the inbreeding coefficient \\(C_I\\), can be used to estimate the degree of relatedness. Hence, we may gain insights into the pedigree joining many individuals (to a certain resolution). Kennett et al’s (2017) \\(PMR\\) estimation, however, did not include a hard-classification method nor was wrapped into any particular software piece.\n\n\n\nFig 1. Pairwise mismatch rate calculation (PMR). A) Pseudohaploid genomes are compared within each pair of individuals. B) Only sites called in both individuals are considered (filled circles) and classified as match (green) or mismatch (red). Accounting for linkage disequilibrium: C) Estimation of PMR using sliding window (as implemented in READ) as well as genome-wide (optional in READv2). D) Estimation of PMR on thinned SNP data (as implemented in BREADR)\n\n\n\n8.2.1 Accounting for pseudohaploidization\nDue to pseudohaploidisation (ie. drawing one allele randomly for each position) identical individuals will have an expected PMR of half of this between unrelated individuals.\nThe estimate of relatedness coefficient \\(r\\) needs therefore be corrected using the expected mismatch rate in non-related individuals. In (Kennett et al. 2017) they chose correction based on the approximate maximum mismatch rates observed: \\(b = max(PMR_{observed})/2\\) . The estimator they used is thus:\n\\[\nr = 1 - ((PMR_{ij} - b)/b) .\n\\]\nPMR estimation in ancient-DNA-based inference of relatedness has first been implemented as separate software with READ (Monroy Kuhn, Jakobsson, and Günther 2018) and then by its successor - READv2 (Alaçamlı et al. 2024). Other software used in relatedness estimation among ancient individuals, such as BREADR (Rohrlach et al. 2023), also build on PMR.\n\n\n8.2.2 Accounting for linkage disequilibrium\nLinkage disequilibrium (LD, non-independent co-inheritance) of the loci included in the PMR estimation will bias the results towards falsely positive relatedness detection. To minimize this effect, different approaches can be employed. This is particularly crucial, when analyzing genome-wide (shotgun) data. In the 1240k SNP panel widely used in ancient genomics the analysed loci have already been selected taking LD into account. The approaches to minimize the LD bias that have been employed in PMR estimation comprise PMR estimation separately over consecutive genome fragments (sliding window) and obtaining median estimate among them (e.g., implemented in READv1, and as an option in READv2; Figure 1C), and decreasing the number of SNPs included in the analysis using a threshold of physical proximity of SNPs along the genome (thinning; e.g., implemented in BREADR; Figure 1D).\n\n\n8.2.3 Good practice\nOnly pairs with at least 10,000 overlapping SNPs of the 1240k SNP panel should be included in PMR estimation (Furtwängler et al. 2020)"
  },
  {
    "objectID": "pmrread.html#read-version-1",
    "href": "pmrread.html#read-version-1",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "8.3 READ (version 1)",
    "text": "8.3 READ (version 1)\nRelationship Estimation from Ancient DNA (Monroy Kuhn, Jakobsson, and Günther 2018) - formalized implementation of PMR for ancient genomic data.\n\nPseudohaploid input data (TPED/TFAM format)\nDivision of the genome into non-overlapping windows of 1 Mbps each and calculation of the proportion of non-matching alleles inside each window (P0) for each pair of individuals.\nNormalization of P0 using the value expected for a randomly chosen pair of unrelated individuals from the population (in order to make the classification independent of within population diversity, SNP ascertainment and marker density).\nThis value is estimated by calculation of the median of all average pairwise P0 per window across all pairs of individuals, which, under sufficient sample size, can be treated as a proxy for an expected P0 in a pair of unrelated individuals. Normalization can also be performed using parameters other than median, e.g., maximum observed P0 value among the pairs.\nClassification of each pair of individuals as unrelated, second-degree (i.e. nephew/niece-uncle/aunt, grandparent-grand- child or half-siblings), first-degree (parent-offspring or siblings) or identical individuals/identical twins from the average across the per-window proportions of non-matching alleles (P0).\n\n\n\n\nFigure 2. Outline of the general READ workflow to estimate the degree of relationship between two individuals. From (Monroy Kuhn, Jakobsson, and Günther 2018)\n\n\n\n8.3.1 Pluses\nThe method has been shown to work quite well with as little as 0.1x shotgun coverage per genome. It has very simple assumptions estimating the expected pairwise mismatch rate from the data without the need for population allele frequencies. It can thus be used as part of initial QC procedures (e.g. identifying duplicated individuals) or in populations (or species) for which little additional information is available.\n\n\n\n8.3.2 Minuses\nREAD had been implemented as a Python 2 script. The last version of Python 2 was released in 2020 and some systems have already stopped supporting the language. Furthermore, READ wrote a large number of temporary files to the hard disk which were then analyzed by a separate R script called from the Python script."
  },
  {
    "objectID": "pmrread.html#readv2",
    "href": "pmrread.html#readv2",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "8.4 READv2",
    "text": "8.4 READv2\nNew implementation of READ (Alaçamlı et al. 2024)\n\n8.4.1 Improvements over version 1\n\nAll analyses are carried out within a single Python3 script using NumPy (Harris et al. 2020) and pandas (McKinney 2010) libraries.\nMajor analysis speed improvement (although more memory-intensive)\nAvoids excessive use of temporary files and the calling of a separate R script.\nMinor gain in accuracy due to changes in some default values (based on performance tests results).\nIntroduction of the “effective number of overlapping SNPs” (number of overlapping SNPs times the pairwise mismatch rate expected for unrelated individuals) representing a measure of the amount of information available for kinship estimation in a given pair of individuals. Provides benchmarking and increases comparability between studies.\nAbility to classify up to third-degree relatives (requires at least 5000 effectively overlapping SNPs).\nAbility to differentiate between different types of first-degree relationships (ie. full siblings vs parent-offspring; requires at least 10,000 effectively overlapping SNPs).\nFeasible for as much as 696 individuals (241 860 pairs) provided enough available memory.\nGenome-wide P0 calculation as default; with the uncertainty estimated using a block-jackknife approach with block sizes of 5 Mb (as commonly employed in human population genomic studies; (Patterson et al. 2012)). Option to use window-based approach of chosen window size also available.\n\n\n\n\nFigure 3. READv2 flowchart. From (Alaçamlı et al. 2024)\n\n\n\n\n8.4.2 Noteworthy properties\nOverall, READv2 performs well down to at least 0.1X sequence data in the simulated dataset. This corresponded to on average about 1,878 overlapping SNPs for each pair of individuals at an expected mismatch for unrelated individuals of ∼0.247.\nIn default settings, READv2 performs a genome-wide estimate of the pairwise mismatch rate, based on which it will assess the degree of relationship in each pair of individuals. This is followed by a separate round of classification for first-degree relatives. Here, the genome is divided into 20Mb windows and the proportion of windows that are classified as either “identical/twin” or “unrelated” is estimated. These proportions correspond to Cotterman coefficients \\(k_0\\) – windows classified as unrelated (i.e. no shared chromosome), and \\(k_2\\) – windows classified as identical (i.e. both chromosomes shared). Expected \\(k_0 + k_2\\) is low for parent-offspring and around 0.5 for full siblings when sufficient data are available.\nIf that proportion is less than 0.3, the pair is classified as “parent-offspring”; if it is between 0.35 and 0.6, the pair is classified as “siblings”. For other proportions, or when the number of effectively overlapping SNPs is below 10,000, the pair remains classified as “first-degree” without further specification.\nThe two types of relations are well separated down to 0.5X coverage in the simulated dataset (or ∼8,000 “effectively overlapping SNPs”), but they overlap at 0.2X and below. Hence, to avoid wrongly classifying parent-offspring pairs as siblings, READv2 applies a default cutoff of 10,000 effectively overlapping SNPs, below which classification is not performed. (In (Rivollat et al. 2020) a cutoff of 7000 effective SNP number cutoff was used.)\n\n\n8.4.3 Warnings\nAt low coverage (0.05x and 0.1x) there are high false positive rates for second- and third-degree relatedness; many unrelated pairs are classified into these categories. At 0.01X, unrelated individuals are even classified as first-degree or identical twins, resulting in a reduced false positive rate for second- and third-degree but an increased false positive rate for first-degree. To avoid false classifications in empirical data, READv2 applies a conservative threshold of 5000 “effectively overlapping SNPs”, below which no attempt to classify third-degree relatives is taken.\nAccording to the authors READv2 alone can lead to very similar results as the combination of READv1 and lcMLkin. output table by including information such as the number of overlapping SNPs, the number of effectively overlapping SNPs, and the kinship coefficient \\(μ\\) (= 1 - normalized P0)\n\n\n8.4.4 Comparison with other methods\nREADv2 is very similar in its approach to BREADR (Rohrlach et al. 2023) and TKGWV2 (Fernandes et al. 2021), with each tool having its own unique features. READv2 has the functionality to separate the different first-degree relationships, BREADR has a better quantification of uncertainty, and TKGWV2 works well with lower amounts of input data.\n\n\n\nTable 2. Comparison of methods available for relatedness reconstruction in ancient genomic data. From Alaçamlı et al. (2024).\n\n\nFor further comparison of some of the methods available, you can also refer to the paper by Akturk et al. (2023)"
  },
  {
    "objectID": "pmrread.html#usage",
    "href": "pmrread.html#usage",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "8.5 Usage",
    "text": "8.5 Usage\n\n8.5.1 Running READ (version 1)\nREAD version 1 is implemented in Python 2.\nThe input for READ is a pair of files in Plink’s TPED/TFAM.\n## Get READ off Bitbucket\ngit clone https://bitbucket.org/tguenther/read.git\nIf starting from eigenstrat files you will need to use for example:\n\nconvertf (Eigensoft) and plink transpose:\n\n## Create a parameter file for Eigensoft convertf:\necho \"\ngenotypename:    myGenotypeFile.geno\nsnpname:         myGenotypeFile.snp\nindivname:       myGenotypeFile.ind\noutputformat:    PACKEDPED\ngenotypeoutname: myGenotypeFile.bed\nsnpoutname:      myGenotypeFile.bim\nindivoutname:    myGenotypeFile.fam\n\" > scratch/myGenotypeFile.GenotToBed.convertf.param\n\n## Convert eigenstrat to packedped\nPathTo_EigensoftConvertf -p myGenotypeFile.GenotToBed.convertf.param\n\n## Transpose packedped to tped\nplink --bfile myGenotypeFile --recode transpose --out myGenotypeFile \n## The output is myGenotypeFile.tped, myGenotypeFile.tfam, myGenotypeFile.nosex, myGenotypeFile.log\nor\n\ntrident genoconvert, if you are working with Poseidon packages:\n\n## Convert eigenstrat-formatted package(s) to plink-formatted\ntrident genoconvert -d ... -d ... --outFormat PLINK\nWith the generated PLINK files you can no run READ:\n## Run READ\npython pathTo_Read.py myGenotypeFile <normalization_method>\n\n8.5.1.1 Options\nnormalization_method:\n\nmedian (default) - assuming that most pairs of compared individuals are unrelated, READ uses the median across all pairs for normalization.\nmean - READ uses the mean across all pairs for normalization, this would be more sensitive to outliers in the data (e.g. recent migrants or identical twins)\nmax - READ uses the maximum across all pairs for normalization. This should be used to test trios where both parents are supposed to be unrelated but the offspring is a first degree relative to both others.\nvalue <val> - READ uses a user-defined value for normalization. This can be used if the value from another population should be used for normalization. That would be useful if only two individuals are available for the test population. A value can be obtained from the NonNormalizedP0 column of the file meansP0_AncientDNA_normalized from a previous run of READ.\n\nOptionally, one can add --window_size <value> at the end in order to modify the window size used (default: 1000000).\n\n\n8.5.1.2 Output:\n“READ_results”:\nPairIndividuals Relationship Z_upper Z_lower I0232I0358 Unrelated NA -17.0238 ... I0354I0360 First Degree 3.8987 -9.6782 I0354I0361 Unrelated NA -15.4353 ... I0421I0430 First Degree 7.5325 -12.1995 I0421I0431 Unrelated NA -3.9037 ...\n““meansP0_AncientDNA_normalized”:\nPairIndividuals Normalized2AlleleDifference StandardError NonNormalizedP0 NonNormalizedStandardError I0232I0358 1.0069 0.0059 0.2531 0.00149 ... I0354I0360 0.7587 0.0138 0.1907 0.0035 I0354I0361 1.0092 0.0067 0.2537 0.0017 ... I0421I0430 0.7409 0.0095 0.1862 0.0024 I0421I0431 1.0115 0.0270 0.2543 0.0068 ...\n“READ_results_plot.pdf”:\n\n\n\n\n\n\n\n\n8.5.2 Running READ v2\nREADv2 is recommended to be ran as conda environment.\n## Preparing the environment\ngit clone https://github.com/GuntherLab/READv2.git\n\nconda create -n readv2 python=3.7 pandas=1.1.1 numpy=1.18.3 pip=22.3.1 matplotlib=3.5.3\n\nconda activate readv2\n\npip install plinkio  ## Used for format conversion of the inpt data.\n## Personally I couldn't make plinkio work and am using Poseidon/trident or eigensoft convertf (as described below).\nThe input for READv2 is a trio of files in Plink’s BED/BIM/FAM format.\nIf starting with files in EIGENSTRAT format, they can be converted to PLINK format using for example:\n\nconvertf (Eigensoft) and plink transpose:\n\n## Create convertf parameter file for converting .geno to .bed files (this can probably also be done using PLINKIO, as implemented in READv2 documentation):\necho \"\ngenotypename:    myGenotypeFile.geno\nsnpname:         myGenotypeFile.snp\nindivname:       myGenotypeFile.ind\noutputformat:    PACKEDPED\ngenotypeoutname: myGenotypeFile.bed\nsnpoutname:      myGenotypeFile.bim\nindivoutname:    myGenotypeFile.fam\n\" > scratch/myGenotypeFile.GenoToBed.convertf.param\n    \n## Convert eigenstrat to packedped\nPathTo_EigensoftConvertf -p myGenotypeFile.GenotToBed.convertf.param\n## The output is myGenotypeFile.bed, myGenotypeFile.fam, myGenotypeFile.bim\nor\n\ntrident genoconvert, if you are working with Poseidon packages:\n\n## Convert eigenstrat-formatted package(s) to plink-formatted       \ntrident genoconvert -d ... -d ... --outFormat PLINK\nWith the PLINK format files you can run READv2:\n## Activate READv2 conda environment\nconda activate readv2\n\n## Run READv2\npython pathTo_Readv2.py -i myGenotypeFile\n\n8.5.2.1 Options\nREADv2 options:\n\n-i, --input_file val – Input file prefix (required). The current READ version only supports Plink bed/bim/fam files.\n-n, --norm_method val – Normalization method (either ‘mean’, ‘median’, ‘max’ or ‘value’).\n\nmedian (default) – assuming that most pairs of compared individuals are unrelated, READ uses the median across all pairs for normalization.\nmean – READ uses the mean across all pairs for normalization, this would be more sensitive to outliers in the data (e.g. recent migrants or identical twins)\nmax – READ uses the maximum across all pairs for normalization. This should be used to test trios where both parents are supposed to be unrelated but the offspring is a first-degree relative to both others.\nvalue – READ uses a user-defined value for normalization. This can be used if the value from another population should be used for normalization. That would be useful if only two individuals are available for the test population. Normalization value needs to be provided through --norm_value\n\n--norm_value val – Provide a user-defined normalization value\n--window_size val – Change window size for block jackknife or for window-based P0 estimates (as in READv1), default: 5000000\n--window_est – Window based estimate of P0 (as opposed to the genome-wide estimate, default in READv2)\n-h, --help – Print help message\n-v, --version – Print version\n\n\n\n8.5.2.2 Output:\n“meansP0_AncientDNA_normalized_READv2”\nPairIndividuals Norm2AlleleDiff StError_2Allele_Norm    Nonnormalized_P0    Nonnormalized_P0_serr   OverlapNSNPs\nI0232I0358  1.0052  0.0076    0.2553  0.0019    261263\n...\nI0234I0423  0.9659  0.0083  0.2453  0.0021    195053\nI0234I0424  0.9985  0.0071    0.2536 0.0018   224327\n...\nI0354I0360  0.7464  0.0137    0.1896 0.0035    21056\nI0354I0361  1.0046  0.0089    0.2552  0.0023    147438\n...\n“Read_Results.tsv”\nPairIndividuals Rel Zup Zdown   P0_mean Nonnormalized_P0    Nonnormalized_P0_serr   1st_Type    Perc_Win_1stdeg_P0  OverlapNSNPs    NSNPsXNorm  KinshipCoefficient\nI0232I0358  Unrelated   NA  12.9726  1.0052  0.2553  0.0019    N/A 0.9150  261263  66362.1298    -0.0052\n...\nI0234I0423  Third Degree    0.3487  7.1920   0.9659  0.2453  0.0021    N/A 0.7974  195053  49544.4533   0.0341\nI0234I0424  Unrelated   NA  12.9201  0.9985  0.2536 0.0018   N/A 0.9020  224327  56980.1981    0.0015\n...\nI0354I0360  First Degree    4.7682   8.7575   0.7464  0.1896 0.0035   N/A 0.3816  21056   5348.3310   0.2536\nI0354I0361  Unrelated   NA  11.0926 1.0046  0.2552  0.0023    N/A 0.9281  147438  37450.0013  -0.0046\n...\n“READ.pdf”:\n\n\n\n\n\n\n\n\n\nAktürk, Şevval, Igor Mapelli, Merve Nur Güler, Kanat Gürün, Büşra Katırcıoğlu, Kıvılcım Başak Vural, Ekin Sağlıcan, et al. 2023. “Benchmarking Kinship Estimation Tools for Ancient Genomes Using Pedigree Simulations.” bioRxiv.\n\n\nAlaçamlı, Erkin, Thijessen Naidoo, Şevval Aktürk, Merve N Güler, Igor Mapelli, Kıvılcım Başak Vural, Mehmet Somel, Helena Malmström, and Torsten Günther. 2024. “READv2: Advanced and User-Friendly Detection of Biological Relatedness in Archaeogenomics.” bioRxiv.\n\n\nFernandes, Daniel M, Olivia Cheronet, Pere Gelabert, and Ron Pinhasi. 2021. “TKGWV2: An Ancient DNA Relatedness Pipeline for Ultra-Low Coverage Whole Genome Shotgun Data.” Sci. Rep. 11 (1): 21262.\n\n\nFurtwängler, Anja, A B Rohrlach, Thiseas C Lamnidis, Luka Papac, Gunnar U Neumann, Inga Siebke, Ella Reiter, et al. 2020. “Ancient Genomes Reveal Social and Genetic Structure of Late Neolithic Switzerland.” Nat. Commun. 11 (1): 1915.\n\n\nHarris, Charles R, K Jarrod Millman, Stéfan J van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” Nature 585 (7825): 357–62.\n\n\nKennett, Douglas J, Stephen Plog, Richard J George, Brendan J Culleton, Adam S Watson, Pontus Skoglund, Nadin Rohland, et al. 2017. “Archaeogenomic Evidence Reveals Prehistoric Matrilineal Dynasty.” Nat. Commun. 8 (February): 14115.\n\n\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in Python,” 56–61.\n\n\nMonroy Kuhn, Jose Manuel, Mattias Jakobsson, and Torsten Günther. 2018. “Estimating Genetic Kin Relationships in Prehistoric Populations.” PLoS One 13 (4): e0195491.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93.\n\n\nRivollat, Maı̈té, Choongwon Jeong, Stephan Schiffels, İşil Küçükkalıpçı, Marie-Hélène Pemonge, Adam Benjamin Rohrlach, Kurt W Alt, et al. 2020. “Ancient Genome-Wide DNA from France Highlights the Complexity of Interactions Between Mesolithic Hunter-Gatherers and Neolithic Farmers.” Sci Adv 6 (22): eaaz5344.\n\n\nRohrlach, Adam B, Jonathan Tuke, Divyaratan Popli, and Wolfgang Haak. 2023. “BREADR: An r Package for the Bayesian Estimation of Genetic Relatedness from Low-Coverage Genotype Data.” bioRxiv. https://doi.org/10.1101/2023.04.17.537144.\n\n\nWright, Sewall. 1922. “Coefficients of Inbreeding and Relationship.” Am. Nat. 56 (645): 330–38."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aktürk, Şevval, Igor Mapelli, Merve Nur Güler, Kanat Gürün, Büşra\nKatırcıoğlu, Kıvılcım Başak Vural, Ekin Sağlıcan, et al. 2023.\n“Benchmarking Kinship Estimation Tools for Ancient Genomes Using\nPedigree Simulations.” bioRxiv.\n\n\nAlaçamlı, Erkin, Thijessen Naidoo, Şevval Aktürk, Merve N Güler, Igor\nMapelli, Kıvılcım Başak Vural, Mehmet Somel, Helena Malmström, and\nTorsten Günther. 2024. “READv2: Advanced and\nUser-Friendly Detection of Biological Relatedness in\nArchaeogenomics.” bioRxiv.\n\n\nBhatia, G, N Patterson, S Sankararaman, and A L Price. 2013.\n“Estimating and Interpreting FST: The Impact of Rare\nVariants.” Genome Research 23 (9): 1514–21. http://genome.cshlp.org/cgi/doi/10.1101/gr.154831.113.\n\n\nEwels, Philip A., Alexander Peltzer, Sven Fillinger, Harshil Patel,\nJohannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso,\nand Sven Nahnsen. 2020. “The Nf-Core Framework for\nCommunity-Curated Bioinformatics Pipelines.” Nature\nBiotechnology 38 (3): 276–78. https://doi.org/10.1038/s41587-020-0439-x.\n\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida\nAndrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia,\nJudith Neukamm, and Alexander Peltzer. 2021. “Reproducible,\nPortable, and Efficient Ancient Genome Reconstruction with\nNf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\n\nFernandes, Daniel M, Olivia Cheronet, Pere Gelabert, and Ron Pinhasi.\n2021. “TKGWV2: An Ancient DNA\nRelatedness Pipeline for Ultra-Low Coverage Whole Genome Shotgun\nData.” Sci. Rep. 11 (1): 21262.\n\n\nFurtwängler, Anja, A B Rohrlach, Thiseas C Lamnidis, Luka Papac, Gunnar\nU Neumann, Inga Siebke, Ella Reiter, et al. 2020. “Ancient Genomes\nReveal Social and Genetic Structure of Late Neolithic\nSwitzerland.” Nat. Commun. 11 (1): 1915.\n\n\nGreen, Richard E, Johannes Krause, Adrian W Briggs, Tomislav Maricic,\nUdo Stenzel, Martin Kircher, Nick Patterson, et al. 2010. “A Draft\nSequence of the Neandertal Genome.” Science 328 (5979):\n710–22. http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=20448178&retmode=ref&cmd=prlinks.\n\n\nHarris, Charles R, K Jarrod Millman, Stéfan J van der Walt, Ralf\nGommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020.\n“Array Programming with NumPy.”\nNature 585 (7825): 357–62.\n\n\nHudson, R R, M Slatkin, and W P Maddison. 1992. “Estimation of\nLevels of Gene Flow from DNA Sequence Data.”\nGenetics 132 (2): 583–89. https://doi.org/10.1093/genetics/132.2.583.\n\n\nKennett, Douglas J, Stephen Plog, Richard J George, Brendan J Culleton,\nAdam S Watson, Pontus Skoglund, Nadin Rohland, et al. 2017.\n“Archaeogenomic Evidence Reveals Prehistoric Matrilineal\nDynasty.” Nat. Commun. 8 (February): 14115.\n\n\nKistler, Logan, Roselyn Ware, Oliver Smith, Matthew Collins, and Robin\nG. Allaby. 2017. “A New Model for Ancient DNA Decay Based on\nPaleogenomic Meta-Analysis.” Nucleic Acids Research 45\n(11): 6310–20. https://doi.org/10.1093/nar/gkx361.\n\n\nLamnidis, Thiseas C, Kerttu Majander, Choongwon Jeong, Elina Salmela,\nAnna Wessman, Vyacheslav Moiseyev, Valery Khartanovich, et al. 2018.\n“Ancient Fennoscandian Genomes Reveal Origin and Spread of\nSiberian Ancestry in Europe.” Nature Communications 9\n(1): 5018. https://doi.org/10.1038/s41467-018-07483-5.\n\n\nLindahl, Tomas, and Barbro Nyberg. 1974. “Heat-Induced Deamination\nof Cytosine Residues in Deoxyribonucleic Acid.”\nBiochemistry 13 (16): 3405–10. https://doi.org/10.1021/bi00713a035.\n\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif\nLazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023.\n“The Allen Ancient DNA Resource (AADR):\nA Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\n\n\nMartin, Simon H, John W Davey, and Chris D Jiggins. 2015.\n“Evaluating the Use of ABBA-BABA Statistics to Locate\nIntrogressed Loci.” Molecular Biology and Evolution 32\n(1): 244–57. https://doi.org/10.1093/molbev/msu269.\n\n\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in\nPython,” 56–61.\n\n\nMeyer, Matthias, Juan-Luis Arsuaga, Cesare de Filippo, Sarah Nagel,\nAyinuer Aximu-Petri, Birgit Nickel, Ignacio Martínez, et al. 2016.\n“Nuclear DNA Sequences from the Middle Pleistocene Sima de Los\nHuesos Hominins.” Nature 531 (7595): 504–7. https://doi.org/10.1038/nature17405.\n\n\nMonroy Kuhn, Jose Manuel, Mattias Jakobsson, and Torsten Günther. 2018.\n“Estimating Genetic Kin Relationships in Prehistoric\nPopulations.” PLoS One 13 (4): e0195491.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin\nRohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich.\n2012a. “Ancient Admixture in Human History.”\nGenetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\n\n\n———. 2012b. “Ancient Admixture in Human History.”\nGenetics 192 (3): 1065–93.\n\n\nPeter, Benjamin M. 2016. “Admixture, Population Structure, and\nF-Statistics.” Genetics 202 (4): 1485–1501.\nhttps://doi.org/10.1534/genetics.115.183913.\n\n\nPeyrégné, Someone. 2020. “Title of the Article.”\nJournal Name 10: 100–110. https://doi.org/10.1000/j.journal.2020.01.001.\n\n\nReich, David, Nick Patterson, Desmond Campbell, Arti Tandon, Stéphane\nMazieres, Nicolas Ray, Maria V Parra, et al. 2012. “Reconstructing\nNative American Population History.” Nature 488 (7411):\n370–74.\n\n\nRivollat, Maı̈té, Choongwon Jeong, Stephan Schiffels, İşil Küçükkalıpçı,\nMarie-Hélène Pemonge, Adam Benjamin Rohrlach, Kurt W Alt, et al. 2020.\n“Ancient Genome-Wide DNA from France Highlights the\nComplexity of Interactions Between Mesolithic Hunter-Gatherers and\nNeolithic Farmers.” Sci Adv 6 (22): eaaz5344.\n\n\nRohrlach, Adam B, Jonathan Tuke, Divyaratan Popli, and Wolfgang Haak.\n2023. “BREADR: An r Package for the Bayesian Estimation of Genetic\nRelatedness from Low-Coverage Genotype Data.” bioRxiv.\nhttps://doi.org/10.1101/2023.04.17.537144.\n\n\nSawyer, Susanna, Johannes Krause, Katerina Guschanski, Vincent\nSavolainen, and Svante Pääbo. 2012. “Temporal Patterns of\nNucleotide Misincorporations and DNA Fragmentation in Ancient\nDNA.” Edited by Carles Lalueza-Fox. PLoS ONE 7 (3):\ne34131. https://doi.org/10.1371/journal.pone.0034131.\n\n\nWeir, B S, and W G Hill. 2002. “Estimating f-Statistics.”\nAnnual Review of Genetics 36: 721–50. https://doi.org/10.1146/annurev.genet.36.050802.093940.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific\nData Management and Stewardship.” Scientific Data 3 (1).\nhttps://doi.org/10.1038/sdata.2016.18.\n\n\nWright, Sewall. 1922. “Coefficients of Inbreeding and\nRelationship.” Am. Nat. 56 (645): 330–38."
  }
]