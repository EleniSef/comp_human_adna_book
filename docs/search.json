[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Methods for human population genetics and ancient DNA",
    "section": "",
    "text": "Preface\nThis book summarises prepared mini-courses for various computational tools and methods in the field of human archaeogenetic data analysis, with a particular emphasis on population genetics.\nThe chapters are contributed by different authors, as indicated in the Yaml-frontmatter of each chapter’s .qmd source file."
  },
  {
    "objectID": "Quarto_intro.html#setting-up",
    "href": "Quarto_intro.html#setting-up",
    "title": "1  Introduction to Quarto",
    "section": "1.1 Setting up",
    "text": "1.1 Setting up\nQuarto is the “next generation” of R Markdown and is usable on different tools.\n\nHere, we will describe how to set up your environment to use Quarto in RStudio, and VSCode.\nQuarto in general is set up to be very intuitive and user friendly. And while it is possible to set up different documents simultaneously, those can also easily be set up to in just the way you need for whatever occasion. So, for either communicating your results to collaborators, discuss code with your supervisor, setting up a website or writing your paper, to just name some scenarios, quarto comes in quite handy. So, let’s begin:"
  },
  {
    "objectID": "Quarto_intro.html#rstudio",
    "href": "Quarto_intro.html#rstudio",
    "title": "1  Introduction to Quarto",
    "section": "1.2 RStudio",
    "text": "1.2 RStudio\nFor this, you have to download RStudio first. If you have done this already, we can get started.\n\n1.2.1 Getting started\nFirst, we have to install the quarto package using the following command in our console:\ninstall.packages(\"quarto\")\nNow we are ready to set up a quarto document.\nFor this we open a new project and select the quarto document we want to create. You can choose to set up a git repository as well. For practicality, I usually also tick the box visual markdown editor.\n\nThe new project will look like this:\n\n\n\n1.2.2 Universal instructions\nWhen setting up your document, quarto will always provide you with some presets. So first, we will have a look into the .qmd files, for they are handled the same way, regardless the format you are setting up (website, book, presentation, etc.).\n\n1.2.2.1 Render & save\nIf we now click on render, we will be provided with the preview of our final version if the project in the viewer.\nImportant: Do not mistake “save” with “render”. Just by saving, your document did not get rendered automatically, unless you tick the box “Render on Save”. You have to tick that box on each .qmd file individually though.\n\n\n\n1.2.2.2 Visual and Source\nIf you have chosen the Visual option on your toolbar, the preview will mostly resemble your .qmd files. If you are more comfortable with the R markdown optics, you can switch to Source.\nIn the Source version, you can write up your document in LaTeX.\n\n\n\n1.2.2.3 .qmd files\nQuarto will automatically provide you with two .qmd files, as well as a .yml file.\nThe .qmd files respond to the individual pages of your website or chapter of your book or pages of your presentation, etc. You can shape them individually or define the layout for all of them in the .yml file, to which we will get later.\nIn your .qmd files you also find a yaml at the top of your document, separated by\n---\n---\nWithin these, you can define the outline of .qmd file individually, starting with the page header. Other options, you might be using in the future are:\nauthor: Jessi Doe\n-&gt; will add an author underneath the header\nexecute:    echo: true\n-&gt; if &gt;true&lt;, code will be visible\ntoc:true\n-&gt; if &gt;true&lt;, a table of content will be automatically added\nbibliography: your_references.bib\n-&gt; file or list of files for your references\nIt is crucial to stick to the spacing. Otherwise, an error will occur.\n\n1.2.2.3.1 Insert\nIf you click Insert, you will realize, quarto provides you with a lot of options and shortcuts as well. So by just selecting on your chosen item to insert, it will be added to the document, while you are also provided with options on the appearance (in the case of figure/images or tables, etc). We will here have a brief look into how to work with R code and how to use the reference option.\n\n\n1.2.2.3.2 R code\nTo add R code to your file, you select Insert, select Code Cell and choose the kind of code you want to insert. In this case, R. There are some things to keep in mind though.\nDepending on how you have set up your .qmd file (or your .yml), your code will be visible or not on your website. To check your output, you can click the green arrow for the latest bit of code or the grey arrow above a bar to run the previous code.\n\nBut in case there are some chunks of code, you do not want to show all the time, there are some nice sets.\nSo if we just load the library tidyverse, for example, the additional information regardless and it will be also visible on our website.\n\nTo avoid this, we can set up a code chunk, looking like this:\n\nThis will prevent the output of this code chunk to be depicted on your website, while the package is otherwise active and can be used in the following R code. This is, by the way, true for all R code and data sets you will use: they will be active in your document and can be used in different code chunks, once provided.\nA code block included in your document could look like the following. Here I used the option\ncode-fold: true so the code can be extended. This option is only available in html though.\n\n\n\n1.2.2.3.3 References\nDepending on which citation program you are using, quarto is able to connect to it (Zotero works, for example). So, when selecting to insert a Citation, you can choose to simply add a reference from your program.\nAlternatively, you are provided with some options to choose from:\n\nIn your source code and your website, a citation will be depicted as follows:\n\nThe citation will also automatically be added to a references.bib as well as to a references.qmd and is therefore available on your website on the page “References”, which also will be created automatically.\n\n\n\n\n1.2.3 Render your document\nWhen done with setting up your documents, you would like to have the actual output. Depending on the format you set in your .yml file, your output can be a html, pdf, MS Word, OpenOffice, or ePUB file. To create those, the terminal in your RStudio Project is used.\nBy using the command:\nquarto render\nall formats you predefined in your .yml file will be rendered.\nIn case you are only interested in one format to be rendered, you can specify your command:\nquarto render -to pdf\nYour rendered document should now look like this:\n\n\n\n1.2.4 Quarto website\nIn the provided .qmd files, the index is also the first page of your website. As you might have noticed, quarto already sets up a navbar as well as a search function on your website.\n\n\n1.2.4.1 .yml files\nWhile your .qmd files contain information about one page, the .yml file defines the overall looks of the website.\nHere, you can define the type of your project (in this case a website), you can change the name of the website (Title), define your navbar (which shall be your landing page and in what order your pages shall be set up) or the overall appearance of your website in general (theme, css, toc, backgroundcolor, etc.). For different styles and layouts, check out the quarto website again.\n\n\n\n\n1.2.5 Quarto book\nThe setup of the .yml file in a quarto book is slightly different than that of the website. So here are some general remarks about them.\n\n1.2.5.1 .yml files\nWe see, for example, that instead of a navbar, we find chapters. Those will appear in the listed order in your book.\nWe also already get provided with a bibliography and the responding .bib file. If you have other .bib files, those can be included in your references, by just adding them to bibliography.\n\n_________________________________________________________________________\nWith this, you should at least have some ideas on how you can use quarto in your daily work routine. Happy coding and please feel free to contact me for any remarks or questions. I am happy to try and help."
  },
  {
    "objectID": "Quarto_intro.html#vscode",
    "href": "Quarto_intro.html#vscode",
    "title": "1  Introduction to Quarto",
    "section": "1.3 VSCode",
    "text": "1.3 VSCode\nMuch of the concepts as described in the RStudio tutorial above apply equally well to using Quarto in VSCode, just with a different interface to execute them.\nHere we will describe how to set up VSCode and Quarto, and how to preview and render Quarto objects within the VSCode interface.\nTo understand about more about the details of which files to edit etc., please see the RStudio description above.\n\n1.3.1 Getting Started\n\nInstall the Quarto CLI tool for your operating system from the Quarto Website\nInstall the VSCode Quarto extension\n\n\n\n1.3.2 Using Quarto\nThe basic workflow is as follows:\n\nCreate or modify .qmd objects etc as described above in the Rstudio section about Quarto markdown files\nWithin VSCode, make sure you’ve opened it in the repository containing all the files\nPress ctrl + shift + p to bring up your command palette\n\nTo preview a local ‘live’ version of HTML or website versions, you can type Quarto: preview. To close the live preview, press ctrl+c in the VSCode terminal.\nTo render all the files e.g. final HTML and/or PDF versions, you can type Quarto: Render Project, where you will be given different options depending on the formats defined in the _quarto.yml file.\n\n\nFor further VSCode integrations, just type Quarto: into your command palette and explore all the options."
  },
  {
    "objectID": "poseidon.html#the-components-of-poseidon",
    "href": "poseidon.html#the-components-of-poseidon",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.1 The components of Poseidon",
    "text": "2.1 The components of Poseidon\n\n\n\nOverview of the Poseidon framework\n\n\nPoseidon is an entire ecosystem build on top of the data format specification of the Poseidon package.\n\n2.1.1 The Poseidon package\nA Poseidon package bundles genotype data in EIGENSTRAT/PLINK format with human- and machine-readable meta-data.\n\n\n\nThe files in a Poseidon package\n\n\nIt includes sample-wise context information like spatio-temporal origin and genetic data quality in the .janno-file, literature in the .bib, and pointers to sequencing data in the .ssf file.\n.janno and .ssf have many predefined and specified columns, but can store arbitrary additional variables.\n\n\n2.1.2 The software tools\ntrident is a command line application to create, download, inspect and merge Poseidon packages – and therefore the central tool of the Poseidon framework. The init subcommand creates new packages from genotype data, fetch downloads them from the public archives through the Web-API, and forge merges and subsets them as specified. list gives an overview over entities in a set of packages and validate confirms their structural integrity.\nxerxes is derived from trident and allows to directly perform various basic and experimental genomic data analyses on Poseidon packages. It implements allele sharing statistics (\\(F_2\\), \\(F_3\\), \\(F_4\\), \\(F_{ST}\\)) with a flexible permutation interface.\njanno is an R package to simplify reading .janno files into R and the popular tidyverse ecosystem (Wickham et al. (2019)). It provides an S3 class janno that inherits from tibble.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\nqjanno is another command line tool to perform SQL queries on .janno files. On start-up it creates an SQLite database in memory and reads .janno files into it. It then sends any user-provided SQL query to the database server and forwards its output.\n\n\n2.1.3 The public archives\nThe Poseidon community maintains public archives for Poseidon packages to establish a central, open point of access for published, archaeogenetic genotype data.\n\nThe Community Archive: Author supplied, per-paper packages with the genotype data published in the respective papers. Partially pre-populated from various versions of the AADR.\nThe AADR Archive: Complete and structurally unified releases of the Allen Ancient DNA Resource (Mallick et al. (2023)) repackaged in the Poseidon package format.\nThe Minotaur Archive: Per-paper packages with genotype data reprocessed by the Minotaur workflow (see below).\n\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif Lazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023. “The Allen Ancient DNA Resource (AADR): A Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\nThe data is versioned with Git and hosted on GitHub for easy co-editing and automatic structural validation.\nIt can be accessed through a Web-API with various endpoints at server.poseidon-adna.org, e.g. /packages for a JSON list of packages in the community archive.\nThis API enables a little Archive explorer web app on the Poseidon website.\n\n\n2.1.4 The Minotaur workflow\nThe Minotaur workflow is a semi-automatic workflow to reproducibly process published sequencing data from the International Nucleotide Sequence Database Collaboration (INSDC) archives into Poseidon packages.\nCommunity members can request new packages by submitting a build recipe as a Pull Request against a dedicated submission GitHub repository. This recipe is derived from a Sequencing Source File (.ssf), describing the sequencing data for the package and where it can be downloaded.\nUsing the recipe, the sequencing data gets processed through nf-core/eager (Fellows Yates et al. (2021)) on computational infrastructure of MPI-EVA, using a standardised, yet flexible, set of parameters.\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\nThe generated genotypes, together with descriptive statistics of the sequencing data (Endogenous, Damage, Nr_SNPs, Contamination), are compiled into a Poseidon package, and made available to users in the Minotaur archive."
  },
  {
    "objectID": "poseidon.html#forging-a-dataset-with-trident",
    "href": "poseidon.html#forging-a-dataset-with-trident",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.2 Forging a dataset with trident",
    "text": "2.2 Forging a dataset with trident\nforge creates new Poseidon packages by extracting and merging packages, populations and individuals/samples from your Poseidon repositories. It can also work directly with your genotype data. In addition, forge allows merging of multiple data sets (packages), in contrast to mergeit which merges only two data sets at a time.\n(-f/--forgeString) can be used to query entire packages, groups or individuals. In general --forgeString query consists of multiple entities, inside \"\" separated by , .\n\nTo include all individuals in a Poseidon package, use * to surround the package title.*2019_Jeong_InnerEurasia* . In cases where only genotype files are available, use the file name prefix.\nTo include certain group(s) from a Poseidon package, simply add them to the -f query. No specific markers are required. Russia_HG_Karelia\nTo extract individuals only, surround them by &lt; and &gt;. &lt;ALA026&gt; . To exclude individuals add package name *package* and &lt;individual&gt; with a dash sign. \"*2021_Saag_EastEuropean-3.2.0*,-&lt;NIK003&gt;\"\n\n\ntrident forge \\\n  -p pileupcaller.double.geno \\\n  -d 2021_Saag_EastEuropean-3.2.0 \\\n  -d 2016_FuNature-2.1.1 \\\n  -f \"*pileupcaller.double*,Russia_AfontovaGora3,&lt;NIK003&gt;\" \\\n  -o testpackage \\\n  --outFormat EIGENSTRAT \\\n  /\nForge selection language\nforge has a an optional flag --intersect, that defines, if the genotype data from different packages should be merged with an intersect instead of the default union operation. The default is to output the union of all SNPs, by setting the additional SNPs from the other merged package as missing in the samples that did not have them originally. This option is useful for making a data set based on Human Origins (HO) SNPs for analysis like PCA and ADMIXTURE.\ntrident forge \\\n  --intersect \\\n  -p pileupcaller.double.geno \\\n  -d 2012_PattersonGenetics-2.1.3 \\\n  -o testpackage_HO \\\n  --outFormat EIGENSTRAT \\\n  /\nIn case of PCA, --forgeFile can be used to merge necessary populations/groups from the available packages in the community archive to create specific PCA configurations.\ntrident forge \\\n  -d /path/to/community/archive \\\n  --forgeFile WestEurasia_poplist.txt \\\n  -o WE_PCA \\\n  /\nIn addition, --selectSnps allows to provide forge with a SNP file in EIGENSTRAT (.snp) or PLINK (.bim) format to create a package with a specific selection. This option generates a package with exactly the SNPs listed in this file."
  },
  {
    "objectID": "poseidon.html#contributing-to-the-community-archive",
    "href": "poseidon.html#contributing-to-the-community-archive",
    "title": "2  Genotype and context data management with Poseidon",
    "section": "2.3 Contributing to the community archive",
    "text": "2.3 Contributing to the community archive\n\n\n\nPoseidon needs your data as soon as it is published\n\n\nTo maintain the public data archives, specifically the community archive and the minotaur archive, Poseidon depends on work donations by an interested community.\nMany practitioners of archaeogenetics both produce genotype data from archaological contexts and require the reference data from other publications, provided in public archives, to contextualize it.\nIf authors themselves provide high-quality, easily accessible versions of their data beyond the raw data available at the INSDC databases, they gain at least three important advantages:\n\nTheir work will be easily findable and potentially cited more often.\nThey have primacy over how their data is communicated. That is, which genotypes, dates or group names they consider correct.\nTheir results for derived, genotype based analyses (PCA, F-Statistics, etc.) can be reproduced exactly.\n\nAnd the whole community wins, because sharing the tedious data preparation tasks empowers all researchers to achieve more in shorter time.\n\n\n\n\nThis tutorial explains the main cornerstones of a workflow to add a new Poseidon package to the community archive after publishing the corresponding dataset. The process is documented in more detail in a Submission guide on the website.\n\nMake yourself familiar with a number of core technologies. This is less daunting than it sounds, because: Superficial knowledge is sufficient and knowing them is useful beyond this particular task.\n\n\nCreating and validating Poseidon packages with the trident tool.\nFree and open source distributed version control with Git.\nCollaborative working on Git projects with GitHub.\nHandling large files in Git using Git LFS.\n\n\nCreate a package from your genotype data and fill it with a suitable set of meta and context information.\n\n\ntrident init allows to wrap genotype data in a dummy Poseidon package. Imagine we had genotype data for a number of individuals in EIGENSTRAT format:\n\n\nmyData.ind\n\nSample1  M       ExamplePop1\nSample2  F       ExamplePop1\nSample3  M       ExamplePop2\n\n\n\nmyData.snp\n\n           rs3094315     1        0.020130          752566 G A\n          rs12124819     1        0.020242          776546 A G\n          rs28765502     1        0.022137          832918 T C\n\n\n\nmyData.geno\n\n099\n922\n999\n\nWith trident init -p myData.geno -o myPackage we can create a basic package around this data.\n$ ls myPackage\nmyData.geno  myData.snp     myPackage.janno\nmyData.ind   myPackage.bib  POSEIDON.yml\nIn a next step we modify POSEIDON.yml, .janno and .bib to include the context information we consider relevant. All of these files are well specified and documented, so we only demonstrate a minimal change for this example:\nWe replace the main contributor for the package.\n\n\nmyPackage/POSEIDON.yml\n\nposeidonVersion: 2.7.1\ntitle: myPackage\ndescription: Empty package template. Please add a description\ncontributor:\n- name: Clemens Schmid               #- name: Josiah Carberry\n  email: clemens_schmid@eva.mpg.de   #  email: carberry@brown.edu\n  orcid: 0000-0003-3448-5715         #  orcid: 0000-0002-1825-0097\npackageVersion: 0.1.0\nlastModified: 2023-10-18\ngenotypeData:\n  format: EIGENSTRAT\n  genoFile: myData.geno\n  snpFile: myData.snp\n  indFile: myData.ind\n  snpSet: Other\njannoFile: myPackage.janno\nbibFile: myPackage.bib\n\nWhen we applied all necessary modifications we can confirm that the package is still valid with trident validate -d myPackage.\n\n\nSubmit the package to the community archive.\n\n\nTo submit the package we have to create a fork of the community archive repository on GitHub. This requires a GitHub account.\n\n\n\n\nPress the fork button in the top right corner to fork a repository on GitHub\n\n\n\nAnd then clone the fork to our computer, while omitting the large genotype data files. Note that this requires several setup steps to work correctly:\n\nGit has be installed for your computer (see here)\nYou must have created an ssh key pair to connect to GitHub via ssh (see here)\nGit LFS has to be installed (see here) and and configured for your user with git lfs install\n\nGIT_LFS_SKIP_SMUDGE=1 git clone git@github.com:&lt;yourGitHubUserName&gt;/community-archive.git\nWith the cloned repository on our system we can copy the files into the repositories directory and commit the changes.\n\n\nin the community-archive directory\n\ncp -r ../myPackage myPackage\ngit add myPackage\ngit commit -m \"added a first draft of myPackage\"\ngit push\n\nIn a last step we can open a Pull Request on GitHub from our fork to the original archive repository. Poseidon core members will take it from here.\n\n\n\n\nWhen you pushed to your fork, GitHub will automatically offer to “contribute” to the source repository"
  },
  {
    "objectID": "fstats.html#admixture---f3-statistics",
    "href": "fstats.html#admixture---f3-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.1 Admixture - F3 Statistics",
    "text": "3.1 Admixture - F3 Statistics\nF3 statistics are a useful analytical tool to understand population relationships. F3 statistics, just as F4 and F2 statistics measure allele frequency correlations between populations and were introduced by Nick Patterson (Patterson et al. 2012), but see also (Peter 2016) for another introduction.\nF3 statistics are used for two purposes: i) as a test whether a target population (C) is admixed between two source populations (A and B), and ii) to measure shared drift between two test populations (A and B) from an outgroup (C).\nF3 statistics are in both cases defined as the product of allele frequency differences between population C to A and B, respectively:\n\\[F3(A,B;C)=\\langle(c−a)(c−b)\\rangle\\]\nHere, \\(\\langle\\cdot\\rangle\\) denotes the average over all genotyped sites, and a, b and c denote the allele frequency for a given site in the three populations A, B and C.\nIt can be shown that if \\(F3(A, B; C)\\) is negative, it provides unambiguous proof that population C is admixed between populations A and B, as in the following phylogeny (taken from Figure 1 from (Patterson et al. 2012):\n\nIntuitively, an F3 statistics becomes negative if the allele frequency of the target population C is on average intermediate between the allele frequencies of A and B. Consider as an extreme example a genomic site where \\(a=0\\), \\(b=1\\) and \\(c=0.5\\). Then we have \\((c−a)(c−b)=−0.25\\), which is negative. So if the entire statistics is negative, it suggests that in many positions, the allele frequency c is indeed intermediate, suggesting admixture between the two sources.\nOne way to understand this is by looking what happens to a list of SNPs and allele frequencies for groups A, B and C:\n\n\n\nSNP\nA\nB\nC\n\\((c-a)(c-b)\\)\n\n\n\n\n1\n1\n0\n0.5\n-0.25\n\n\n2\n0.8\n0\n0\n0\n\n\n3\n0\n0.7\n0\n0\n\n\n4\n0.1\n0.5\n0.3\n-0.04\n\n\n5\n0\n0.1\n0.2\n0.02\n\n\n6\n1\n0.2\n0.9\n-0.07\n\n\n\\(F_3(A, B;C)\\)\n\n\n\n-0.057\n\n\n\nEvery SNP where C has an allele frequency intermediate between A and B contributes negatively. Here, the average is also negative, providing evidence for admixture. For statistical certainty, an error bar for this estimate is needed, which is typically computed via Jackknife (see for example the xerxes whitepaper).\n\n\n\n\n\n\nCaution\n\n\n\nIf an F3 statistics is not negative, it does not proof that there is no admixture!\n\n\n\n3.1.1 Computing Admixture-F3 with xerxes\nWe will use this statistics to test if Finnish are admixed between East and West, using different Eastern and Western sources. In the West, we use French, Icelandic, Lithuanian and Norwegian as source, and in the East we use Nganasan and one of the ancient individuals analysed in (Lamnidis et al. 2018), from the site of Bolshoy Oleni Ostrov from the Northern Russian Kola-peninsula, and dating to 3500 years before present.\n\n\n\n\n\n\nTip\n\n\n\nIf you happen to have downloaded a copy of the Poseidon Community Archive already, then just use the path to that archive in the following commands. Otherwise you download the entire archive via\n\ntrident fetch -d /somewhere/to/store/the/archive --downloadAll \n\nor just the relevant packages for the examples in this chapter:\n\ntrident fetch -d /somewhere/to/store/the/archive -f \"*2012_PattersonGenetics*,*2014_RaghavanNature*,*2014_LazaridisNature*,*2018_Lamnidis_Fennoscandia*\"\n\n\n\nWe use the software xerxes fstats from the Poseidon Framework. Here is a command line that computes 8 statistics for us:\n\nxerxes fstats -d ~/dev/poseidon-framework/community-archive \\\n    --stat \"F3(Nganasan,French,Finnish)\" \\\n    --stat \"F3(Nganasan, Icelandic, Finnish)\" \\\n    --stat \"F3(Nganasan, Lithuanian, Finnish)\" \\\n    --stat \"F3(Nganasan, Norwegian, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, French, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Icelandic, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Lithuanian, Finnish)\" \\\n    --stat \"F3(Russia_Bolshoy, Norwegian, Finnish)\" \\\n\n\n\n\n\n\n\nNote\n\n\n\nNote that xerxes fstats will automatically find the right packages from your local archive that contain these groups. You can see in the output of the program which packages contribute:\n[Info]    5 relevant packages for chosen statistics identified:\n[Info]    *2012_PattersonGenetics-2.1.3*\n[Info]    *2014_LazaridisNature-4.0.2*\n[Info]    *2016_LazaridisNature-2.1.3*\n[Info]    *2018_Lamnidis_Fennoscandia-2.1.0*\n[Info]    *2019_Flegontov_PalaeoEskimo-2.2.1*\nSo these five packages contain the samples requested in these statistics. You can inquire about this also more manually using trident list\n\n\nHere is the result that you should get, nicely layouted in a Text-table:\n.-----------.----------------.------------.---------.---.---------.----------------.--------------------.------------------.---------------------.\n| Statistic |       a        |     b      |    c    | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife |  Z_score_Jackknife  |\n:===========:================:============:=========:===:=========:================:====================:==================:=====================:\n| F3        | Nganasan       | French     | Finnish |   | 593124  | -1.0450e-3     | -1.0451e-3         | 1.2669e-4        | -8.249133659451905  |\n| F3        | Nganasan       | Icelandic  | Finnish |   | 593124  | -1.1920e-3     | -1.1920e-3         | 1.3381e-4        | -8.908381869946188  |\n| F3        | Nganasan       | Lithuanian | Finnish |   | 593124  | -1.1605e-3     | -1.1605e-3         | 1.5540e-4        | -7.4680182465607245 |\n| F3        | Nganasan       | Norwegian  | Finnish |   | 593124  | -1.0913e-3     | -1.0914e-3         | 1.3921e-4        | -7.83945981272796   |\n| F3        | Russia_Bolshoy | French     | Finnish |   | 542789  | -6.1807e-4     | -6.1809e-4         | 1.0200e-4        | -6.059872900228102  |\n| F3        | Russia_Bolshoy | Icelandic  | Finnish |   | 542789  | -6.2801e-4     | -6.2802e-4         | 1.1792e-4        | -5.325695961373772  |\n| F3        | Russia_Bolshoy | Lithuanian | Finnish |   | 542789  | -3.7310e-4     | -3.7310e-4         | 1.2973e-4        | -2.8760029685791637 |\n| F3        | Russia_Bolshoy | Norwegian  | Finnish |   | 542789  | -3.7646e-4     | -3.7653e-4         | 1.1630e-4        | -3.2375830440434323 |\n'-----------'----------------'------------'---------'---'---------'----------------'--------------------'------------------'---------------------'\n\n\n\n\n\n\nTip\n\n\n\nUse the option -f &lt;FILE&gt; to output the results additionally to a tab-separated file, or --raw if you prefer the standard output to be tab-separated\n\n\nYou can see that in all cases this statistic is negative (Estimate_Total). The next two columns ( Estimate_Jackknife and StdErr_Jackknife) are computed using Jackknifing (see the xerxes whitepaper for details). The last column is the Z score, and it is important here: It gives the deviation of the f3 statistic from zero in units of the standard error. As general rule, a Z score of -3 or more suggests a significant rejection of the Null hypothesis that the statistic is not negative. In this case, all of the statistics are significantly negative (with one borderline exception), proving that Finnish have ancestral admixture of East and West Eurasian ancestry. Here, Eastern ancestry is represented by Nganasan or Russia_Bolshoy, respectively, while Western ancestry by French, Icelandic, Lithuanian and Norwegian, respectively. Note that the statistics does not suggest when this admixture happened!\n\n\n3.1.2 Running xerxes via a configuration file\nAs you will have noticed, the command line above is getting quite long, since a separate --stat option has to be entered for every statistic to be computed. There is a more powerful and elegant interface to xerxes, which uses a configuration file in YAML format. To illustrate it, let us consider the configuration file (which can be found in fstats_working/F3_finnish.config in the git-repository of this book) needed to compute the same statistic as above:\nfstats:\n- type: F3\n  a: [\"Nganasan\", \"Russia_Bolshoy\"]\n  b: [\"French\", \"Icelandic\", \"Lithuanian\", \"Norwegian\"]\n  c: [\"Finnish\"]\nYou can then run xerxes as\n\nxerxes fstats -d ~/dev/poseidon-framework/community-archive --statConfig fstats_working/F3_finnish.config\n\nSo xerxes then automatically creates all combinations of populations listed in slots a, b and c.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are actually three types of F3-statistics supported by xerxes:\n\nF3vanilla: The purest form, defined literally as \\(\\langle(c−a)(c−b)\\rangle\\)\nF3: A bias-corrected version, which is only valid for groups in C that have non-zero heterozygosity\nF3star: This one is normalised by the heterozgygosity of the third population, C, as suggested in (Patterson et al. 2012) and implemented in the Admixtools package.\n\nThe white-paper explains this in detail.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe configuration file format has a lot more options. Here is a bit more complex example, but see also the documentation:\n# You can define groups right within the configuration file.\n# here we use negative selection to remove individuals from the\n# newly defined groups\ngroupDefs:\n  CEU2: [\"CEU.SG\", \"-&lt;NA12889.SG&gt;\", \"-&lt;NA12890.SG&gt;\"]\n  FIN2: [\"FIN.SG\", \"-&lt;HG00383.SG&gt;\", \"-&lt;HG00384.SG&gt;\"]\n  GBR2: [\"GBR.SG\", \"-&lt;HG01791.SG&gt;\", \"-&lt;HG02215.SG&gt;\"]\n  IBS2: [\"IBS.SG\", \"-&lt;HG02238.SG&gt;\", \"-&lt;HG02239.SG&gt;\"]\nfstats:\n- type: F2 # this will create 2x2 = 4 F2-Statistics\n  a: [\"French\", \"Spanish\"]\n  b: [\"Han\", \"CEU2\"]\n- type: F3vanilla # This will create 3x2x1 = 6 Statistics\n  a: [\"French\", \"Spanish\", \"Mbuti\"]\n  b: [\"Han\", \"CEU2\"]\n  c: [\"&lt;Chimp.REF&gt;\"]\n- type: F4 # This will create 5x5x4x1 = 100 Statistics\n  a: [\"&lt;I0156.SG&gt;\", \"&lt;I0157.SG&gt;\", \"&lt;I0159.SG&gt;\", \"&lt;I0160.SG&gt;\", \"&lt;I0161.SG&gt;\"]\n  b: [\"&lt;I0156.SG&gt;\", \"&lt;I0157.SG&gt;\", \"&lt;I0159.SG&gt;\", \"&lt;I0160.SG&gt;\", \"&lt;I0161.SG&gt;\"]\n  c: [\"CEU2\", \"FIN2\", \"GBR2\", \"IBS2\"]\n  d: [\"&lt;Chimp.REF&gt;\"]\n# Altogether: 110 statistics of different types\nwhich will not just create multiple statistic using row-combinations, as described, but also uses newly defined groups and combines multiple statistic types (F2, F3 and F4) in one run."
  },
  {
    "objectID": "fstats.html#f4-statistics",
    "href": "fstats.html#f4-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.2 F4 Statistics",
    "text": "3.2 F4 Statistics\nA different way to test for admixture is by “F4 statistics” (or “D statistics” which is very similar), also introduced in (Patterson et al. 2012).\nF4 statistics are also defined in terms of correlations of allele frequency differences, similarly to F3 statistics, but involving four different populations, not just three. Specifically we define\n\\[F4(A,B;C,D)=\\langle(a−b)(c−d)\\rangle.\\]\n\n3.2.1 Shaping intuition - the ABBA- and BABA-sites\nTo understand the statistics, consider the following tree:\n\nIn this tree, without any additional admixture, the allele frequency difference between A and B should be completely independent from the allele frequency difference between C and D. In that case, F4(A, B; C, D) should be zero, or at least not statistically different from zero. However, if there was gene flow from C or D into A or B, the statistic should be different from zero. Specifically, if the statistic is significantly negative, it implies gene flow between either C and B, or D and A. If it is significantly positive, it implies gene flow between A and C, or B and D.\nIt is helpful to again consider an example using a SNP list, this time assuming that every population is just a single (haploid) individual, so each allele frequency can just be 0 or 1. For example:\n\n\n\nSNP\nA\nB\nC\nD\n\\((a-b)(c-d)\\)\n\n\n\n\n1\n1\n0\n0\n0\n0\n\n\n2\n1\n0\n1\n1\n0\n\n\n3\n0\n1\n1\n0\n-1\n\n\n4\n0\n1\n0\n1\n1\n\n\n5\n1\n0\n0\n1\n-1\n\n\n6\n1\n0\n0\n0\n0\n\n\n\\(F_4(A, B;C, D)\\)\n\n\n\n\n-0.0167\n\n\n\nYou can see that the only SNPs that contribute positively to this statistics are SNPs where the alleles are distributed as 1010 and 0101, and the only SNPs that contribute negatively are 1001 and 0110. In the literature, the two patterns have been dubbed “ABBA” and “BABA”, which is why the statistical test behind this statistic (see below) was sometimes called the ABBA-BABA test (see for example (Martin, Davey, and Jiggins 2015)).\nThe intuition here is straight-forward: In positions that are polymorphic in both \\((A,B)\\) and \\((C,D)\\), this statistic asks whether B is genetically more similar to C than it is to D. This is most useful as a test for “treeness”: If A, B, C, D are related to each other as indicated in the above tree, then C should be equally closely related to C as to D. But if we actually find evidence that B is closer to C than to D, or vice versa, then this means that the tree above cannot be correct, but that there must be a closer connection between B and C or B and D, depending on the sign of the statistic.\n\n\n3.2.2 From single samples to allele frequencies\nSo the ABBA- and BABA-categories of SNPs help shape intuition for how this statistic behaves for single haploid genomes. But what about population allele frequencies? Looking back at the formula \\(\\langle(a−b)(c−d)\\rangle\\) this doesn’t help very much with intuition how this behaves with frequencies. Well, a nice feature of F4-Statistics is that averages factor out. This means, that if you have multiple samples in one or multiple slots A, B, C or D, the total F4-statistic of the groups is exactly equal to the average of F4-Statistics of the individuals. Here is a more mathematical definition.\nLet’s say we have 2 individuals in each of A and B, so we may perhaps write \\(A=\\{A_1,A_2\\}\\) and \\(B=\\{B_1,B_2\\}\\). Then one can show to have\n\\[F4(A, B; C, D) = \\text{Average of}[F4(A_1, B_1; C, D), F4(A_1, B_2; C, D), F4(A_2, B_1; C, D), F4(A_2, B_2; C, D)]\\]\nso just thte average over all individual-based F4-statistics. And this can be shown to be true for arbitrary numbers of samples. So in other words: An F4-Statistic always measures the average excess of pairwise BABA SNPs over ABBA SNPs. To me, this is a useful insight, as I find thinking in terms of ABBA-BABA somehow more helpful than thinking in terms of correlations of allele-frequency differences (which is really what the original formula is).\n\n\n\n\n\n\nNote\n\n\n\nF4-statistics have been famously used to show that Neanderthals are more closely related to Non-African populations than to Africans, suggesting gene-flow between Neanderthals and Non-Africans (shown in (Green et al. 2010)). You can reproduce this famous result with\n\nxerxes fstats -d ~/poseidon_repo --stat 'F4(&lt;Chimp.REF&gt;,&lt;Altai_published.DG&gt;,Yoruba,French)' \\\n  --stat 'F4(&lt;Chimp.REF&gt;,&lt;Altai_published.DG&gt;,Sardinian,French)'\n\nwhich shows that the first statistic is significantly positive with a Z-score of 7.99, while the second one is insignificantly different from zero (Z=1.01)\n\n\nThe way this statistic is often used, is to put a divergent outgroup as population A, for which we know for sure that there was no admixture into either C or D. With this setup, we can then test for gene flow between B and D (if the statistic is positive), or B and C (if it is negative).\n\n\n3.2.3 Running F4-Statistics with xerxes\nHere, we can use this statistic to test for East Asian admixture in Finns, similarly to the test using Admixture F3 statistics above. We will again use xerxes fstats. We again prepare a configuration file (in fstats_working/F4_finish.config in the git-repository of this book), this time with four populations (A, B, C, D):\nfstats:\n- type: F4\n  a: [\"Mbuti\"]\n  b: [\"Nganasan\", \"Russia_Bolshoy\"]\n  c: [\"French\", \"Icelandic\", \"Lithuanian\", \"Norwegian\"]\n  d: [\"Finnish\"]\nYou can again run via\n\nxerxes fstats -d ~/dev/poseidon-framework/community-archive --statConfig fstats_working/F4_finnish.config\n\nThe result is:\n.-----------.-------.----------------.------------.---------.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |   a   |       b        |     c      |    d    | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=======:================:============:=========:=========:================:====================:==================:====================:\n| F4        | Mbuti | Nganasan       | French     | Finnish | 593124  | 2.3114e-3      | 2.3115e-3          | 1.2676e-4        | 18.235604067907143 |\n| F4        | Mbuti | Nganasan       | Icelandic  | Finnish | 593124  | 1.6590e-3      | 1.6590e-3          | 1.4861e-4        | 11.163339072181776 |\n| F4        | Mbuti | Nganasan       | Lithuanian | Finnish | 593124  | 1.3290e-3      | 1.3290e-3          | 1.4681e-4        | 9.052979707622278  |\n| F4        | Mbuti | Nganasan       | Norwegian  | Finnish | 593124  | 1.6503e-3      | 1.6503e-3          | 1.5358e-4        | 10.745850997260929 |\n| F4        | Mbuti | Russia_Bolshoy | French     | Finnish | 542789  | 1.8785e-3      | 1.8785e-3          | 1.2646e-4        | 14.854487416366263 |\n| F4        | Mbuti | Russia_Bolshoy | Icelandic  | Finnish | 542789  | 1.0829e-3      | 1.0828e-3          | 1.4963e-4        | 7.236818881873822  |\n| F4        | Mbuti | Russia_Bolshoy | Lithuanian | Finnish | 542789  | 5.4902e-4      | 5.4907e-4          | 1.4601e-4        | 3.7605973064589096 |\n| F4        | Mbuti | Russia_Bolshoy | Norwegian  | Finnish | 542789  | 9.3473e-4      | 9.3475e-4          | 1.5302e-4        | 6.108881868125652  |\n'-----------'-------'----------------'------------'---------'---------'----------------'--------------------'------------------'--------------------'\nAs you can see, in all cases, the Z score is positive and larger than 3, indicating a significant deviation from zero, and implying gene flow between Nganasan and Finnish, and BolshoyOleniOstrov and Finnish, when compared to French, Icelandic, Lithuanian or Norwegian."
  },
  {
    "objectID": "fstats.html#outgroup-f3-statistics",
    "href": "fstats.html#outgroup-f3-statistics",
    "title": "3  Introduction to F3- and F4-Statistics",
    "section": "3.3 Outgroup-F3-Statistics",
    "text": "3.3 Outgroup-F3-Statistics\nOutgroup F3 statistics are a special case how to use F3 statistics. The definition is the same as for Admixture F3 statistics, but instead of a target C and two source populations A and B, one now gives an outgroup C and two test populations A and B.\nTo get an intuition for this statistics, consider the following tree:\n\nIn this scenario, the statistic F3(A, B; C) measures the branch length from C to the common ancestor of A and B, coloured red. So this statistic is simply a measure of how closely two population A and B are related with each other, as measured from a distant outgroup. It is thus a similarity measure: The higher the statistic, the more genetically similar A and B are to one another.\nHere is again a SNP table to illustrate, using haploid individuals:\n\n\n\nSNP\nA\nB\nC\n\\((c-a)(c-b)\\)\n\n\n\n\n1\n1\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n\n\n3\n0\n0\n1\n1\n\n\n4\n1\n0\n1\n0\n\n\n5\n1\n1\n0\n1\n\n\n6\n0\n0\n1\n1\n\n\n\\(F_3(A, B;C)\\)\n\n\n\n0.5\n\n\n\nYou can see that each position which is similar between A and B, but different to C contributes 1, all other SNPs 0. So it directly measures similarity between A and B on alleles that differ from the outgroup C.\n\n\n\n\n\n\nNote\n\n\n\nNote that the averaging-relation shown for F4 statistics above is also true for Outgroup-F3 statistics, but only for populations A and B, not for C. So if you have multiple samples in A and B, you may think of this statistic being the average over all pairwise nucleotide similarities between individuals in A and B with respect to the same outgroup C.\n\n\nWe can use this statistic to measure for example the genetic affinity to East Asia, by performing the statistic F3(Han, X; Mbuti), where Mbuti is a distant African population and acts as outgroup here, Han denote Han Chinese, and X denotes various European populations that we want to test.\nYou can again define a configuration file that performs looping over various populations X for you:\nfstats:\n- type: F3\n  a: [\"Han\"]\n  b: [\"Chuvash\", \"Albanian\", \"Armenian\", \"Bulgarian\", \"Czech\", \"Druze\", \"English\",\n      \"Estonian\", \"Finnish\", \"French\", \"Georgian\", \"Greek\", \"Hungarian\", \"Icelandic\",\n      \"Italian_North\", \"Italian_South\", \"Lithuanian\", \"Maltese\", \"Mordovian\", \"Norwegian\",\n      \"Orcadian\", \"Russian\", \"Sardinian\", \"Scottish\", \"Sicilian\", \"Spanish_North\",\n      \"Spanish\", \"Ukrainian\", \"Finland_Levanluhta\", \"Russia_Bolshoy\", \"Russia_Chalmny_Varre\", \"Saami.DG\"]\n  c: [\"Mbuti\"]\nwhich cycles through many populations from Europe, including the ancient individuals from Chalmny Varre, Bolshoy Oleni Ostrov and Levänluhta (described in (Lamnidis et al. 2018)). We store this file in a file called fstats_working/OutgroupF3_europe.config and run via:\n\nxerxes fstats --statConfig fstats_working/OutgroupF3_europe.config -d ~/dev/poseidon-framework/community-archive -f fstats_working/outgroupf3_europe.tsv\n\n\n\n\n\n\n\nWarning\n\n\n\nOften in Outgroup-F3-statistics you use single genomes for population C, sometimes even single haploid genomes. In this case, F3 and F3star will get undefined results, because ordinary F3 and F3star statistics require population C to have non-zero average heterozygosity, so you will need at least one diploid sample, or multiple haploid or diploid samples.\nUse F3vanilla if your third population C is a single pseudo-haploid sample.\n\n\nHere is the output of this run (but note that a tab-separated version was also stored in fstats_working/outgroupf3_europe.tsv using the option -f):\n.-----------.-----.----------------------.-------.---.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |  a  |          b           |   c   | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=====:======================:=======:===:=========:================:====================:==================:====================:\n| F3        | Han | Chuvash              | Mbuti |   | 593124  | 5.3967e-2      | 5.3967e-2          | 5.0668e-4        | 106.51180329550319 |\n| F3        | Han | Albanian             | Mbuti |   | 593124  | 4.9972e-2      | 4.9973e-2          | 4.9520e-4        | 100.91326321202445 |\n| F3        | Han | Armenian             | Mbuti |   | 593124  | 4.9531e-2      | 4.9531e-2          | 4.7771e-4        | 103.68366652942314 |\n| F3        | Han | Bulgarian            | Mbuti |   | 593124  | 5.0103e-2      | 5.0103e-2          | 4.8624e-4        | 103.04188532686614 |\n| F3        | Han | Czech                | Mbuti |   | 593124  | 5.0536e-2      | 5.0536e-2          | 4.9261e-4        | 102.58792370749681 |\n| F3        | Han | Druze                | Mbuti |   | 593124  | 4.8564e-2      | 4.8564e-2          | 4.6788e-4        | 103.79674299622445 |\n| F3        | Han | English              | Mbuti |   | 593124  | 5.0280e-2      | 5.0281e-2          | 4.9183e-4        | 102.23198323949656 |\n| F3        | Han | Estonian             | Mbuti |   | 593124  | 5.1154e-2      | 5.1155e-2          | 5.0350e-4        | 101.59882496016485 |\n| F3        | Han | Finnish              | Mbuti |   | 593124  | 5.1784e-2      | 5.1784e-2          | 5.0603e-4        | 102.33488758899031 |\n| F3        | Han | French               | Mbuti |   | 593124  | 5.0207e-2      | 5.0208e-2          | 4.8552e-4        | 103.40976592749682 |\n| F3        | Han | Georgian             | Mbuti |   | 593124  | 4.9711e-2      | 4.9711e-2          | 4.8100e-4        | 103.34881140790415 |\n| F3        | Han | Greek                | Mbuti |   | 593124  | 4.9874e-2      | 4.9874e-2          | 4.8994e-4        | 101.79554640756365 |\n| F3        | Han | Hungarian            | Mbuti |   | 593124  | 5.0497e-2      | 5.0498e-2          | 4.9878e-4        | 101.24215699276706 |\n| F3        | Han | Icelandic            | Mbuti |   | 593124  | 5.0680e-2      | 5.0680e-2          | 4.9729e-4        | 101.91303336514295 |\n| F3        | Han | Italian_North        | Mbuti |   | 593124  | 4.9903e-2      | 4.9904e-2          | 4.8436e-4        | 103.03094306099203 |\n| F3        | Han | Italian_South        | Mbuti |   | 592980  | 4.9201e-2      | 4.9201e-2          | 5.1170e-4        | 96.15239597674244  |\n| F3        | Han | Lithuanian           | Mbuti |   | 593124  | 5.0896e-2      | 5.0896e-2          | 5.0638e-4        | 100.50984037418753 |\n| F3        | Han | Maltese              | Mbuti |   | 593124  | 4.8751e-2      | 4.8751e-2          | 4.7500e-4        | 102.63442479673623 |\n| F3        | Han | Mordovian            | Mbuti |   | 593124  | 5.1820e-2      | 5.1820e-2          | 4.8853e-4        | 106.07409963190884 |\n| F3        | Han | Norwegian            | Mbuti |   | 593124  | 5.0724e-2      | 5.0724e-2          | 4.9514e-4        | 102.4454387098217  |\n| F3        | Han | Orcadian             | Mbuti |   | 593124  | 5.0469e-2      | 5.0469e-2          | 4.9485e-4        | 101.98814656611475 |\n| F3        | Han | Russian              | Mbuti |   | 593124  | 5.1277e-2      | 5.1277e-2          | 4.8613e-4        | 105.48070801791317 |\n| F3        | Han | Sardinian            | Mbuti |   | 593124  | 4.9416e-2      | 4.9417e-2          | 4.8908e-4        | 101.04049389691913 |\n| F3        | Han | Scottish             | Mbuti |   | 593124  | 5.0635e-2      | 5.0635e-2          | 5.0565e-4        | 100.13962104744425 |\n| F3        | Han | Sicilian             | Mbuti |   | 593124  | 4.9194e-2      | 4.9194e-2          | 4.8157e-4        | 102.15353663091187 |\n| F3        | Han | Spanish_North        | Mbuti |   | 593124  | 5.0032e-2      | 5.0032e-2          | 4.9377e-4        | 101.32594226555439 |\n| F3        | Han | Spanish              | Mbuti |   | 593124  | 4.9693e-2      | 4.9693e-2          | 4.8551e-4        | 102.35200847948641 |\n| F3        | Han | Ukrainian            | Mbuti |   | 593124  | 5.0731e-2      | 5.0731e-2          | 4.9506e-4        | 102.47529111692852 |\n| F3        | Han | Finland_Levanluhta   | Mbuti |   | 303033  | 5.4488e-2      | 5.4488e-2          | 5.7681e-4        | 94.46487653920919  |\n| F3        | Han | Russia_Bolshoy       | Mbuti |   | 542789  | 5.7273e-2      | 5.7273e-2          | 5.2875e-4        | 108.31739594898687 |\n| F3        | Han | Russia_Chalmny_Varre | Mbuti |   | 428215  | 5.4000e-2      | 5.4000e-2          | 5.6936e-4        | 94.84371082564112  |\n| F3        | Han | Saami.DG             | Mbuti |   | 585193  | 5.4727e-2      | 5.4728e-2          | 5.5546e-4        | 98.5265149143263   |\n'-----------'-----'----------------------'-------'---'---------'----------------'--------------------'------------------'--------------------'\nNow it’s time to plot these results using R. Let’s first read in the table:\n\nd &lt;- read.csv(\"fstats_working/outgroupf3_europe.tsv\", sep = \"\\t\")\n\nWe can check that it worked:\n\nhead(d)\n\n  Statistic   a         b     c  d NrSites Estimate_Total Estimate_Jackknife\n1        F3 Han   Chuvash Mbuti NA  593124       0.053967           0.053967\n2        F3 Han  Albanian Mbuti NA  593124       0.049972           0.049973\n3        F3 Han  Armenian Mbuti NA  593124       0.049531           0.049531\n4        F3 Han Bulgarian Mbuti NA  593124       0.050103           0.050103\n5        F3 Han     Czech Mbuti NA  593124       0.050536           0.050536\n6        F3 Han     Druze Mbuti NA  593124       0.048564           0.048564\n  StdErr_Jackknife Z_score_Jackknife\n1       0.00050668          106.5118\n2       0.00049520          100.9133\n3       0.00047771          103.6837\n4       0.00048624          103.0419\n5       0.00049261          102.5879\n6       0.00046788          103.7967\n\n\nNice, now on to plotting (here I’m using Base R for zero-dependency pain, you’re welcome!):\n\norder &lt;- order(d$Estimate_Total) # order the estimates for visual effect\nx &lt;- d$Estimate_Jackknife[order]\nxErr &lt;- d$StdErr_Jackknife[order]\ny &lt;- seq_along(d$b)\nplot(x, y, xlab = \"Z Score\", ylab = NA, yaxt = \"n\", # plot no y-axis ticks\n     xlim = c(0.048,0.06),\n     main = \"F3(Han, X; Mbuti)\")\n# plot the labels\ntext(x + 0.001, y, labels = d$b, adj=0)\n# plot the error bars\narrows(x - xErr, y, x + xErr, y, length=0.05, angle=90, code=3)\n\n\n\n\n\n\n\n\nAs expected, the ancient samples and modern Saami are the ones with the highest allele sharing with present-day East Asians (as represented by Han) compared to many other Europeans.\n\n\n\n\nGreen, Richard E, Johannes Krause, Adrian W Briggs, Tomislav Maricic, Udo Stenzel, Martin Kircher, Nick Patterson, et al. 2010. “A Draft Sequence of the Neandertal Genome.” Science 328 (5979): 710–22. http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=20448178&retmode=ref&cmd=prlinks.\n\n\nLamnidis, Thiseas C, Kerttu Majander, Choongwon Jeong, Elina Salmela, Anna Wessman, Vyacheslav Moiseyev, Valery Khartanovich, et al. 2018. “Ancient Fennoscandian Genomes Reveal Origin and Spread of Siberian Ancestry in Europe.” Nature Communications 9 (1): 5018. https://doi.org/10.1038/s41467-018-07483-5.\n\n\nMartin, Simon H, John W Davey, and Chris D Jiggins. 2015. “Evaluating the Use of ABBA-BABA Statistics to Locate Introgressed Loci.” Molecular Biology and Evolution 32 (1): 244–57. https://doi.org/10.1093/molbev/msu269.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\n\n\nPeter, Benjamin M. 2016. “Admixture, Population Structure, and F-Statistics.” Genetics 202 (4): 1485–1501. https://doi.org/10.1534/genetics.115.183913."
  },
  {
    "objectID": "eager.html#why-do-we-need-nf-coreeager",
    "href": "eager.html#why-do-we-need-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.1 Why do we need nf-core/eager?",
    "text": "4.1 Why do we need nf-core/eager?\n\nCompared to other Next-Generation Sequencing data, the chemical structure and increased risk of present-day contamination require methods specialized for ancient DNA in both the wet and the dry lab.\nEnsuring the authenticity of ancient genomic data is one of the main focuses of bioinformatic tools developed for the study of ancient DNA and underlines the necessity of reproducibility of results.\nWith an increasing number of laboratories contributing to the field, the available computational resources and previous bioinformatic experience varies greatly. To increase accessibility, newly developed tools should be adaptable to different environments, efficient, consistently maintained and well-documented."
  },
  {
    "objectID": "eager.html#what-can-nf-coreeager-do",
    "href": "eager.html#what-can-nf-coreeager-do",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.2 What can nf-core/eager do?",
    "text": "4.2 What can nf-core/eager do?\nnf-core/eager streamlines the initial steps of ancient DNA analysis from FASTQ files after sequencing to variant calling (Fellows Yates et al. 2021).\n\nPreprocessing:\n\nFastQC (sequencing quality control)\nAdapterRemoval2/fastp (sequencing artifact clean-up)\n\nMapping:\n\nBWA aln/BWA mem/CircularMapper/Bowtie2 (alignment)\nSAMtools (mapping quality filtering)\nPicard MarkDuplicates/DeDup (PCR duplicate removal)\nSAMtools/PreSeq/Qualimap2/BEDtools/Sex.DetERRmine/EndorSpy/MtNucRatio (mapping statistics)\n\naDNA evaluation:\n\nDamageProfiler/mapDamage2 (damage assessment)\nPMDtools (aDNA read selection)\nmapDamage2/Bamutils (damage removal)\nANGSD (human contamination estimation)\nBBduk/HOPS/Kraken & Kraken Parse/MALT & MaltExtract (metagenomic screening)\n\nVariant calling: GATK UnifiedGenotyper & HaplotypeCaller/sequenceTools pileupCaller/VCF2Genome/MultiVCFAnalyzer/freebayes/ANGSD\nReport generation: MultiQC (summarize all generated statistics)"
  },
  {
    "objectID": "eager.html#how-do-i-use-nf-coreeager",
    "href": "eager.html#how-do-i-use-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.3 How do I use nf-core/eager?",
    "text": "4.3 How do I use nf-core/eager?\n\n4.3.1 Installation\nYou need:\n\na Unix machine (HPC-cluster or a computer running Linux or MacOS)\nan installation of docker or apptainer (formerly known as singularity) or conda, java and nextflow (e.g. via conda)\ninternet connection\n\nDownload the latest version of nf-core/eager\nnextflow pull nf-core/eager \n# for a specific version\nnextflow pull nf-core/eager -r 2.5.0\nRun a test specifying your choice of conda, docker or singularity\nnextflow run nf-core/eager -r 2.5.0 -profile test_tsv,docker\nTo optimize the use of available clusters, queues and resources, check if a Nextflow pipeline configuration is already available for your institution or computing environment. If not, prepare a custom profile tailored to your computational resources and setup. These are then added as a profile.\nnextflow run nf-core/eager -r 2.5.0 -profile test_tsv,eva #for MPI-EVA\n\n\n4.3.2 Input preparation\nYou can run nf-core/eager by providing either a path to fastq or bam files or a path to a tab-separated table of input data to --input. For a large number of samples and convenience, a tsv is usually the preferred option. Using a tsv input also allows for merging of different files (e.g. different libraries, different UDG treatments, etc.) at different stages of the pipeline.\n\n\n\nPipeline stages and merging steps performed for a single sample with different libraries and different UDG treatments.\n\n\nA tsv input file contains the following columns, detailing the name of the sample, library, sequencing lane, colour chemistry depending on the sequencer used, target organism, library strandedness, UDG treatment, path to fastq with forward reads (SE and PE), path to reverse reads (only PE), path to bam (optional). nf-core/eager will treat the data according to the provided information, e.g. only trim UDG half data and genotype single-stranded libraries using single-stranded mode.\nSample_Name Library_ID Lane Colour_Chemistry SeqType Organism Strandedness UDG_Treatment R1                                                                                                                                  R2                                                                                                                                  BAM\nJK2782      JK2782     1    4                PE      Mammoth  single       half          https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2782_TGGCCGATCAACGA_L008_R1_001.fastq.gz.tengrand.fq.gz https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2782_TGGCCGATCAACGA_L008_R2_001.fastq.gz.tengrand.fq.gz NA\nJK2802      JK2802     2    2                SE      Mammoth  double       full          NA                                                                                                                                  NA                                                                                                                                  https://github.com/nf-core/test-datasets/raw/eager/testdata/Mammoth/fastq/JK2802_AGAATAACCTACCA_L008_R1_001.fastq.gz.tengrand.bam\nCollecting and double-checking this information is time consuming, but crucial!\nIf you realize you have different libraries from same individual, you should enter the same Sample_Name for all respective libraries. nf-core/eager will then produce all steps for the independent libraries (e.g. endogenous DNA, sequencing quality control, contamination estimation, etc.), but merge the deduplicated bam files before genotyping, genetic sex estimation and coverage calculation. To avoid re-mapping the whole dataset and conserve computing resources, also consider providing the mapped bam files to nf-core/eager directly.\n\n\n\n\n\n\nTip\n\n\n\nAt DAG, we can take advantage of all the information entered in Pandora to produce a eager-ready tsv with pandora2eager.\n\n\n\n\n4.3.3 Parameter customization\nBy default nf-core/eager runs the following, when you only provide input data and a reference genome:\nnextflow run nf-core/eager --input &lt;INPUT&gt;.tsv --fasta '&lt;REFERENCE&gt;.fasta' -profile eva\n\nPreprocessing:\n\nFastQC (sequencing quality control)\nAdapterRemoval2 (sequencing artifact clean-up)\n\nMapping:\n\nBWA aln (alignment)\nPicard MarkDuplicates (PCR duplicate removal)\nSAMtools/PreSeq/Qualimap2/EndorSpy (mapping statistics)\n\naDNA evaluation: DamageProfiler (damage assessment)\nReport generation: MultiQC (summarize all generated statistics)\n\nThe most direct way to add analysis steps (e.g. turn on genotyping) or change settings (e.g. shorter read length cut-off) is to add more parameters to the command line, in this case --run_genotyping --genotyping_tool pileupcaller and --clip_readlength 25, respectively. However, this gets cumbersome for the rather extensive workflows we usually employ for human aDNA analysis, including read trimming based on UDG treatment, genetic sex estimation, human nuclear contamination estimation, mitochondrial to nuclear ratio estimation and genotyping.\nBut the power of nf-core/eager lies in its adaptability to your specific analysis needs and the possibility to ‘remember’ your favorite settings with a personal configuration file. This separate file can contain all parameters for your required tools, as well as custom computational resource requests. For 1240K capture data, a profile mapping to the hs37d5 reference genome with genotyping could look like this:\nprofiles{\n  TF_hs37 { #name of the profile\n    params {\n        config_profile_description = \"human 1240K data hs37d5 + genotyping\"\n        config_profile_contact = \"Selina Carlhoff (@scarlhoff)\"\n        email = \"selina_carlhoff@eva.mpg.de\"\n        snpcapture_bed = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        fasta = \"/PATH/hs37d5/hs37d5.fa\"\n        fasta_index = \"/PATH/hs37d5/hs37d5.fa.fai\"\n        bwa_index = \"/PATH/hs37d5/\"\n        skip_preseq = true\n        clip_readlength = 30\n        preserve5p = true\n        bwaalnn = 0.01\n        bwaalnl = 16500\n        run_bam_filtering = true\n        bam_mapping_quality_threshold = 30\n        bam_filter_minreadlength = 30\n        bam_unmapped_type = \"discard\"\n        run_trim_bam = true\n        bamutils_clip_double_stranded_half_udg_left = 2\n        bamutils_clip_double_stranded_half_udg_right = 2\n        bamutils_clip_single_stranded_none_udg_left = 0\n        bamutils_clip_single_stranded_none_udg_right = 0\n        run_genotyping = true\n        genotyping_tool = \"pileupcaller\"\n        genotyping_source = \"trimmed\"\n        pileupcaller_bedfile = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        pileupcaller_snpfile = \"/PATH/1240K.snp\"\n        run_mtnucratio = true\n        mtnucratio_header = \"MT\"\n        run_sexdeterrmine = true\n        sexdeterrmine_bedfile = \"/PATH/1240K.pos.list_hs37d5.0based.bed\"\n        run_nuclear_contamination = true\n        contamination_chrom_name = \"X\"\n    }\n    process {\n    maxRetries = 2\n        withName:bwa {\n        time = { task.attempt == 3 ? 1440.h : task.attempt == 2 ? 72.h : 48.h }\n        }\n        withName:markduplicates {\n        memory = { task.attempt == 3 ? 16.GB : task.attempt == 2 ? 8.GB : 4.GB }\n        }\n        withName: mtnucratio {\n            memory = '10.G'\n            time = '24.h'\n        }\n    }\n  }\n}\nThe configuration file is then provided to nf-core/eager via the -profile and -c flag.\nnextflow run nf-core/eager -–input &lt;INPUT&gt;.tsv -profile TF_hs37,eva,archgen -c /&lt;PATH&gt;/eager2.config\nFull documentation of all parameters is available on the nf-core/eager website.\n\n\n\n\n\n\nTip\n\n\n\nThe standardised parameters for the DAG automated pipeline can be found at /mnt/archgen/Autorun_eager/conf/Autorun.config.\n\n\n\n\n4.3.4 Run submission\nOnce all input files and parameters are prepared, you are ready for submission. To make sure that the workflow continues running when you disconnect from the cluster or shut down your computer, nf-core/eager should be run in a screen session.\n# create a screen session\nscreen -R eager\n# submit nf-core/eager run\nnextflow run nf-core/eager –input &lt;INPUT&gt;.tsv -profile &lt;YOUR_PROFILE&gt; -c /&lt;PATH&gt;/&lt;YOUR_CONFIG&gt;.config\n# disconnect from screen session by pressing Ctrl+A+D\n# reconnect to screen session\nscreen -r eager\n# end screen session after successful pipeline execution\nexit\nAfter submitting a command specifying all the parameters you would like to use, Nextflow generates the corresponding shell scripts and submits each job to your scheduler according to your requested computational resources. You can track the execution status live in the terminal or on Nextflow Tower. For use with tower, you should assign the run an identifiable name with -name and activate tracking using -with-tower.\n\n\n4.3.5 Output\nDuring the progression of the run, the results of each pipeline steps are collected in separate output directories. These contain the raw outputs of each tool, including any generated files (e.g. deduplicated bam files or genotypes).\n&lt;RUNNAME&gt;/\n- results/\n  - adapterremoval/\n  - damageprofiler/\n  - deduplication/\n  - documentation/\n  - endorspy/\n  - fastqc/\n  - genotyping/\n  - lanemerging/\n  - mapping/\n  - merged_bams/\n  - multiqc/\n  - nuclear_contamination/\n  - pipeline_info/\n  - qualimap/\n  - reference_genome/\n  - samtools/\n  - sex_determination/\n  - trimmed_bam/\n- work/\nBut as a first overview, we want to look at the summary of all statistics aggregated in multiqc/multiqc_report.html.\n\n\n\nScreenshot of the top section of a MultiQC report\n\n\nThis table collects the output from all tools, so you can get an overview of sequenced reads per sequencing run, endogenous DNA per library, covered SNPs per sample and much more. You can also inspect and export crucial plots, such as read length distribution and damage profile. The end of the report also contains a list of all software versions and an overview of which profiles were used.\n\n\n\nAncient DNA damage plot as generated by MultiQC\n\n\n\n\n4.3.6 Trouble shooting\nA common issue with nf-core/eager, especially when used in combination with the SGE scheduler, are memory issues with java-driven tools, e.g. MarkDuplicates. Sometimes the pipeline does not catch cases properly, where the allocated memory is exceeded, and the job keeps running instead of being re-submitted with larger memory allocation. Therefore, if you notice jobs running much longer than expected, it is worth checking the work/ directory, where all information about each submitted job is recorded. Each job is assigned a randomly generated name using numbers and letter which you can identify from the log printed in to your screen, the &lt;RUNNAME&gt;/.nextflow.log or Nextflow tower. Each work directory contains files tracing the execution of the job.\n&lt;RUNNAME&gt;/\n- work/\n  - &lt;WORKDIRECTORY&gt;/\n    - .command.sh # exact command run for this tool\n    - .command.run # exact command submitted to the scheduler\n    - .command.log # any messages during the execution\n    - .command.err # any error messages\n    - .command.out # any output messages\n    - .command.trace # assigned computational resources\nIf you spot java.lang.OutOfMemoryError: unable to create new native thread in .command.log or command.err, you can delete the individual job from the scheduling queue. It will be re-submitted automatically with larger memory allocation.\nIf you’ve had an issue with a run or want to restart the pipeline, you can do so using -resume. Nextflow will used cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. You can also supply a run name to resume a specific run: -resume [run-name]. Use the nextflow log command to show previous run names."
  },
  {
    "objectID": "eager.html#how-do-i-report-the-usage-of-nf-coreeager",
    "href": "eager.html#how-do-i-report-the-usage-of-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.4 How do I report the usage of nf-core/eager?",
    "text": "4.4 How do I report the usage of nf-core/eager?\nIf you use nf-core/eager for your analysis, please cite Fellows Yates et al. (2021) and the release of nf-core/eager on zenodo, as well as the nf-core publication (Ewels et al. 2020). As nf-core/eager is only the pipeline connecting multiple tools, please also cite the version of each used tool, the respective individual publications and add the full command for easy reproducibility.\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\nEwels, Philip A., Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso, and Sven Nahnsen. 2020. “The Nf-Core Framework for Community-Curated Bioinformatics Pipelines.” Nature Biotechnology 38 (3): 276–78. https://doi.org/10.1038/s41587-020-0439-x.\nnextflow run nf-core/eager\n        -profile eva,archgen\n        -r 2.4.0\n        --input &lt;INPUT&gt;.tsv\n        --min_adap_overlap 1\n        --clip_readlength 30\n        --clip_min_read_quality 20\n        --preserve5p\n        --mapper bwaaln\n        --bwaalnnn 0.01\n        --bwaalno 2\n        --run_bam_filtering true\n        --bam_mapping_quality_threshold 30\n        --bam_filter_minreadlength 30\n        --bam_unmapped_type discard\n        --dedupper markduplicates\n        --damageprofiler_length 100\n        --damageprofiler_threshold 15\n        --damageprofiler_yaxis 0.3"
  },
  {
    "objectID": "eager.html#how-do-i-update-nf-coreeager",
    "href": "eager.html#how-do-i-update-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.5 How do I update nf-core/eager?",
    "text": "4.5 How do I update nf-core/eager?\nnextflow pull nf-core/eager\n#or for a specific version\nnextflow pull nf-core/eager -r 2.5.0"
  },
  {
    "objectID": "eager.html#whats-next-for-nf-coreeager",
    "href": "eager.html#whats-next-for-nf-coreeager",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.6 What’s next for nf-core/eager?",
    "text": "4.6 What’s next for nf-core/eager?\nWhile nf-core/eager 2.5.0 has only been released recently, behind the scenes the development team has been very busy re-writing nf-core/eager to be more efficient and include even more functionality. So look out for nf-core/eager 3.0 release sometime soon!\n\n\n\nOverview of the development status of nf-core/eager 3.0 as of October 2023, new functionality marked in purple."
  },
  {
    "objectID": "eager.html#how-can-i-get-help-with-problems-or-questions",
    "href": "eager.html#how-can-i-get-help-with-problems-or-questions",
    "title": "4  Introduction to nf-core/eager",
    "section": "4.7 How can I get help with problems or questions?",
    "text": "4.7 How can I get help with problems or questions?\nCheck the website for in-depth documentation of nf-core/eager.\n Raise your issue on GitHub.\nJoin the #eager channel on the nf-core Slack."
  },
  {
    "objectID": "mobest.html#overview",
    "href": "mobest.html#overview",
    "title": "5  mobest - Spatiotemporal Ancestry Interpolation and Search",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nmobest is an R package providing types and functions for spatiotemporal interpolation of human genetic ancestry components, probabilistic similarity search and the calculation of a derived measure of ancestry relocation and mobility. The workflow in mobest version 1.0.0 was specifically developed for Schmid and Schiffels (2023).\nmobest assumes you have a set of genetic samples with spatial (two coordinates in a projected reference system) and temporal positions (years BC/AD) for which you calculated a derived, numeric measure of genetic ancestry (e.g. coordinates in a PCA or MDS space).\n\n\n\nSpatiotemporal distribution of the archaeogenetic samples in Schmid and Schiffels (2023).\n\n\n\n\n\nDistribution of the archaeogenetic samples in a genetic space constructed via multidimensional scaling.\n\n\nThe package then provides functions to perform spatiotemporal interpolation using Gaussian process regression (GPR, “kriging”) with the laGP R package (Gramacy (2016)) to reconstruct an ancestry field based on the ancestry measure you provided.\n\nGramacy, Robert B. 2016. “laGP: Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R.” Journal of Statistical Software 72 (1): 1–46. https://doi.org/10.18637/jss.v072.i01.\n\n\n\n\n\n\nSparse point cloud in space and time.\n\n\n\n\n\n\n\nDense cloud after interpolation.\n\n\n\n\n\n\n\n\n\nOne timeslice of the interpolated field.\n\n\n\n\n\n\n\nInterpolated C1 mean and error for this timeslice.\n\n\n\n\n\nmobest finally allows to derive a similarity likelihood for samples of interest within the interpolated field, which – under certain circumstances – can be interpreted as an origin probability.\n\n\n\n\n\n\n“Cutting the field”, so measuring the likelihoods for a specific ancestry profile (C1 value) for every grid point in space.\n\n\n\n\n\n\n\nSimilarity probability for the Stuttgart sample calculated for the time slice at 5600 BC with two MDS-based ancestry components C1 and C2.\n\n\n\n\n\nThe Stuttgart sample used for illustrative purposes here was taken from an Early Neolithic individual from Southern Germany and first published in Lazaridis et al. (2014)."
  },
  {
    "objectID": "mobest.html#sec-install",
    "href": "mobest.html#sec-install",
    "title": "5  mobest - Spatiotemporal Ancestry Interpolation and Search",
    "section": "5.2 Installing mobest",
    "text": "5.2 Installing mobest\nmobest is an R package and can be installed directly from GitHub with the following code on the R console:\nif(!require('remotes')) install.packages('remotes')\nremotes::install_github('nevrome/mobest')\nYou can also install specific/older versions of mobest with the following syntax: nevrome/mobest[@ref|#pull|@*release]. For example to install the publication release version you can run remotes::install_github('nevrome/mobest@1.0.0').\nFor any of this to work a number of system libraries (mostly for processing geospatial data) have to be installed on your system, primarily for one particular dependency of mobest: the sf R package (Pebesma (2018)). The following table includes the libraries and the names of the relevant packages in the package management systems of various Linux distributions and MacOS.\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\n\nSystem library\ndeb package(Ubuntu/Debian)\nrpm package(Fedora/CentOS)\npkgbuild package(Arch)\nbrew package(MacOS)\n\n\n\n\nGDAL\nlibgdal-dev\ngdal\ngdal\ngdal\n\n\nGEOS\nlibgeos-devlibgeos++-dev\ngeos-devel\ngeos\ngeos\n\n\nPROJ\nlibproj-dev\nproj-develsqlite-devel\nproj\nproj\n\n\nUDUNITS-2\nlibudunits2-dev\nudunits\nudunits\nudunits\n\n\n\nThe sf package maintainers provide a good explanation how to install these: https://r-spatial.github.io/sf/#installing"
  },
  {
    "objectID": "mobest.html#sec-basic",
    "href": "mobest.html#sec-basic",
    "title": "5  mobest - Spatiotemporal Ancestry Interpolation and Search",
    "section": "5.3 A basic similarity search workflow",
    "text": "5.3 A basic similarity search workflow\nThis section explains the setup for a basic ancestry similarity search with mobest in R.\nFor didactic purposes we use a simplified version of the data and code generated for the publication that introduced mobest: Schmid and Schiffels (2023) . This is a fairly generic setup you can adjust to the needs of other and more specific projects.\nThe script explained in the following sections as well as the data required for it can be downloaded in its entirety here:\n\nsimilarity_search.R\nsamples_basic.csv\n\n\n5.3.1 Preparing the computational environment\nFor this script we use various packages beyond base R, among which the following ones are required:\n\nreadr for loading .csv input data\nmagrittr for the pipe operator %&gt;%\nsf for loading and manipulating spatial data\nrnaturalearth for downloading spatial reference data (Massicotte and South (2024))\nggplot2 (and cowplot) to visualize intermediate and final results (Wilke (2024))\ndplyr for data manipulation of data.frames\nmobest (obviously)\n\n\nMassicotte, Philippe, and Andy South. 2024. Rnaturalearth: World Map Data from Natural Earth. https://docs.ropensci.org/rnaturalearth/.\n\nWilke, Claus O. 2024. Cowplot: Streamlined Plot Theme and Plot Annotations for ’Ggplot2’. https://wilkelab.org/cowplot/.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\nreadr, magrittr, ggplot2 and dplyr are all in the tidyverse (Wickham et al. (2019)) and can be installed in one go with install.packages(\"tidyverse\") on the R console. For the installation of sf and mobest please see the instructions above.\nWe will generally call functions explicitly with their namespace using :: (so e.g. readr::read_csv()). The only exceptions are magrittr and ggplot2, because we will use their functions so often that it becomes tedious to type them out. Instead we load them at the beginning.\nlibrary(magrittr)\nlibrary(ggplot2)\n\n\n5.3.2 Preparing the input data\n\n5.3.2.1 Generating the the spatial prediction grid\nmobest’s similarity search is typically run for a regular grid of spatial positions in the area of interest. It provides a function, mobest::create_prediction_grid(), to create such a grid, given a specification of the desired area. This area is typically the land area in a certain part of planet Earth.\n\n5.3.2.1.1 Defining the research area\nIn a first step we therefore have to define the research area for our analysis as a polygon in space. One way of doing so is to provide a list of latitude and longitude coordinates (extracted e.g. from Google Maps). The following code defines a simple research area covering large parts of Western Eurasia.\nresearch_area_4326 &lt;- sf::st_polygon(\n  list(\n    cbind(\n      c(35.91,11.73,-11.74,-15.47,\n        37.06,49.26,49.56,35.91), # longitudes\n      c(25.61,28.94,31.77, 62.73,\n        65.67,44.56,28.55,25.61)  # latitudes\n    )\n  )\n) %&gt;% sf::st_sfc(crs = 4326)\nSpatial coordinates require a coordinate references system (CRS). For lat-lon coordinates we typically use WGS84 with the EPSG code 4326. st_polygon() creates a simple polygon as a clockwise arrangement of individual coordinates and st_sfc() properly defines this polygon as a geographic area on Earth.\nA simple way to interactively inspect this polygon on a world map in R is provided by the mapview package (Appelhans et al. (2023)): mapview::mapview(research_area_4326).\n\nAppelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan Woellauer. 2023. Mapview: Interactive Viewing of Spatial Data in r. https://github.com/r-spatial/mapview.\n\n\n\nThe defined research area plotted on top of a map.\n\n\nWith the research area properly defined we can move to the next challenge and extract the land area in the research area. For that we have to obtain a dataset with polygons that trace the world’s coastlines. The naturalearthdata project provides open worldwide geodata in different resolutions and in easy to use data formats.\nThe rnaturalearth package makes it easy to download this data right into sf objects in R.\nworldwide_land_outline_4326 &lt;- rnaturalearth::ne_download(\n  scale = 50, type = 'land', category = 'physical',\n  returnclass = \"sf\"\n)\nWe can then crop the land outline to the research area to obtain the land area we are interested in.\nresearch_land_outline_4326 &lt;- sf::st_intersection(\n  worldwide_land_outline_4326,\n  research_area_4326\n)\nUsing ggplot2, we can finally plot the resulting spatial multi-polygon.\nggplot() + geom_sf(data = research_land_outline_4326)\n\n\n\nThe land area within the research area.\n\n\n\n\n5.3.2.1.2 Projecting the spatial data\nAt this point we run into a specific issue of mobest: It requires its “independent” spatial and temporal coordinates to be coordinates in a Cartesian system describing Euclidean space.\nFor the spatial coordinates that means we can not work with latitude and longitude coordinates on a sphere, but have to transform them. We have to apply map projection to represent the curved, two dimensional surface of our planet on a simple plane.\n\n\n\n\n\n\nNote\n\n\n\nThe question how exactly this should be done and which CRS to choose depends on the position, size and shape of the research area. Each map projection algorithm has different properties regarding whether they manage to preserve or distort size, shape, distances and directions of areas and lines compared to the actual circumstances on Earth. Generally the larger the research area the bigger the distortion of these properties becomes.\nBut for mobest we ideally want to represent all them accurately. mobest is therefore unfit for origin search on a global scale, but can usually be well applied for individual countries with the projections recommended by their cartographic agencies. For an intermediate, continental scale, as in this example, we have to choose our CRS wisely.\n\n\nWe decided to follow the recommendation of Annoni et al. (2003) and chose ETRS89 Lambert Azimuthal Equal Area coordinate reference system as in EPSG code 3035.\n\nAnnoni, A. et al. 2003. “Map Projections for Europe.” Technical Report EUR 20120 EN. European Commission Joint Research Centre. http://mapref.org/LinkedDocuments/MapProjectionsForEurope-EUR-20120.pdf.\n\nTsoulos, Lysandros. 2003. “An Equal Area Projection for Statistical Mapping in the EU.” In Map Projections for Europe, edited by A. Annoni et al., 50–55. European Commission Joint Research Centre. http://mapref.org/LinkedDocuments/MapProjectionsForEurope-EUR-20120.pdf.\nOur decision comes at the price of increased inaccuracy especially in the North- and South-East of the research area where we get very far away from the specified centre for EPSG:3035 at 52° latitude and 10° longitude (see Tsoulos (2003) p.53 for a visualization of the deformative effects).\nTo transform the land outline in the research area from EPSG:4326 to EPSG:3035 we can apply sf::st_transform().\nresearch_land_outline_3035 &lt;- research_land_outline_4326 %&gt;%\n  sf::st_transform(crs = 3035)\nNote how the change in the coordinate system affects the map plot.\nggplot() + geom_sf(data = research_land_outline_3035)\n\n\n\nThe research area land polygon now transformed to EPSG:3035.\n\n\n\n\n5.3.2.1.3 Creating the prediction grid\nTo finally create the prediction grid we can use mobest::create_prediction_grid(). It takes the land outline polygon and overlays its bounding box with a regular grid (using sf::st_make_grid()), where each cell has the size corresponding to the spatial_cell_size parameter. It then determines the centres of each grid cell and crops the resulting, regular point cloud with the land area.\nNote that spatial_cell_size uses the unit of the CRS, so in our case for EPSG:3035 meters. That means a value of 50000 translates to one point every 50km.\nspatial_pred_grid &lt;- mobest::create_prediction_grid(\n  research_land_outline_3035,\n  spatial_cell_size = 50000\n)\nThe total number of resulting spatial prediction positions is 4738 in this example.\ncreate_prediction_grid returns an object of class mobest_spatialpositions, which is derived from tibble::tibble(). That means we can print it on the R console and it will behave as a tibble. It will also work seamlessly as an input for ggplot2, which we can now use to visualize the point cloud.\nggplot() +\n  geom_sf(data = research_land_outline_3035) +\n  geom_point(\n    data = spatial_pred_grid,\n    mapping = aes(x, y),\n    color = \"red\",\n    size = 0.25\n  )\n\n\n\nThe 4738 spatial prediction grid points plotted on top of the land area.\n\n\n\n\n\n5.3.2.2 Reading the input samples\nmobest requires a set of data points, archaeogenetic samples, to inform the ancestry field interpolation. For each sample the position in space, time and a dependent variable space (e.g. the coordinates in a PCA analysis) must be known. This information must be provided in a specific format. A typical mobest workflow involves preparing a sample list in a .xlsx or .csv table, which could then be read into R and transformed to the correct format.\nFor this tutorial we rely on the data used and published in Schmid and Schiffels (2023). You can download a simplified version of this dataset here (samples_basic.csv) and load it into R.\nsamples_basic &lt;- readr::read_csv(\"path/to/your/downloaded/samples_basic.csv\")\nsamples_basic includes the following columns/variables:\n\n\n\n\n\n\n\n\nColumn\nType\nDescription\n\n\n\n\nSample_ID\nchr\nA sample identifier\n\n\nLatitude\ndbl\nThe latitude coordinate where this sample was recovered\n\n\nLongitude\ndbl\nThe longitude coordinate\n\n\nDate_BC_AD_Median\nint\nThe median age of this sample in years BC/AD(negative numbers for BC, positive ones for AD)\n\n\nMDS_C1\ndbl\nThe coordinate of this sample on dimension 1 of an MDS analysis.See Schmid and Schiffels (2023) for more details on how this was obtained\n\n\nMDS_C2\ndbl\nThe coordinate of this sample on MDS dimension 2\n\n\n\nThese variables are a minimum for a meaningful mobest run and must be known for all samples. Samples with missing information in any of these columns have to excluded from the input.\nJust as for the research area we have to transform the coordinates from longitude and latitude coordinates to a projected system, specifically the same as the one we selected above. To do this we can construct an sf object from the sample table, apply sf::st_transform() and then transform this result back to a tibble with the x and y coordinates of EPSG:3035 in extra columns. This last step makes the code a bit awkward.\nsamples_projected &lt;- samples_basic %&gt;%\n  # make the tibble an sf object\n  sf::st_as_sf(\n    coords = c(\"Longitude\", \"Latitude\"),\n    crs = 4326\n  ) %&gt;%\n  # transform the coordinates\n  sf::st_transform(crs = 3035) %&gt;% \n  # reshape the sf object back into a simple tibble\n  dplyr::mutate(\n    x = sf::st_coordinates(.)[,1],\n    y = sf::st_coordinates(.)[,2]\n  ) %&gt;%\n  sf::st_drop_geometry()\nWith the coordinates in the same reference system as the landmass polygons we prepared above we can now combine both in a single figure:\nggplot() +\n  geom_sf(data = research_land_outline_3035) +\n  geom_point(\n    data = samples_projected,\n    mapping = aes(x, y),\n    color = \"darkgreen\",\n    size = 0.25\n  )\n\n\n\nThe spatial distribution of the informative samples.\n\n\nA number of samples are outside of the area we want to predict here. That is no problem. They will inform the field in the north-eastern fringes of the area and do no harm. It is much more problematic that some zones within our research area are severely under-sampled. We have to keep sampling gaps like this in mind when we interpret the results of the similarity search.\n\n\n\n5.3.3 Specifying the search sample\nmobest’s similarity search usually takes the perspective of an individual sample for which we want to determine similarity probabilities for a spatial prediction grid at a specific point in time. For this sample, the “search sample”, we require the same information as for the input samples: The position in space, time and the dependent variable space (e.g. PCA or MDS space).\nTechnically this is only a requirement of the mobest interface. Conceptually such a similarity search only really requires the dependent variable space position of interest. The added benefit of having all information there is the relative time search setting (see below) and a very comprehensive output table for the most common use-case.\nIn this example we will locate one specific sample with a pretty well studied ancestry history: The sample named Stuttgart published in Lazaridis et al. (2014). We can select it as a subset of our sample table:\n\nLazaridis, Iosif, Nick Patterson, Alissa Mittnik, Gabriel Renaud, Swapan Mallick, Karola Kirsanow, Peter H Sudmant, et al. 2014. “Ancient Human Genomes Suggest Three Ancestral Populations for Present-Day Europeans.” Nature 513 (7518): 409–13. https://doi.org/10.1038/nature13673.\nsearch_samples &lt;- samples_projected %&gt;%\n  dplyr::filter(\n    Sample_ID == \"Stuttgart_published.DG\"\n  )\nWith this setup the search sample itself will be part of the samples used to inform the ancestry field interpolation (samples_projected). This is no problem - the search sample is a known data point in space and time that can very well be employed to build a better model of the past ancestry distribution. There may be research questions for which this might not be desired, though. Then it can just as well be excluded from the samples_projected table.\n\n\n5.3.4 Running mobest’s interpolation and search function\nWith the input data, both the spatial prediction grid and the samples to inform the ancestry field interpolation, prepared and ready, we can now run mobest::locate(). For that we first have to split and transform the input into the required data structures.\n\n5.3.4.1 Building the input data for the interpolation\nHere is how the interface of mobest::locate() looks:\nmobest::locate(\n  # spatiotemporal coordinates of the reference samples\n  # informing the ancestry field\n  independent = ...,\n  # genetic coordinates of the reference samples\n  dependent   = ...,\n  # ---\n  # interpolation settings for each ancestry component\n  kernel = ...,\n  # ---\n  # spatiotemporal coordinates of the sample of interest\n  search_independent = ...,\n  # genetic coordinates of the sample of interest\n  search_dependent   = ...,\n  # ---\n  # spatial search grid: where to search\n  search_space_grid  = ...,\n  # search time: when to search\n  search_time      = ...,\n  search_time_mode = ...,\n  # ---\n  # should the result be normalized\n  normalize = ...\n)\nEach of these arguments requires specific input.\n\n5.3.4.1.1 Independent and dependent positions\nThe locest() arguments independent and dependent take the spatiotemporal and genetic (as for example derived from MDS/PCA) positions of the interpolation-informing samples. The terms independent and dependent allude to the notion and terminology of a statistical model, where positions in dependent, genetic space are predicted based on positions in independent, spatiotemporal space.\nSpatiotemporal positions are encoded in mobest with a custom data type: mobest_spatiotemporalpositions. For the independent argument of locest() we have to construct an object of this type with mobest::create_spatpos() to represent the positions of the input samples in samples_projected.\nind &lt;- mobest::create_spatpos(\n  id = samples_projected$Sample_ID,\n  x  = samples_projected$x,\n  y  = samples_projected$y,\n  z  = samples_projected$Date_BC_AD_Median\n)\nThe dependent, genetic variables are also encoded in a custom, tabular type: mobest_observations with the constructor function mobest::create_observations().\ndep &lt;- mobest::create_obs(\n  C1 = samples_projected$MDS_C1,\n  C2 = samples_projected$MDS_C2\n)\nNote that you can have an arbitrary number of these components with arbitrary names. The only condition is, that the very same set and names are used below for the search samples and for the kernel parameter settings of each dependent variable.\nThe lengths of the vectors (samples_projected$...) used for create_spatpos() and create_obs() all have to be identical. And their order has to be the same as well: Although the input is distributed over two constructors they describe the same samples.\nFor the search sample in search_samples, finally, we have to construct objects of the same type and structure:\nsearch_ind &lt;- mobest::create_spatpos(\n  id = search_samples$Sample_ID,\n  x  = search_samples$x,\n  y  = search_samples$y,\n  z  = search_samples$Date_BC_AD_Median\n)\nsearch_dep &lt;- mobest::create_obs(\n  C1 = search_samples$MDS_C1,\n  C2 = search_samples$MDS_C2\n)\n\n\n5.3.4.1.2 Kernel parameter settings\nThe locest() argument kernel takes an object of the class mobest_kernelsetting. This type encodes kernel configurations for each dependent variable, so the parameters for the Gaussian process regression (GPR) interpolation that should be used for this variable. These include mostly the lengthscale parameters in space (x and y) and time, as well as the nugget parameter (Gramacy (2020), specifically here). In very simple terms:\n\n———. 2020. Surrogates: Gaussian Process Modeling, Design and  Optimization for the Applied Sciences. Boca Raton, Florida: Chapman Hall/CRC.\n\nLengthscale parameters: How far in space and time should an individual sample’s genetic position inform the interpolated field.\nNugget: Error term to model local variability of the dependent variable, so for observations from the same position in space and time.\n\nHere is a possible kernel configuration for our example. We construct two kernel settings, one for each ancestry component, with mobest::create_kernel() in mobest::create_kernset().\nkernset &lt;- mobest::create_kernset(\n  C1 = mobest::create_kernel(\n    dsx = 800 * 1000, dsy = 800 * 1000, dt = 800,\n    g = 0.1\n  ),\n  C2 = mobest::create_kernel(\n    dsx = 800 * 1000, dsy = 800 * 1000, dt = 800,\n    g = 0.1\n  )\n)\nNote how we scale the lengthscale parameters: dsx and dsy are set in meters (800 * 1000m = 800km) and dt in years (800y). g is dimensionless. With the setting specified here both dependent variables will be interpolated with the same, very smooth (several hundred kilometres and years in diameter) kernel.\nThe main question naturally arising from this, is how to set these parameters for a given dataset and research question. There are various empirical ways to find optimal values through numerical optimization. See Supplementary Text 2 of Schmid and Schiffels (2023) and the mobest documentation for the approaches we applied.\n\n\n\n\n\n\nNote\n\n\n\nWhile estimating the nugget is generally advisable, we would argue, that the computationally expensive crossvalidation workflow to estimate the lengthscale parameters is not always necessary for basic applications of mobest.\nThe analysis in Schmid and Schiffels (2023) showed that Gaussian process regression returns reasonably accurate interpolation results for a large range of kernel parameter settings, as long as they reflect a plausible intuition about the mobility behaviour of human ancestry, which generally operates on a scale of hundreds of kilometres and years.\nmobest is primarily a visualization method and adjusting its parameters to ones liking is legitimate if the choices are communicated transparently.\n\n\n\nSchmid, Clemens, and Stephan Schiffels. 2023. “Estimating Human Mobility in Holocene Western Eurasia with Large-Scale Ancient Genomic Data.” Proceedings of the National Academy of Sciences 120 (9). https://doi.org/10.1073/pnas.2218375120.\n\n\n5.3.4.1.3 Search positions\nWith input data and settings out of the way we can now specify the points in space and time where we actually want to perform the search. For these positions the GPR model is queried to return a mean and error, which are in turn used to calculate the probability density of a specific dependent variable space position, e.g. a specific coordinate on the first coordinate of an MDS analysis.\nWe already performed all necessary work for the search_space_grid argument, so the spatial positions of the prediction grid. We can just enter spatial_pred_grid here.\nThe search time can be specified as an integer vector of years: e.g. search_time = c(-500, -200, 100). This vector gets interpreted by mobest::locate() in two different ways, which can be selected with the switch argument search_time_mode. search_time_mode can either be \"relative\" (which is the default!) or absolute.\n\n\"relative\": The search_time is interpreted as a \\(\\Delta t\\) relative to the age of the search sample(s). Negative values point to ages that are older then the sample age, so in their relative past, and positive ones to younger ages in their relative future. In this example -500 would be interpreted as 500 years prior to the year the Stuttgart sample presumably died (so -5242-500 = -5742 BC/AD), and 100 as an age 100 years after their death (so -5242+100 = -5142 BC/AD).\n\"absolute\": The values in search_time are simply interpreted as absolute ages in years BC/AD.\n\nFor this example we will set the search time to an \"absolute\" value.\nsearch_time = -6800\nsearch_time_mode = \"absolute\"\nThis will search at exactly one point in time; a single timeslice 6800 BC.\n\n\n5.3.4.1.4 Normalization\nThe last relevant option of locate(), normalize, concerns the normalization of the output. mobest’s search calculates likelihoods for each search point. This is a dimensionless measure that is hard to compare across multiple runs with different parameter settings. If normalize is set to TRUE, then the densities for sets of spatial points that share all other parameters (including the timeslice) are rescaled to a sum of one, so to proper probabilities.\nWe assume users generally want to use mobest, specifically locate(), to calculate similarity probability density maps for individual samples, time slices and parameter settings. The most natural normalization for this case is to unify the scaling of these maps. This renders them comparable. normalize should therefore be set to TRUE for basic applications. This is also encoded as the the default setting.\n\n\n\n5.3.4.2 Calling mobest::locate()\nIn the previous sections we have thoroughly prepared the input for a first, simple run of mobest::locate(). We can now finally call the function.\nsearch_result &lt;- mobest::locate(\n  independent        = ind,\n  dependent          = dep,\n  kernel             = kernset,\n  search_independent = search_ind,\n  search_dependent   = search_dep,\n  search_space_grid  = spatial_pred_grid,\n  search_time        = -6800,\n  search_time_mode   = \"absolute\"\n)\nThis typically runs for a couple of seconds, uses every available processor core and returns an object search_result, which we will inspect below.\n\n\n\n5.3.5 Inspecting the computed results\nmobest::locate() returns an object of class mobest_locateoverview. It includes the relevant information for visualization and further processing of the analysis results.\n\n5.3.5.1 The mobest_locateoverview table\nmobest_locateoverview is derived from tibble and has a large set of columns, many not immediately relevant to the basic example here. This applies especially for the variables documenting the excessive permutation mechanics hidden behind the relatively simple interface of mobest::locate(). locate() is, in fact, a wrapper function for the more flexible function mobest::locate_multi(), which can handle permutations in various additional input parameters.\nEach row of the mobest_locateoverview table stores the calculated interpolated mean, error and similarity probability (field_mean, field_sd, probability) for one permutation of the input point positions in independent and dependent variable space (independent_table_id and dependent_setting_id), one dependent variable dependent_var_id, one iteration of the kernel settings (kernel_setting_id: dsx, dsy, dt, g), one prediction grid point emerging as a combination of spatial grid and search timeslice (pred_grid_id: field_id, field_geo_id, field_x, field_y, field_z, search_time) and finally one search sample (search_id, search_x, search_y, search_z, search_measured).\nHere is a list of the variables returned in mobest_locateoverview for each of these result iterations.\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nindependent_table_id\nIdentifier of the spatiotemporal position permutation\n\n\ndependent_setting_id\nIdentifier of the dependent variable space position permutation\n\n\ndependent_var_id\nIdentifier of the dependent variable\n\n\nkernel_setting_id\nIdentifier of the kernel setting permutation\n\n\npred_grid_id\nIdentifier of the spatiotemporal prediction grid\n\n\ndsx\nKernel lengthscale parameter on the spatial x axis\n\n\ndsy\nKernel lengthscale parameter on the spatial y axis\n\n\ndt\nKernel lengthscale parameter on the temporal axis\n\n\ng\nKernel nugget parameter\n\n\nfield_id\nIdentifier of the spatiotemporal prediction point\n\n\nfield_x\nSpatial x axis coordinate of the prediction point\n\n\nfield_y\nSpatial y axis coordinate of the prediction point\n\n\nfield_z\nTemporal coordinate (age) of the prediction point\n\n\nfield_geo_id\nIdentifier of the spatial prediction point\n\n\nfield_mean\nMean value predicted by the GPR model for the dependent variable\n\n\nfield_sd\nError term predicted by the GPR model for the dependent variable\n\n\nsearch_id\nIdentifier of the search sample\n\n\nsearch_x\nSpatial x axis coordinate of the search sample\n\n\nsearch_y\nSpatial y axis coordinate of the search sample\n\n\nsearch_z\nTemporal coordinate (age) of the search sample\n\n\nsearch_time\nSearch time as provided by the user in locate()’s search_time argument\n\n\nsearch_measured\nGenetic coordinate of the search sample in the dependent variable space\n\n\nprobability\nProbability density for search_measured given all other parameters\n\n\n\nAs a result of the permutation of parameters, the size of the prediction grid and the number of search points, the number of rows in a mobest_locateoverview table can be calculated as a product of the individual counts of all relevant entities. One way to quickly validate the output of locate() and locate_multi() is to calculate the number of expected results based on the input and compare it with the actual number of rows in the output. For our example this calculation is fairly simple:\nWe have:\n\n\\(1\\) set of input point positions in independent variable space (independent_table_id)\n\\(1\\) set of input point positions in dependent variable space (dependent_setting_id)\n\\(2\\) dependent variables (dependent_var_id)\n\\(1\\) set of kernel parameter settings (kernel_setting_id)\n\\(4738\\) spatial prediction grid positions\n\\(1\\) time slice of interest\n\\(1\\) search sample\n\nThis means we expect exactly \\(2 * 4738 = 9476\\) rows in search_result, which we can confirm with nrow(search_result).\n\n\n5.3.5.2 Creating similarity probability maps for individual dependent variables\nThe most basic similarity probability map we can create with search_result is a map for just one parameter permutation, including only one dependent variable. In this case the relevant similarity probability observations are easy to obtain. We can just filter by dependent_var_id to only include either C1 or C2.\nresult_C1 &lt;- search_result %&gt;%\n  dplyr::filter(dependent_var_id == \"C1\")\nAnd this is then easy to plot with geom_raster(). We can then plot C1 and C2 together using cowplot::plot_grid().\np_C1 &lt;- ggplot() +\n  geom_raster(\n    data = result_C1,\n    mapping = aes(x = field_x, y = field_y, fill = probability)\n  ) +\n  coord_fixed()\n\n# for C2\nresult_C2 &lt;- search_result %&gt;%\n  dplyr::filter(dependent_var_id == \"C2\")\np_C2 &lt;- ggplot() +\n  geom_raster(\n    data = result_C2,\n    mapping = aes(x = field_x, y = field_y, fill = probability)\n  ) +\n  coord_fixed()\n\n# arrange both plots together\ncowplot::plot_grid(p_C1, p_C2, labels = c(\"C1\", \"C2\"))\n\n\n\nThe similarity probability search results for the sample Stuttgart for 6800 BC.\n\n\n\n\n5.3.5.3 Combining the information from multiple dependent variables\nThe results for individual dependent variables, so ancestry components like MDS or PCA dimensions, can be informative, but are usually under-powered to exclude highly improbable search results. Generally combining multiple ancestry components improves the accuracy of the results for individual samples, and we think this is best done by multiplying the results for the different dependent variables. This way spatial areas with high similarity probability for all dependent variables are naturally up-weighted, whereas areas that are unlikely similar for some dependent variables are down-weighted.\nTo perform the multiplication (and the re-normalization afterwards), mobest includes a function mobest::multiply_dependent_probabilities(). It works on objects of type mobest_locateoverview and yields tabular objects of type mobest_locateproduct. multiply_dependent_probabilities() is aware of the parameter permutations potentially encoded in the mobest_locateoverview table. It only combines the probabilities for dependent variables that share all other parameters. The number of rows in mobest_locateproduct will therefore be \\(\\frac{\\text{Number of rows in mobest\\_locateoverview}}{\\text{Number of dependent variables}}\\).\nIf we call it for search_result the output will thus have \\(9476/2=4738\\) rows.\nsearch_product &lt;-\n  mobest::multiply_dependent_probabilities(search_result)\nmobest_locateproduct tables feature a perfect subset of the columns in mobest_locateoverview. We can plot the combined similarity probability map with the code already applied for the individual dependent variables.\nggplot() +\n  geom_raster(\n    data = search_product,\n    mapping = aes(x = field_x, y = field_y, fill = probability)\n  ) +\n  coord_fixed()\n\n\n\nThe combined (\\(\\text{C1}*\\text{C2}\\)) similarity probability search results for the sample Stuttgart for 6800 BC.\n\n\nThis concludes a very basic similarity search workflow. Please see the documentation at https://nevrome.de/mobest for various other tutorials describing more advanced applications of mobest, starting with better map plotting."
  },
  {
    "objectID": "fst.html#theory-primer",
    "href": "fst.html#theory-primer",
    "title": "6  Measuring population structure using Fst",
    "section": "6.1 Theory primer",
    "text": "6.1 Theory primer\n\n6.1.1 Genetic drift\nGenetic drift is the process by which allele frequencies change randomly due to random fluctuations. Various models exist to model such fluctuations, but the most widely used one is the Wright-Fisher model. In that model, a parent generation of N individuals produces exactly N individuals as offspring, which make up the next generation. To model the fluctuations, every “child” gets assigned a random “parent” from the previous generation.\nExample: With \\(N=100\\) (haploid individuals), we might consider a genetic locus with two alleles \\(A\\) and \\(B\\), and in the parent generation, say, 50 individuals carried allele B, and 50 carried allele B. Then, the number of individuals carrying B in the next generation is the number of children who get assigned a parent with allele B. These will be close to 50, but not exactly, due to noise.\nAs a statistical process, this amounts to a binomial process, where the N offspring individuals are drawn, each carrying a 50% probability to have A vs. B. In the third generation, this probability may then have already shifted away from 50.\nHere is a simple function in R to model the allele frequency in a number of \\(g\\) successive generations, given a (haploid) population size \\(n\\) and a starting frequency \\(x0\\):\n\nwfsim &lt;- function(n, g, x0) {\n  res &lt;- numeric(g + 1)\n  res[1] &lt;- x0\n  for (i in 2:(g + 1)) {\n    res[i] &lt;- (rbinom(1, n, res[i - 1])) / n\n  }\n  return(res)\n}\n\nWe can test it:\n\nset.seed(1)\nwfsim(100, 10, 0.5)\n\n [1] 0.50 0.52 0.56 0.46 0.45 0.47 0.48 0.61 0.60 0.58 0.61\n\n\nSo indeed the allele frequency changes randomly. We can visualise it for more generations:\n\ntime_series &lt;- wfsim(100, 100, 0.5)\nplot(time_series, type = \"l\", ylim = c(0, 1),\n     xlab = \"generation\", ylab = \"allele frequency\")\n\n\n\n\nWe can better understand this random process, by simulating it many times and plotting the results together:\n\ngens &lt;- 100\nsims100 &lt;- replicate(50, wfsim(100, gens, 0.5))\nmatplot(sims100, type = \"l\", lty = 1, col = \"black\",\n        ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\")\n\n\n\n\nThis shows how the variance increases with time, and eventually more and more of these curves get absorbed at either \\(x=0\\) or \\(x=1\\), a process called “fixation”.\nHow does this process depend on the population size? We can take a look. Here are three families of simulations, with three different population sizes:\n\ngens &lt;- 1000\nsims100 &lt;- replicate(50, wfsim(100, gens, 0.5))\npar(mfrow = c(1, 3))\nmatplot(sims100, type = \"l\", lty = 1, col = \"black\",\n        xlim = c(0, 100), ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\", main = \"N = 100\")\n\nsims1000 &lt;- replicate(50, wfsim(1000, gens, 0.5))\nmatplot(sims1000, type = \"l\", lty = 1, col = \"black\",\n        xlim = c(0, 100), ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\", main = \"N = 1000\")\n\nsims10000 &lt;- replicate(50, wfsim(10000, gens, 0.5))\nmatplot(sims10000, type = \"l\", lty = 1, col = \"black\",\n        xlim = c(0, 100), ylim = c(0, 1),\n        xlab = \"generation\", ylab = \"allele frequency\", main = \"N = 10000\")\n\n\n\n\nThis shows that larger populations have weaker fluctuations than small populations.\n\n\n6.1.2 \\(F_\\text{ST}\\) quantifies genetic drift\nTo quantify genetic drift, we can measure the variance of this process over time. The following plot uses the same data as shown above and estimates the variance:\n\npar(mfrow = c(1, 3))\nplot(apply(sims100,   1, var), type = \"l\", ylim = c(0, 0.25),\n     xlab = \"generations\", ylab = \"Variance\", main = \"N = 100\")\nplot(apply(sims1000,  1, var), type = \"l\", ylim = c(0, 0.25),\n     xlab = \"generations\", ylab = \"Variance\", main = \"N = 1000\")\nplot(apply(sims10000, 1, var), type = \"l\", ylim = c(0, 0.25),\n     xlab = \"generations\", ylab = \"Variance\", main = \"N = 10000\")\n\n\n\n\nso an increasing variance. But it doesn’t go up forever, but reaches a plateau. This is because of fixation: Once all curves reach fixaton at either \\(x=0\\) or \\(x=1\\), variance does no longer increase. In fact, the maximum variance corresponds to the state where all curves have been fixed. The variance to that state corresponds to the variance of a Bernoulli-process, which is \\(x_0(1-x_0)\\), so it depends on the starting frequency.\nIt is this plateau of the variance that defines \\(F_\\text{ST}=1\\)! Here is an illustration using again three families of simulations, but this time with the same population size but different starting frequencies:\n\nsims_x05 &lt;- replicate(1000, wfsim(100, gens, 0.5))\nsims_x03 &lt;- replicate(1000, wfsim(100, gens, 0.3))\nsims_x02 &lt;- replicate(1000, wfsim(100, gens, 0.2))\n\nplot_dat &lt;- cbind(\n  apply(sims_x05, 1, var),\n  apply(sims_x03, 1, var),\n  apply(sims_x02, 1, var)\n)\n\npar(mfrow = c(1, 1))\n\ncols &lt;- c(\"blue\", \"red\", \"green\")\nmatplot(plot_dat, type = \"l\", ylim = c(0, 0.25), lty = 1,\n             xlab = \"generations\", ylab = \"Variance\", col = cols)\nlegend(x = \"bottomright\",\n       legend = c(\"x = 0.5\", \"x = 0.3\", \"x = 0.2\", \"FST = 1\"),\n       lty = c(1, 1, 1, 2), col = c(cols, \"black\"))\n\ntheory_values &lt;- sapply(c(0.5, 0.3, 0.2), function(x) x * (1 - x))\nabline(h = theory_values, lty = 2, col = cols)\n\n\n\n\n\n\n6.1.3 Formal definition of \\(F_\\text{ST}\\)\n(Weir and Hill 2002) (explained and summarised in (Bhatia et al. 2013)) give a more formal evolutionary definition of \\(F_\\text{ST}\\), in terms of covariance between derived and ancestral populations. Specifically, for a given SNP, the definition involves the conditional probability of allele frequency \\(p_i\\) in population \\(i\\), given an ancestral allele frequency \\(p_\\text{anc}\\), which is defined as a random process with the expectation\n\nWeir, B S, and W G Hill. 2002. “Estimating f-Statistics.” Annual Review of Genetics 36: 721–50. https://doi.org/10.1146/annurev.genet.36.050802.093940.\n\\[E(p_i|p_\\text{anc}) = p_\\text{anc}\\]\nand variance \\[Var(p_i|p_\\text{anc}) = F_\\text{ST}^i p_\\text{anc}(1-p_\\text{anc}).\\]\nThis form of the conditional variance can be understood by analysing the equation for the two boundary cases: For \\(F_\\text{ST}^i=0\\), there is no variance, so the conditional probability of the derived frequency will be completely determined by the ancestral frequency with no random change. In contrast \\(F_\\text{ST}^i=1\\) means that the variance in the derived allele frequency is that of a binomial distribution with variance \\(p_\\text{anc}(1-p_\\text{anc})\\), indicating random but complete fixation of the frequency to 0 or 1.\n\\(F_\\text{ST}\\) between two populations A and B is then defined as \\[F_\\text{ST}(A,B) = \\frac{F_\\text{ST}^A+F_\\text{ST}^B}{2}\\].\n\n\n6.1.4 Estimating \\(F_\\text{ST}\\) from genomic data\nWhile there are various mathematical definitions for both the theoretical definition and estimation for \\(F_\\text{ST}\\), which differ in subtle ways, we here follow the excellent paper by (Bhatia et al. 2013), which proposes the following estimator, termed Hudson-estimator, which in turn is based on a proposal by (Hudson, Slatkin, and Maddison 1992) and has been implemented in the ADMIXTOOLS package (Patterson et al. 2012):\n\nHudson, R R, M Slatkin, and W P Maddison. 1992. “Estimation of Levels of Gene Flow from DNA Sequence Data.” Genetics 132 (2): 583–89. https://doi.org/10.1093/genetics/132.2.583.\n\\[F_\\text{ST}=1-\\frac{H_w}{H_b}\\]\nHere, \\(H_w\\) is the average heterozygosity within each population, and \\(H_b\\) is the average heterozygosity between two populations. We can easily read off the two boundaries of the definition: At the lower end, we have \\(F_\\text{ST}=0\\) if and only if \\(H_w=H_b\\), so there is no difference between heterozygosity measured within or between groups, which is equivalent to saying that the two populations are the same. On the upper end we have \\(F_\\text{ST}=1\\) if and only if \\(H_w=0\\), so all observed variants are fully fixed in both populations (but not necessarily different between the populations).\nIt is fairly straight forward to see (and shown in (Bhatia et al. 2013)) that the Hudson-estimator above can be recast as\n\nBhatia, G, N Patterson, S Sankararaman, and A L Price. 2013. “Estimating and Interpreting FST: The Impact of Rare Variants.” Genome Research 23 (9): 1514–21. http://genome.cshlp.org/cgi/doi/10.1101/gr.154831.113.\n\\[F_\\text{ST}(A,B)=\\frac{(a-b)^2}{a(1-b)+b(1-a)}\\]\nHere, \\(a\\) and \\(b\\) denote population allele frequencies, which are in principle unobserved, but can be approximated by sample allele frequencies. This approximation is biased, and (Patterson et al. 2012) gives additional formulae for an (asympotically) unbiased estimator (which is for example also used in Poseidon’s tool xerxes, as detailed in the whitepaper).\nFrom this definition, you can see that \\(F_\\text{ST}(A,B)\\) is closely related to F2-statistics, introduced in (Patterson et al. 2012):\n\\[F_2(A,B)=(a-b)^2\\].\nIn some sense, \\(F_\\text{ST}(A,B)\\) can be considered a normalised version of \\(F_2(A,B)\\). While both statistics range mathematically from 0 to 1, the upper bound 1 has very different meanings in both. A theoretical value of \\(F_2=1\\) would mean that both populations are fixed at different alleles in all studied SNPs, which is practically not possible (even completely random fixations would suggest that 1/4 of them would agree given that there are only four nucleotides, let alone the fact that such deeply diverged populations/species would not be alignable anymore). One can say that the time-scale on which \\(F_2\\) approaches 1, for non-ascertained SNPs, so the entire genome, is the time scale of nucleotide substitutions (i.e. mutations plus fixation) along species branches, which in neutral evolution is given by the inverse mutation rate \\(1/\\mu\\). This would mean something on the order of \\(10^8\\) generations, which is arguably of the same order of magnitude as the depth of the entire tree of life. In contrast, \\(F_\\text{ST}\\) approaches 1 on the time-scale of fixation of standing variation, which is \\(2N\\) generations, which for humans is on the order of 10000 generations, so around the depth of modern-human diversity from its origins in Africa several hundred thousand years ago. Arguably, this time scale is much more useful for data analyses and thus easier to interpret.\nOf course, in practice, one uses some ascertained SNP set, as also here in our examples below, in which case values are much higher because we consider only variants that are segregating in human populations within a relatively high allele frequency.\nIf you’ve gone through our chapter on F3 and F4 statistics, you will have already encountered our software xerxes. You can compute both the biased and the approximately unbiased estimators for \\(F_\\text{ST}(A,B)\\), using the FST or FSTvanilla statistics, as defined in the whitepaper.\nFor what follows, we will use the approximately unbiased form FST.\n\\(F_\\text{ST}(A,B)\\) has a convenient and untuitive scale: It ranges from 0 to 1, where \\(F_\\text{ST}(A,B)=0\\) denotes that \\(A\\) and \\(B\\) are the same population, with no differentiation whatsoever. On the other hand of the spectrum we have \\(F_\\text{ST}(A,B)=1\\), which would mean that two populations are fully separated.\nAnother way to see this measure is to consider it as relative shared variance: If you consider genetic variation between \\(A\\) and \\(B\\), and within each of \\(A\\) and \\(B\\), then \\(F_\\text{ST}(A,B)\\) can be considered to measure the average variance between populations relative to the average variance within populations, again with intuitive boundaries 0 and 1."
  },
  {
    "objectID": "fst.html#computing-fst-using-xerxes",
    "href": "fst.html#computing-fst-using-xerxes",
    "title": "6  Measuring population structure using Fst",
    "section": "6.2 Computing FST using xerxes",
    "text": "6.2 Computing FST using xerxes\nFor human present-day populations, we can compute pairwise FSt using xerxes.\nWe here chose a number of populations from (Patterson et al. 2012) with more than 10 samples per population, and prepare the following config file for xerxes:\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\nfstats:\n- type: FST\n  a: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n  b: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n- type: F2\n  a: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\n  b: [\"Adygei\", \"Balochi\", \"Basque\", \"BedouinA\", \"BedouinB\", \"Biaka\", \"Brahui\", \"Burusho\", \"Druze\", \"French\", \"Han\", \"Hazara\", \"Italian_North\", \"Japanese\", \"Kalash\", \"Karitiana\", \"Makrani\", \"Mandenka\", \"Mayan\", \"Mozabite\", \"Orcadian\", \"Palestinian\", \"Papuan\", \"Pathan\", \"Pima\", \"Russian\", \"Sardinian\", \"Sindhi_Pakistan\", \"Yakut\", \"Yoruba\"]\nThis will then produce all combinations of \\(FST(A, B)\\) and \\(F_2(A, B)\\) as indicated in the population lists.\n\n\n\n\n\n\nNote\n\n\n\nNote that the config-file engine in xerxes always computes all the combinations of populations, even for cases of \\(A=B\\). It also doesn’t know about symmetry, so will happily compute the redundant statistics \\(FST(\\text{Adygei}, \\text{Adygei})\\) and \\(FST(\\text{Adygei}, \\text{Adygei})\\). While this could be possibly improved, there is no big harm done, as this runs fairly quickly.\n\n\nWe run this config file using the command line\nREPO=/path/to/community-archive/2012_PattersonGenetics\n\nxerxes fstats -d $REPO --statConfig fstat_world_config.yaml -f fstat_world_output.tsv &gt; fstat_world_table.txt\nThe standard output, is a nicely layouted ASCII Table, which looks like this in the beginning:\n.-----------.-----------------.-----------------.---.---.---------.----------------.--------------------.------------------.--------------------.\n| Statistic |        a        |        b        | c | d | NrSites | Estimate_Total | Estimate_Jackknife | StdErr_Jackknife | Z_score_Jackknife  |\n:===========:=================:=================:===:===:=========:================:====================:==================:====================:\n| FST       | Adygei          | Adygei          |   |   | 593124  | 0.0000         | 0.0000             | 0.0000           | NaN                |\n| FST       | Adygei          | Balochi         |   |   | 593124  | 1.2789e-2      | 1.2789e-2          | 3.3572e-4        | 38.09517110646904  |\n| FST       | Adygei          | Basque          |   |   | 593124  | 1.8790e-2      | 1.8790e-2          | 4.0141e-4        | 46.810358341103225 |\n| FST       | Adygei          | BedouinA        |   |   | 593124  | 1.3017e-2      | 1.3017e-2          | 2.9647e-4        | 43.90737238689979  |\n| FST       | Adygei          | BedouinB        |   |   | 593124  | 3.3455e-2      | 3.3454e-2          | 5.7648e-4        | 58.03217592610529  |\n| FST       | Adygei          | Biaka           |   |   | 593124  | 0.1716         | 0.1716             | 1.2185e-3        | 140.85275693678508 |\n| FST       | Adygei          | Brahui          |   |   | 593124  | 1.4644e-2      | 1.4644e-2          | 3.4481e-4        | 42.46989237781921  |\n| FST       | Adygei          | Burusho         |   |   | 593124  | 1.8566e-2      | 1.8566e-2          | 3.8156e-4        | 48.6573908240317   |\n| FST       | Adygei          | Druze           |   |   | 593124  | 1.2173e-2      | 1.2173e-2          | 2.6659e-4        | 45.65975464203526  |\n| FST       | Adygei          | French          |   |   | 593124  | 9.7730e-3      | 9.7730e-3          | 3.1627e-4        | 30.9006924987833   |\n| FST       | Adygei          | Han             |   |   | 593124  | 9.8759e-2      | 9.8759e-2          | 1.1973e-3        | 82.48660429503893  |\n| FST       | Adygei          | Hazara          |   |   | 593124  | 3.0725e-2      | 3.0726e-2          | 7.1478e-4        | 42.98629834431124  |\n| FST       | Adygei          | Italian_North   |   |   | 593124  | 8.6600e-3      | 8.6601e-3          | 2.7883e-4        | 31.058813893781032 |\n\nbut of course has many more lines (&gt;1800 in this case). We also used the -f flag to output a tab-separated file, here named fstat_world_output.tsv, which is easier to read into R."
  },
  {
    "objectID": "fst.html#plotting-results-in-r",
    "href": "fst.html#plotting-results-in-r",
    "title": "6  Measuring population structure using Fst",
    "section": "6.3 Plotting results in R",
    "text": "6.3 Plotting results in R\nAll of the following code uses strictly only base-R for maximum compatibility. The code should run on any R installation.\nWe first load the data\n\ndat &lt;- dat &lt;- subset(read.table(\"fst_working/fstat_world_output.tsv\", sep=\"\\t\", header = TRUE),\n                     select=-c(c, d, Z_score_Jackknife))\ndatFST &lt;- dat[dat$Statistic == \"FST\",]\ndatF2 &lt;- dat[dat$Statistic == \"F2\",]\nhead(datFST)\n\n  Statistic      a        b NrSites Estimate_Total Estimate_Jackknife\n1       FST Adygei   Adygei  593124       0.000000           0.000000\n2       FST Adygei  Balochi  593124       0.012789           0.012789\n3       FST Adygei   Basque  593124       0.018790           0.018790\n4       FST Adygei BedouinA  593124       0.013017           0.013017\n5       FST Adygei BedouinB  593124       0.033455           0.033454\n6       FST Adygei    Biaka  593124       0.171600           0.171600\n  StdErr_Jackknife\n1       0.00000000\n2       0.00033572\n3       0.00040141\n4       0.00029647\n5       0.00057648\n6       0.00121850\n\n\nOk, this looks good. Let’s check out the largest values\n\nhead(dat[order(-dat$Estimate_Total),])\n\n    Statistic         a         b NrSites Estimate_Total Estimate_Jackknife\n166       FST     Biaka Karitiana  593124         0.3021             0.3021\n456       FST Karitiana     Biaka  593124         0.3021             0.3021\n473       FST Karitiana    Papuan  593124         0.3011             0.3011\n676       FST    Papuan Karitiana  593124         0.3011             0.3011\n468       FST Karitiana  Mandenka  593124         0.2798             0.2798\n526       FST  Mandenka Karitiana  593124         0.2798             0.2798\n    StdErr_Jackknife\n166        0.0016933\n456        0.0016933\n473        0.0026493\n676        0.0026493\n468        0.0017116\n526        0.0017116\n\n\nwhich shows that the largest FST values of around 0.3 are observed between Karitiana, from South America, and Biaka from Papua Neu Guinea (but note that these values are dependent on the ascertainment of SNPs, which here causes inflation)\nHere is a histogram of the values\n\nhist(datFST$Estimate_Total, xlab = \"FST\", ylab = \"Nr of pairs\",\n     main = \"\")\n\n\n\n\nSo most values are in the range of a few percent and 20 percent, with a mean of\n\nmean(datFST$Estimate_Total)\n\n[1] 0.09015616\n\n\nWe can compare that to F2:\n\nhist(datF2$Estimate_Total, xlab = \"F2\", ylab = \"Nr of pairs\",\n     main = \"\")\n\n\n\n\nwhich is an order of magnitude smaller.\nSo one of the key things to visualise is the pairwise matrix of FST, which we can quickly compute using the xtabs function from the stats package (part of base R):\n\nfstMat &lt;- xtabs(Estimate_Total ~ a + b, datFST)\nf2Mat &lt;- xtabs(Estimate_Total ~ a + b, datF2)\n\nand plot a simple heatmap using the powerful heatmap function from the stats package:\n\nheatmap(fstMat, symm = TRUE, hclustfun = function(m) hclust(m, method=\"ward.D2\"))\n\n\n\n\nwhich we can compare to the output using F2, which looks almost the same:\n\nheatmap(f2Mat, symm = TRUE, hclustfun = function(m) hclust(m, method=\"ward.D2\"))\n\n\n\n\nOK, let’s look at the dendrogram a bit closer:\n\nfstDist &lt;- as.dist(fstMat)\ndendro &lt;- hclust(fstDist, method=\"ward.D2\")\nplot(dendro, hang = -1, ylab = \"FST\", xlab = \"\", main = \"\")\n\n\n\n\nwhich again shows the strong drift that Native American populations (Karitiana) and Mayans experienced in their ancestral past.\nThis nicely shows how FST is affected by total drift, which is inversely proportional to population size, and proportional to total divergence time. A long branch can be caused by either low population size (as in the ancestral population of indigenous Americans) or long divergence time (as between populations from Africa and those outside of Africa)."
  },
  {
    "objectID": "pca_mds.html#theory",
    "href": "pca_mds.html#theory",
    "title": "7  Dimensionality reduction using PCA and MDS",
    "section": "7.1 Theory",
    "text": "7.1 Theory\n\n\n\n\n\n\nComparison of PCA and MDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA\nMDS\n\n\n\n\nInput\noriginal data matrix / similarity matrix\npairwise distance matrix\n\n\nFocus\ncaptures maximum variance in data\npreserves pairwise distances\n\n\nMissing data?\nFill-in OR projection\nNot an issue if using summary statistics, but this hides the uncertainty of the statistic\n\n\n\n\n\n\n\n7.1.1 Rationale\nThe datasets used in human ancient DNA analysis are often extremely multidimensional, often including data from thousands of individuals, across hundreds of thousands (or millions!) of single nucleotide polymrphisms (SNPs) (Mallick et al. 2023). Even when choosing to summarise this genome-wide information to single statistics of genetic similarity (e.g. with Outgroup F3), a similarity matrix across individuals can become very large when comparing across hundreds of individuals. As the name implies, dimensionality reduction methods can reduce the number of dimensions in the underlying data, while also aiming to minimise the loss of information. The two such methods we will focus on in this tutorial are Principal Component Analysis (PCA) and Multi-Dimensional Scaling (MDS). Both methods reveal structure within the dataset, and part of that structure is due to shared population history between individuals/populations. It is for that reason that both these methods are indispensible parts of an archaeogeneticist’s toolkit.\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif Lazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023. “The Allen Ancient DNA Resource (AADR): A Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\n\n\n7.1.2 Introduction to dimension reduction\nWhen using either of these methods, we are essentially representing the data on a new set of orthogonal axes, with its origin in the center of the data. In PCA we typically use the original data for this transformation (i.e. the genotype matrix), and attempt to find the axes that capture the most variation among the samples. A covariance matrix (i.e. a similarity matrix) is often calculated and used as a useful intermediate step in PCA. Instead, in MDS we start with a pairwise distance matrix (typically a matrix of 1-F3), and attempt to find a spatial representation that best captures the distances between points.\n\n\n\nA visual representation of the transformation PCA applies to a cloud of points, the range of which is represented by the blue oval. First, the data is rescaled around its own mean value, effectively moving the origin to the center of the data cloud (here shown s a red point). Then, the axes of maximal variation are discovered using linear algebra. Finally, the data is transformed to represent it along the identified axes of variation.\n\n\nThe results of both of these methods are (usually) a 2 dimensional plot in which the distances between individual points roughly correlates to the genetic distance between these individuals. Therefore, genetically similar individuals will be plotted close to one another, and further away from individuals that are more genetically dissimilar.\n\n\n7.1.3 The problem with missing data\nA recurring issue when analysing ancient DNA is the high degree of missing data (i.e. missingness). We often apply a minimum coverage filter to our datasets: A generally accepted rule-of-thumb for the 1240K dataset is a minimum of 15 000 covered (i.e. non-missing) SNPs. Another way to express this cutoff is to say that we will “happily” analyse data that is missing a genotype call in 98.8% of all SNPs in the dataset! So how does this high rate of missingness affect MDS and PCA?\n\n7.1.3.1 MDS\nMissingness does not affect MDS as adversely as it does PCA, on account of the use of a pairwise distance matrix of 1-F3. This matrix will only have missing values in cases where there is no overlapping coverage between two individuals/populations used in an F3 statistic. Instead, the issue with MDS is that all F3 statistics are treated as equally reliable, regardless of their associated error bar.\n\n\n7.1.3.2 PCA\nUnlike MDS, PCA is severely affected by missing data. During the rescaling of the data around its own mean values, missing data is “filled-in” to the mean value (mean imputation). This can cause points to shift towards the origin by a distance relative to the degree of missingness.\nBelow is a plot of the results of PCA on a dataset of differnt worldwide populations (Reich et al. 2012). In an attempt to limit the effects of colonial admixture on the studied Native American populations, the authors masked parts of the genomes of Native Americans that matched the European or African populations in their dataset, replacing those genotypes with missing data.\n\nReich, David, Nick Patterson, Desmond Campbell, Arti Tandon, Stéphane Mazieres, Nicolas Ray, Maria V Parra, et al. 2012. “Reconstructing Native American Population History.” Nature 488 (7411): 370–74.\n\n\n\nPCA results on the same dataset based on the raw data (left), and after masking parts of the genome in Native American populations that match European populations. Due to mean imputation, masked individuals are attracted to the Origin.\n\n\nAs you can see, individuals whose genotypes were masked are shifted towards the plot’s origin. So how can we use PCA with ancient samples that have high degrees of missingness? The answer is by using a Least Squares Projection, a.k.a. lsqproject!\n\n\n\n7.1.4 Projection\nThe idea of projection is simple, and applies similarly to both PCA and MDS. In PCA, you use a subset of the dataset to calculate your axes of variation, and then apply the resulting transformation to additional data, thus projecting them onto those axes. The important detail is that the variation between projected individuals is not taken into account when deciding which the axes of maximal variation are. Similarly, in MDS you project points to the MDS space based on their distances to the points that constructed the space, disregarding the distances of the projected points to one another.\nBelow is a PCA plot calculated on present-day West Eurasian populations together with some Mesolithic hunter-gatherer individuals (in light brown). In the right side plot, the ancient individuals have been included in the calculation of the principal components, while in the left side they are projected on the principal components of the present-day West Eurasians.\n\n\n\nPCA plot of ancient Mesolithic hunter-gatherers and present-day West Eurasian populations. On the right, the hunter gatherers are included in the principal component calculation, while on the left, they are projected on principal components calculated on the present-day populations only. The hunter-gatherers are part of three different groups: Eastern European hunter-gatherers (brown right-facing triangles), Scandinavian hunter-gatherers (brown diamonds), and Western European hunter-gatherers (brown half-filled circles).\n\n\nThere are two things to note here:\n\nFirst, comparing the placement of Eastern European hunter-gatherers (brown right-facing triangles) between the two plots, you can see that projecting these individuals does indeed provide results that are not affected by mean imputation, and thus are not shifted towards the origin.\nSecondly, if you compare the positions of the Western European hunter-gatherers (brown half-filled circles), you will notice that projection causes these individuals to be plotted closer to present-day populations.\n\nThe degree of missingness in the Western European hunter-gatherers (WHG) is relatively low, and hence the shift in their placement between the two plots is not the result of mean imputation. Instead, when projected the WHG illustrate the effects of shrinkage.\n\n\n7.1.5 Shrinkage\nShrinkage comes in two flavours:\n\nThe kind of shrinkage you saw with the WHGs above, is pretty intuitive. When projecting populations on axes of variation that do not capture all the variation of the projected populations, they will appear as if they have less variation than reality. This translates to the points “shrinking” towards the origin slightly. that is to say, because the WHG individuals come from a population that harboured far more genetic variation than is present within present-day West Eurasian populations, much of their true variation is “hidden” when projecting them.\nThe second kind of shrinkage (a.k.a. projection bias) arises because “samples used to calculate the PC axes”stretch” the axes” (from the smartpca documentation). This problem is exacerbated in datasets where the number of markers far exceeds the number of samples used for PC calculation. This is often the case in human population genomics.\n\nWhile the first shrinkage flavour can be argued to be a feature of PCA, projection bias can be a problem when trying to compare present-day populations to projected ancient populations. A demonstration of the effects of shrinkage can be seen below:\n\n\n\nUsing 10 individuals of each of the three tested populations (Yoruba, French, Han) to calculate PCs, and then projected another 10 individuals of each population reveals the effects of shrinkage on the positions of the projected individuals. In the absence of shrinkage, all points originating from the same population would be overlapping.\n\n\nShrinkage can be corrected by scaling the eigenvectors of the projected and/or non-projected individuals to bring them more in line with one another. Below is the same dataset as above, but ran through smartpca with the parameter shrinkmode: YES:\n\n\n\nUsing 10 individuals of each of the three tested populations (Yoruba, French, Han) to calculate PCs, and then projected another 10 individuals of each population. Shrinkage correction was done using ‘shrinkmode: YES’\n\n\nAs a note of caution, shrinkmode: Yes increases the runtime greatly. An alternative would be to identify specific present-day populations that are of interest for the ancient-to-modern comparison, and project those as well. So, for example, if we were to compare Iron Age individuals from Germany with present-day individuals from Germany, then we could decide to take out some or all present-day Germans and project those as well. That would make them fully comparable."
  },
  {
    "objectID": "pca_mds.html#practice",
    "href": "pca_mds.html#practice",
    "title": "7  Dimensionality reduction using PCA and MDS",
    "section": "7.2 Practice",
    "text": "7.2 Practice\n\n7.2.1 Preparation\n\n7.2.1.1 Get trident\nTrident is a Poseidon framework data management tool. It enables downloading Poseidon packages (genomic datasets, usually including ancient individuals, comprising genome-wide SNP data) from the Poseidon server, as well as creating and manipulating such packages.\ntrident for Linux:\n\n# download Trident v1.4.0.3 binary\nwget https://github.com/poseidon-framework/poseidon-hs/releases/download/v1.4.0.3/trident-Linux\n# rename to trident\nmv trident-Linux trident\n\n# make it executable\nchmod +x trident\n# run it\n./trident -h\n\ntrident for MacOS:\n\n# download Trident v1.4.0.3 binary\ncurl -LO https://github.com/poseidon-framework/poseidon-hs/releases/download/v1.4.0.3/trident-macOS\n# rename to trident\nmv trident-macOS trident\n\n# make it executable\nchmod +x trident\n# run it\n./trident -h\n\ntrident for Windows:\n\nDownload trident-Windows.exe file\n\n\n7.2.1.2 Prepare practise dataset:\nHere, we are downloading packages (listed in pca_mds_working/exampleData.fetchFile.txt) file from the Poseidon server into the scratch/poseidon-repository directory. The datasets come from the following publications: Patterson et al. 2012, Lazaridis et al. 2014, Raghavan et al. 2014, and Jeong et al. 2019\n\nmkdir -p scratch/poseidon-repository\n# This will take a few seconds to pull the data from the server\n./trident fetch -d scratch/poseidon-repository --fetchFile \"pca_mds_working/exampleData.fetchFile.txt\"\n\n\n# Check composition of one of the downloaded packages\nls scratch/poseidon-repository/2014_LazaridisNature-4.0.2\n\n\n# List all groups (populations) comprised by the downloaded packages\n./trident list --groups -d scratch/poseidon-repository/\n\n\n# Summarize information about  the downloaded packages\n./trident summarise -d scratch/poseidon-repository\n\n\n# Choose populations for the analysis (list for this exercise in \"exampleData.forgeFile.txt\"\")\nhead pca_mds_working/exampleData.forgeFile.txt\n\n\n# Count the number of listed populations to include\nwc -l pca_mds_working/exampleData.forgeFile.txt\n\n\n# Create (forge) a new repository with chosen groups from the downloaded packages\n./trident forge \\\n  -d scratch/poseidon-repository \\\n  -o scratch/forged_package \\\n  -n PCA_package_1 \\\n  --forgeFile pca_mds_working/exampleData.forgeFile.txt \\\n  --outFormat EIGENSTRAT\n\nThe created repository comprises genomic data for: 111 modern Eurasian populations, 6 modern Native American populations, and 1 Upper Palaeolithic Siberian individual MA-1 (“Mal’ta”).\n\n\n\n7.2.2 Run PCA\n\n# Prepare parameter file for the smartpca run\nmkdir -p scratch/smartpca_runs/poplist1 scratch/smartpca_runs/poplist2/\n\ncat &lt;&lt;EOF &gt; scratch/smartpca_runs/poplist1/parameters.par\ngenotypename:   scratch/forged_package/PCA_package_1.geno   ## Genotype data\nsnpname:    scratch/forged_package/PCA_package_1.snp        ## SNP information\nindivname:  scratch/forged_package/PCA_package_1.ind        ## Individual information\n\nevecoutname:    scratch/smartpca_runs/poplist1/PCA_poplist1.evec           ## Eigenvectors\nevaloutname:    scratch/smartpca_runs/poplist1/PCA_poplist1.eval           ## Eigenvalues\n\npoplistname:    pca_mds_working/PCA_poplists/PCA_poplist1.txt\n\nlsqproject: YES     ## Project individuals not included in PC calculation onto the PCs\noutliermode: 2      ## Turns off automatic outlier removal.\nnumoutevec:  4       ## The number of eigenvectors to print per sample. Default is 10.\nEOF\n\nSo prepared parameter file will cause smartpca to estimate PCs using only the individuals from the populations listed in PCA_poplist1.txt and project all the remaining individuals onto those estimated PCs.\n\n# Run smartpca\nsmartpca -p scratch/smartpca_runs/poplist1/parameters.par\n\n\n# Inspect the output files\nls scratch/smartpca_runs/poplist1/\nhead scratch/smartpca_runs/poplist1/PCA_poplist1.evec\n\nAdding populations (Native Americans)\n\n# Look into other provided poplists\nwc -l pca_mds_working/PCA_poplists/*\n\ndiff -y --suppress-common-lines pca_mds_working/PCA_poplists/PCA_poplist1.txt pca_mds_working/PCA_poplists/PCA_poplist2.txt\n\n\n# Replace poplist1 with poplist2 in the smartpca parameter file\nsed 's/poplist1/poplist2/g' scratch/smartpca_runs/poplist1/parameters.par &gt; scratch/smartpca_runs/poplist2/parameters.par\ncat scratch/smartpca_runs/poplist2/parameters.par\n\n\n# Rerun smartpca using poplist2 (additional populations)\nsmartpca -p scratch/smartpca_runs/poplist2/parameters.par\n\n# Inspect smartpca output\nls scratch/smartpca_runs/poplist2/\n\nSkipping projection (running smartpca without a poplist)\n\nmkdir -p scratch/smartpca_runs/all_pops\nhead -n 7 scratch/smartpca_runs/poplist1/parameters.par | sed 's/poplist1/all_pops/g' &gt; scratch/smartpca_runs/all_pops/parameters.par\ntail -n 2 scratch/smartpca_runs/poplist1/parameters.par &gt;&gt; scratch/smartpca_runs/all_pops/parameters.par\necho \"maxpops: 200\" &gt;&gt; scratch/smartpca_runs/all_pops/parameters.par\necho \"fastmode: YES\" &gt;&gt; scratch/smartpca_runs/all_pops/parameters.par\ncat scratch/smartpca_runs/all_pops/parameters.par\n\n\n## Runtime of about 2 minutes\nsmartpca -p scratch/smartpca_runs/all_pops/parameters.par\nls scratch/smartpca_runs/all_pops/\n\nAs a result we have three PCAs:\n\n\n\n\n\n\n\n\n\ndata used in PC estimation\ndata projected\n\n\n\n\nPCA_poplist1\nEurasians\nNative Americans, Mal’ta\n\n\nPCA_poplist2\nEurasians, Native Americans\nMal’ta\n\n\nPCA_all_pops\nEurasians, Native Americans, Mal’ta\n\n\n\n\n\n\n7.2.3 Plot PCA\n\nlibrary(tidyverse)\n\nif(!require('remotes')) install.packages('remotes')\nif (!require('janno')) remotes::install_github('poseidon-framework/janno')\n\n## Load in poplist data\n## poplist1 -- Eurasian populations\npoplist1 &lt;- readr::read_tsv(\"pca_mds_working/PCA_poplists/PCA_poplist1.txt\", col_names = \"Pops\", col_types = 'c')\n## poplist2 -- Eurasian populations + 6 Native American populations\npoplist2 &lt;- readr::read_tsv(\"pca_mds_working/PCA_poplists/PCA_poplist2.txt\", col_names = \"Pops\", col_types = 'c')\n\n## Load in eigenvector data\nPCA_poplist1_ev &lt;- readr::read_fwf(\"scratch/smartpca_runs/poplist1/PCA_poplist1.evec\", col_positions=readr::fwf_widths(c(20,11,12,12,12,19), col_names = c(\"Ind\",\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"Pop\")), col_types = 'cnnnnc', comment=\"#\")\nPCA_poplist2_ev &lt;- readr::read_fwf(\"scratch/smartpca_runs/poplist2/PCA_poplist2.evec\", col_positions=readr::fwf_widths(c(20,11,12,12,12,19), col_names = c(\"Ind\",\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"Pop\")), col_types = 'cnnnnc', comment=\"#\")\nPCA_all_pops_ev &lt;- readr::read_fwf(\"scratch/smartpca_runs/all_pops/PCA_all_pops.evec\", col_positions=readr::fwf_widths(c(20,11,12,12,12,19), col_names = c(\"Ind\",\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"Pop\")), col_types = 'cnnnnc', comment=\"#\")\n\n## Finally, we load in the metadata from the forged package annotation file (janno). Here, we keep only the individual Ids, country and their Lat/Lon position.\nmetadata&lt;-janno::read_janno(\"scratch/forged_package/PCA_package_1.janno\", to_janno=F)%&gt;% select(Poseidon_ID, Latitude, Longitude, Country) %&gt;% mutate(Longitude=as.double(Longitude), Latitude=as.double(Latitude))\n\n## Finally, we add the Lat/Lon information to our datasets\nPCA_poplist1_ev &lt;- left_join(PCA_poplist1_ev, metadata, by=c(\"Ind\"=\"Poseidon_ID\")) %&gt;% mutate(Country=as.factor(Country))\nPCA_poplist2_ev &lt;- left_join(PCA_poplist2_ev, metadata, by=c(\"Ind\"=\"Poseidon_ID\")) %&gt;% mutate(Country=as.factor(Country))\nPCA_all_pops_ev &lt;- left_join(PCA_all_pops_ev, metadata, by=c(\"Ind\"=\"Poseidon_ID\")) %&gt;% mutate(Country=as.factor(Country))\n\n\n## First we subset the dataset to only the populations in the poplist\nmoderns_pl1 &lt;- PCA_poplist1_ev %&gt;% filter(Pop %in% poplist1$Pops)\n\n\np &lt;- ggplot() +\n     coord_equal(xlim=c(-0.05,0.05),ylim=c(-0.05,0.15)) +\n     theme_minimal()\n\np + geom_point(\n        data=moderns_pl1, ##The input data for plotting\n        aes(x=PC1, y=PC2) ## Define the x and y axis\n        )\n\n\n## We can see how genetic similarity depends on geographical location by colouring the poins by longitude or latitude\nLon_plot &lt;- p +\n    geom_point(data=moderns_pl1, aes(x=PC1, y=PC2, col=Longitude)) ## Here we also define the colour of the points based on a variable\n\nLat_plot &lt;- p +\n    geom_point(data=moderns_pl1, aes(x=PC1, y=PC2, col=Latitude))\n\ngridExtra::grid.arrange(Lon_plot, Lat_plot, ncol=2)\n\n\n## As the orientation (+/-) of PC coordinates sometimes can change between runs of PCA, we use this code to ensure the same \"orientation\" for all users and thus enable making comparisons.\ncorner_inds_pl1 &lt;- moderns_pl1 %&gt;% select(Ind, PC1, PC2) %&gt;% filter(Ind %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_pl1$PC1[1] &gt; corner_inds_pl1$PC1[2]) { PCA_poplist1_ev &lt;- PCA_poplist1_ev %&gt;% mutate(PC1=-PC1)}\nif (corner_inds_pl1$PC2[1] &gt; corner_inds_pl1$PC2[2]) { PCA_poplist1_ev &lt;- PCA_poplist1_ev %&gt;% mutate(PC2=-PC2)}\nmoderns_pl1 &lt;- PCA_poplist1_ev %&gt;% filter(Pop %in% poplist1$Pops)\n\n\n## Let's now colour the points by country. \nPCA_plot_1 &lt;- p +\n    geom_point(data=moderns_pl1, \n               aes(x=PC1, y=PC2, col=Country), \n               alpha=0.5    ## Makes points semi transparent (so that aggregations of points are visible).\n              )\nPCA_plot_1\n\n\n\n\nPractice plot A) PCA estimated using modern Eurasian genomic data with Mal’ta projected (“PCA_poplist1”)\n\n\n\n## Now, let's see where Mal'ta individual got projected\nPCA_plot_1 +\n    geom_point(\n        data=PCA_poplist1_ev %&gt;% filter(Ind==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=PC1, y=PC2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n    )\nggsave(\"scratch/PCA_plot_1.png\")\n\nPCA with Native American populations added to the analysis\n\n## First we reorient the PCA\ncorner_inds_pl2 &lt;- PCA_poplist2_ev %&gt;% select(Ind, PC1, PC2) %&gt;% filter(Ind %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_pl2$PC1[1] &gt; corner_inds_pl2$PC1[2]) { PCA_poplist2_ev &lt;- PCA_poplist2_ev %&gt;% mutate(PC1=-PC1)}\nif (corner_inds_pl2$PC2[1] &gt; corner_inds_pl2$PC2[2]) { PCA_poplist2_ev &lt;- PCA_poplist2_ev %&gt;% mutate(PC2=-PC2)}\nmoderns_pl2 &lt;- PCA_poplist2_ev %&gt;% filter(Pop %in% poplist2$Pops)\n\n## Then we plot the output\nPCA_plot_2 &lt;-  ggplot() +\n     coord_equal(xlim=c(-0.05,0.05),ylim=c(-0.05,0.15)) +\n     theme_minimal() +\n     geom_point(data=moderns_pl2 %&gt;% filter(Country!=\"Brazil\" & Country!=\"Mexico\"), \n                aes(x=PC1, y=PC2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=moderns_pl2 %&gt;% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=PC1, y=PC2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n       geom_point(\n        data=PCA_poplist2_ev %&gt;% filter(Ind==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=PC1, y=PC2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\nPCA_plot_2\nggsave(\"scratch/PCA_plot_2.png\")\n\n\n\n\nPractice plot B) PCA estimated using modern Eurasian and Native American genomic data with Mal’ta projected (“PCA_poplist2”)\n\n\nAnd a PCA with all populations, including Mal’ta, used or estimation (no projection)\n\n## First we reorient the PCA\ncorner_inds_ap &lt;- PCA_all_pops_ev %&gt;% select(Ind, PC1, PC2) %&gt;% filter(Ind %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_ap$PC1[1] &gt; corner_inds_ap$PC1[2]) { PCA_all_pops_ev &lt;- PCA_all_pops_ev %&gt;% mutate(PC1=-PC1)}\nif (corner_inds_ap$PC2[1] &gt; corner_inds_ap$PC2[2]) { PCA_all_pops_ev &lt;- PCA_all_pops_ev %&gt;% mutate(PC2=-PC2)}\nmoderns_ap &lt;- PCA_all_pops_ev #%&gt;% filter(Pop %in% poplist2$Pops) ## Poplist 2 contains all the present-day populations.\n\n## Then we plot the output\nPCA_plot_ap &lt;-  ggplot() +\n     coord_equal(xlim=c(-0.05,0.05),ylim=c(-0.05,0.15)) +\n     theme_minimal() +\n     geom_point(data=moderns_ap %&gt;% filter(Country!=\"Brazil\" & Country!=\"Mexico\"), \n                aes(x=PC1, y=PC2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=moderns_ap %&gt;% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=PC1, y=PC2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(\n        data=PCA_all_pops_ev %&gt;% filter(Ind==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=PC1, y=PC2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\nPCA_plot_ap\nggsave(\"scratch/PCA_plot_ap.png\")\n\n\n\n\nPractice plot C) PCA estimated using modern Eurasian and Native American, and Mal’ta genomic data with no projection (“PCA_all_pops”\n\n\n\n\n7.2.4 PLINK MDS\nNow let’s get an MDS plot for pairwise distances for the same dataset\n\n##Convert package to PLINK format\n./trident genoconvert -d scratch/forged_package --outFormat PLINK\n\nls scratch/forged_package/\n\n# Compute pairwise distances of all individuals\nplink --bfile scratch/forged_package/PCA_package_1 --distance-matrix --out scratch/pairwise_distances\n\n\n## Read in individual IDs from MDS results\ninds &lt;- readr::read_tsv(\"scratch/pairwise_distances.mdist.id\", col_types=\"cc\", col_names=c(\"Population\", \"Poseidon_ID\"))\ninds\n\n\nmetadata &lt;- janno::read_janno(\"scratch/forged_package/PCA_package_1.janno\", to_janno=F)%&gt;% select(Poseidon_ID, Latitude, Longitude, Country) %&gt;% mutate(Longitude=as.double(Longitude), Latitude=as.double(Latitude))\n\n## Finally, we add the Lat/Lon information to our datasets\ninds &lt;- left_join(inds, metadata, by=\"Poseidon_ID\")\n\ndist_mat &lt;- matrix(scan(\"scratch/pairwise_distances.mdist\"), ncol=nrow(inds))\ndim(dist_mat)\n\n\n?heatmap\n\n\n# first try and filter for a few populations:\nunique(inds$Population)\n\nindices &lt;- inds$Population %in% c('French', 'Greek', 'Nganasan')\nhead(indices, 40)\n\n## Generate a heatmap of pairwise distances\nheatmap(dist_mat[indices,indices], labRow = inds$Population[indices], labCol = inds$Population[indices])\n\n\nlibrary(ggplot2)\nlibrary(magrittr) # This is for the pipe operator %&gt;%\nmds_coords &lt;- cmdscale(dist_mat)\ncolnames(mds_coords) &lt;- c(\"C1\", \"C2\")\nmds_coords &lt;- tibble::as_tibble(mds_coords) %&gt;%\n    dplyr::bind_cols(inds)\nmds_coords\n\n\ncorner_inds &lt;- mds_coords %&gt;% dplyr::select(Poseidon_ID, C1, C2) %&gt;% dplyr::filter(Poseidon_ID %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds$C1[1] &gt; corner_inds$C1[2]) { mds_coords &lt;- mds_coords %&gt;% mutate(C1=-C1)}\nif (corner_inds$C2[1] &gt; corner_inds$C2[2]) { mds_coords &lt;- mds_coords %&gt;% mutate(C2=-C2)}\n\nggplot(mds_coords) + \n       geom_point(data=mds_coords %&gt;% filter(Country!=\"Brazil\" & Country!=\"Mexico\"),                 aes(x=C1, y=C2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=mds_coords %&gt;% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=C1, y=C2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n    theme_minimal() +\n    coord_equal() +\n     geom_point(\n        data=mds_coords%&gt;% filter(Poseidon_ID==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=C1, y=C2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\nggsave(\"scratch/MDS_plot_ap.png\")\n\n\n\n\nPractice plot D) MDS estimated using modern Eurasian and Mal’ta genomic data (“MDS_plot_ap”)\n\n\n\n## How does MDS compare to PCA if we restrict to the populations in poplist1?\n## Read in the poplist\npoplist1 &lt;- readr::read_tsv(\"pca_mds_working/PCA_poplists/PCA_poplist1.txt\", col_names = \"Pops\", col_types = 'c')\n\n## Filter distance matrix\nindices_pl1 &lt;- inds$Population %in% poplist1$Pops\n\ndist_mat[indices_pl1, indices_pl1]\n\n\n## Do MDS\nmds_coords_pl1 &lt;- cmdscale(dist_mat[indices_pl1,indices_pl1])\ncolnames(mds_coords_pl1) &lt;- c(\"C1\", \"C2\")\nmds_coords_pl1 &lt;- tibble::as_tibble(mds_coords_pl1) %&gt;%\n    dplyr::bind_cols(inds %&gt;% dplyr::filter(inds$Population %in% poplist1$Pops))\nmds_coords_pl1\n\n\n## Reorient\ncorner_inds_mds1 &lt;- mds_coords_pl1 %&gt;% dplyr::select(Poseidon_ID, C1, C2) %&gt;% dplyr::filter(Poseidon_ID %in% c(\"HGDP00607\", \"Sir50\"))\nif (corner_inds_mds1$C1[1] &gt; corner_inds_mds1$C1[2]) { mds_coords_pl1 &lt;- mds_coords_pl1 %&gt;% mutate(C1=-C1)}\nif (corner_inds_mds1$C2[1] &gt; corner_inds_mds1$C2[2]) { mds_coords_pl1 &lt;- mds_coords_pl1 %&gt;% mutate(C2=-C2)}\n\n## Plot\nggplot(mds_coords_pl1) + \n     geom_point(data=mds_coords_pl1 %&gt;% filter(Country!=\"Brazil\" & Country!=\"Mexico\"), \n                aes(x=C1, y=C2, col=Country), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n     geom_point(data=mds_coords_pl1 %&gt;% filter(Country==\"Brazil\" | Country==\"Mexico\"), #Plotting the additional American populations separately to keep colors for other countries the same between the plots\n                aes(x=C1, y=C2), \n                alpha=0.5    ## Makes points semi transparent.\n               ) +\n    theme_minimal() +\n    coord_equal()+\n     geom_point(\n        data=mds_coords%&gt;% filter(Poseidon_ID==\"MA1.SG\"), ## Extract MA1 from the entire dataset\n        aes(x=C1, y=C2),       ## Set the x and y axes for this set of points\n        pch=17                   ## Change shape of point to solid triangle\n     )\n\nggsave(\"scratch/MDS_poplist1.png\")\n\n\n\n\nPractice plot E) MDS estimated using modern Eurasian and Native American, and Mal’ta genomic data (“MDS_poplist1”)\n\n\n\n\n7.2.5 Compare plots\nLet’s now sum up, by directly comparing all the plots we have generated:\n\n\n\n\n\n\nPCA estimated using modern Eurasian genomic data with Mal’ta projected (“PCA_poplist1”)\nPCA estimated using modern Eurasian and Native American genomic data with Mal’ta projected (“PCA_poplist2”)\nPCA estimated using modern Eurasian and Native American, and Mal’ta genomic data with no projection (“PCA_all_pops”)\nMDS estimated using modern Eurasian, and Mal’ta genomic data (“MDS_plot_ap”)\nMDS estimated using modern Eurasian and Native American, and Mal’ta genomic data (“MDS_poplist1”)\n\nThe dataset excluding Native American data comprises much less variation that makes up PC2 and hence the PC2 variation of Eurasians in plot A is stretched up compared to these in plots B and C (including Native Americans). In plot B and C it is therefore more difficult to observe differences within Eurasians along PC2 than in plot A. The variation making PC1 is comparable between the three plots. Individual Mal’ta is more closely related to Native Americans than an average Eurasian, so without Native Americans in the dataset (A) it ends up within the Eurasian variation as there is no Native American genetic signal present that would “pull” him away from the Eurasian variation towards the American variation. When Mal’ta is included in the estimation in plot C (not only projected, like in A and B), we can observe the effects of missing data, inherent for ancient genomes, causing this individual to be pulled towards the plot’s origin.\nIn MDS the missingness in Mal’ta’s data does not affect its position as it does in PCA. Inclusion of the diverged Native American data in the estimation does cause the decrease of the distances with the Eurasian population along the C2, but the difference between plot E and plot D is not as pronounced as between B and A."
  },
  {
    "objectID": "pca_mds.html#conclusions",
    "href": "pca_mds.html#conclusions",
    "title": "7  Dimensionality reduction using PCA and MDS",
    "section": "7.3 Conclusions",
    "text": "7.3 Conclusions\nIn PCA it is important to estimate the Eigenvalues using high-coverage samples, hence usually modern datasets, such as the 1000 Genomes Project (1kGP) or Human Genome Diversty Project (HGDP), are used for the estimation and the ancient samples are then projected onto the estimated PCs. Also, as mentioned above in the “Shrinkage” section, it is a good approach to project also some modern data if they are to be directly compared to the ancient samples.\nWhile MDS will be less sensitive to the effects of missing data, it disregards the uncertainty of the underlying pairwise distance estimates.\nIt is thus the best practice to perform both analyses and compare them taking the shortcomings of each into account when interpreting the relative positions of the studied individuals/populations obtained using these methods."
  },
  {
    "objectID": "pmrread.html#background-measures-of-relatedness",
    "href": "pmrread.html#background-measures-of-relatedness",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "8.1 Background: Measures of relatedness",
    "text": "8.1 Background: Measures of relatedness\n\n8.1.1 Coefficient of relationship\nCoefficient of relationship, denoted \\(r_{ij}\\) , defined by Sewall Wright in 1922 (Wright 1922), is a measure of the degree of biological relationship between two individuals, commonly used in genetics and genealogy. It calculates the proportion of genes that two individuals have in common as a result of their genetic relationship. The coefficient of relationship is a derivative of the coefficient of inbreeding (\\(f_k\\) or \\(C_{I_k}\\)) defined by Wright a year earlier. A coefficient of inbreeding for an individual is typically one-half the coefficient of relationship between the parents.\nCoefficient of relationship between the parents approaches a value of 1 as the level of inbreeding increases and approaches 0 the more remote the common ancestors are.\nIn human relationships, coefficient of relationship is often calculated based on the knowledge of the family tree, typically extending to up to three or four generations, using a formula:\n\\[\nr_{ij} = \\sum (^1/_2)^{L_{ij}}\n\\] where \\(L\\) is the numbers of generation links between two individuals (\\(i\\) and \\(j\\)). E.g. full siblings are linked by two links through the mother (siblingA - mother - siblingB) and two links through the father (siblingA - father - siblingB), therefore the coefficient of relationship between them is \\(r = (^1/_2)^2 + (^1/_2)^2 = (^1/_4) + (^1/_4) = (^1/_2)\\) , while e.g. a person with their aunt are linked by three links through the shared grandmother/mother and three through the shared grandfather/father, so \\(r=(^1/_2)^3 + (^1/_2)^3 = (^1/_8) + (^1/_8) = (^1/_4)\\) .\nNote that under such definition, the coefficient of relationship is a lower bound and an actual value that may be up to a few percent higher due to unaccounted for consanguinity within the pedigree. The value is accurate to within 1% if the full family tree of both individuals is known to a depth of seven generations.\n\n\n8.1.2 Kinship coefficient\nKinship coefficient, denoted \\(\\phi_{ij}\\) [fa:i], is the probability that one allele sampled from individual \\(i\\) and one allele sampled from the same locus from individual \\(j\\) are identical by descent.\n\\(1 - \\phi_{ij}\\) can thus be interpreted as the probability that a randomly sampled allele from each individual is not identical by descent. Assuming that alleles are not under linkage disequilibrium, this value can be estimated from genome-wide data for a pair of individuals.\nKinship coefficient \\(\\phi_{ij}\\) , under some assumptions such as limited inbreeding, is related to Wright’s coefficient of relationship, denoted \\(r_{ij}\\) , via:\n\\[\n\\phi_{ij} = (^1/_2)r_{ij}\n\\] and hence provides a direct relationship between the degrees of relatedness from the pedigree and the expected kinship coefficient \\(\\phi_{ij}\\) .\n\n\n\nTable 1. Values of the coefficient of relatedness and the kinship coefficient for different pedigree relationships up to the second-degree, assuming that \\(C_I\\) = 0. From Rohrlach et al. (2023)."
  },
  {
    "objectID": "pmrread.html#pairwise-mismatch-rate-pmr",
    "href": "pmrread.html#pairwise-mismatch-rate-pmr",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "8.2 Pairwise Mismatch Rate (PMR)",
    "text": "8.2 Pairwise Mismatch Rate (PMR)\nMethods of relatedness estimation applied routinely to modern data are not applicable to ancient genomes due to small numbers of individuals sampled and high rate of data missingness (ie. low coverage), as well as due to lack of diploid phased genomic data available for majority of such samples. Thus, estimation of the coefficient of relatedness for ancient individuals using these methods is prone to biases and generally unreliable.\nPairwise mismatch rate (PMR) was introduced by (Kennett et al. 2017) as a means to estimate relatedness between ancient individuals (Figure 1). For each pair of individuals they computed the average mismatch rate across all autosomal SNPs covered by at least one sequence read for both of the two compared individuals (when &gt;1 sequence read was present for one individual at a given site, a random read was sampled for the analysis) and computed standard errors using a weighted block jackknife. Mismatch rates significantly lower (Z&gt;3) than the highest observed value, provided putative evidence of relatedness.\nThe PMR can be used to estimate the kinship coefficient, which, assuming that we can account for the inbreeding coefficient \\(C_I\\), can be used to estimate the degree of relatedness. Hence, we may gain insights into the pedigree joining many individuals (to a certain resolution). Kennett et al’s (2017) \\(PMR\\) estimation, however, did not include a hard-classification method nor was wrapped into any particular software piece.\n\n\n\nFig 1. Pairwise mismatch rate calculation (PMR). A) Pseudohaploid genomes are compared within each pair of individuals. B) Only sites called in both individuals are considered (filled circles) and classified as match (green) or mismatch (red). Accounting for linkage disequilibrium: C) Estimation of PMR using sliding window (as implemented in READ) as well as genome-wide (optional in READv2). D) Estimation of PMR on thinned SNP data (as implemented in BREADR)\n\n\n\n8.2.1 Accounting for pseudohaploidization\nDue to pseudohaploidisation (ie. drawing one allele randomly for each position) identical individuals will have an expected PMR of half of this between unrelated individuals.\nThe estimate of relatedness coefficient \\(r\\) needs therefore be corrected using the expected mismatch rate in non-related individuals. In (Kennett et al. 2017) they chose correction based on the approximate maximum mismatch rates observed: \\(b = max(PMR_{observed})/2\\) . The estimator they used is thus:\n\\[\nr = 1 - ((PMR_{ij} - b)/b) .\n\\]\nPMR estimation in ancient-DNA-based inference of relatedness has first been implemented as separate software with READ (Monroy Kuhn, Jakobsson, and Günther 2018) and then by its successor - READv2 (Alaçamlı et al. 2024). Other software used in relatedness estimation among ancient individuals, such as BREADR (Rohrlach et al. 2023), also build on PMR.\n\n\n8.2.2 Accounting for linkage disequilibrium\nLinkage disequilibrium (LD, non-independent co-inheritance) of the loci included in the PMR estimation will bias the results towards falsely positive relatedness detection. To minimize this effect, different approaches can be employed. This is particularly crucial, when analyzing genome-wide (shotgun) data. In the 1240k SNP panel widely used in ancient genomics the analysed loci have already been selected taking LD into account. The approaches to minimize the LD bias that have been employed in PMR estimation comprise PMR estimation separately over consecutive genome fragments (sliding window) and obtaining median estimate among them (e.g., implemented in READv1, and as an option in READv2; Figure 1C), and decreasing the number of SNPs included in the analysis using a threshold of physical proximity of SNPs along the genome (thinning; e.g., implemented in BREADR; Figure 1D).\n\n\n8.2.3 Good practice\nOnly pairs with at least 10,000 overlapping SNPs of the 1240k SNP panel should be included in PMR estimation (Furtwängler et al. 2020)"
  },
  {
    "objectID": "pmrread.html#read-version-1",
    "href": "pmrread.html#read-version-1",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "8.3 READ (version 1)",
    "text": "8.3 READ (version 1)\nRelationship Estimation from Ancient DNA (Monroy Kuhn, Jakobsson, and Günther 2018) - formalized implementation of PMR for ancient genomic data.\n\nPseudohaploid input data (TPED/TFAM format)\nDivision of the genome into non-overlapping windows of 1 Mbps each and calculation of the proportion of non-matching alleles inside each window (P0) for each pair of individuals.\nNormalization of P0 using the value expected for a randomly chosen pair of unrelated individuals from the population (in order to make the classification independent of within population diversity, SNP ascertainment and marker density).\nThis value is estimated by calculation of the median of all average pairwise P0 per window across all pairs of individuals, which, under sufficient sample size, can be treated as a proxy for an expected P0 in a pair of unrelated individuals. Normalization can also be performed using parameters other than median, e.g., maximum observed P0 value among the pairs.\nClassification of each pair of individuals as unrelated, second-degree (i.e. nephew/niece-uncle/aunt, grandparent-grand- child or half-siblings), first-degree (parent-offspring or siblings) or identical individuals/identical twins from the average across the per-window proportions of non-matching alleles (P0).\n\n\n\n\nFigure 2. Outline of the general READ workflow to estimate the degree of relationship between two individuals. From (Monroy Kuhn, Jakobsson, and Günther 2018)\n\n\n\n8.3.1 Pluses\nThe method has been shown to work quite well with as little as 0.1x shotgun coverage per genome. It has very simple assumptions estimating the expected pairwise mismatch rate from the data without the need for population allele frequencies. It can thus be used as part of initial QC procedures (e.g. identifying duplicated individuals) or in populations (or species) for which little additional information is available.\n\n\n\n8.3.2 Minuses\nREAD had been implemented as a Python 2 script. The last version of Python 2 was released in 2020 and some systems have already stopped supporting the language. Furthermore, READ wrote a large number of temporary files to the hard disk which were then analyzed by a separate R script called from the Python script."
  },
  {
    "objectID": "pmrread.html#readv2",
    "href": "pmrread.html#readv2",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "8.4 READv2",
    "text": "8.4 READv2\nNew implementation of READ (Alaçamlı et al. 2024)\n\n8.4.1 Improvements over version 1\n\nAll analyses are carried out within a single Python3 script using NumPy (Harris et al. 2020) and pandas (McKinney 2010) libraries.\nMajor analysis speed improvement (although more memory-intensive)\nAvoids excessive use of temporary files and the calling of a separate R script.\nMinor gain in accuracy due to changes in some default values (based on performance tests results).\nIntroduction of the “effective number of overlapping SNPs” (number of overlapping SNPs times the pairwise mismatch rate expected for unrelated individuals) representing a measure of the amount of information available for kinship estimation in a given pair of individuals. Provides benchmarking and increases comparability between studies.\nAbility to classify up to third-degree relatives (requires at least 5000 effectively overlapping SNPs).\nAbility to differentiate between different types of first-degree relationships (ie. full siblings vs parent-offspring; requires at least 10,000 effectively overlapping SNPs).\nFeasible for as much as 696 individuals (241 860 pairs) provided enough available memory.\nGenome-wide P0 calculation as default; with the uncertainty estimated using a block-jackknife approach with block sizes of 5 Mb (as commonly employed in human population genomic studies; (Patterson et al. 2012)). Option to use window-based approach of chosen window size also available.\n\n\n\n\nFigure 3. READv2 flowchart. From (Alaçamlı et al. 2024)\n\n\n\n\n8.4.2 Noteworthy properties\nOverall, READv2 performs well down to at least 0.1X sequence data in the simulated dataset. This corresponded to on average about 1,878 overlapping SNPs for each pair of individuals at an expected mismatch for unrelated individuals of ∼0.247.\nIn default settings, READv2 performs a genome-wide estimate of the pairwise mismatch rate, based on which it will assess the degree of relationship in each pair of individuals. This is followed by a separate round of classification for first-degree relatives. Here, the genome is divided into 20Mb windows and the proportion of windows that are classified as either “identical/twin” or “unrelated” is estimated. These proportions correspond to Cotterman coefficients \\(k_0\\) – windows classified as unrelated (i.e. no shared chromosome), and \\(k_2\\) – windows classified as identical (i.e. both chromosomes shared). Expected \\(k_0 + k_2\\) is low for parent-offspring and around 0.5 for full siblings when sufficient data are available.\nIf that proportion is less than 0.3, the pair is classified as “parent-offspring”; if it is between 0.35 and 0.6, the pair is classified as “siblings”. For other proportions, or when the number of effectively overlapping SNPs is below 10,000, the pair remains classified as “first-degree” without further specification.\nThe two types of relations are well separated down to 0.5X coverage in the simulated dataset (or ∼8,000 “effectively overlapping SNPs”), but they overlap at 0.2X and below. Hence, to avoid wrongly classifying parent-offspring pairs as siblings, READv2 applies a default cutoff of 10,000 effectively overlapping SNPs, below which classification is not performed. (In (Rivollat et al. 2020) a cutoff of 7000 effective SNP number cutoff was used.)\n\n\n8.4.3 Warnings\nAt low coverage (0.05x and 0.1x) there are high false positive rates for second- and third-degree relatedness; many unrelated pairs are classified into these categories. At 0.01X, unrelated individuals are even classified as first-degree or identical twins, resulting in a reduced false positive rate for second- and third-degree but an increased false positive rate for first-degree. To avoid false classifications in empirical data, READv2 applies a conservative threshold of 5000 “effectively overlapping SNPs”, below which no attempt to classify third-degree relatives is taken.\nAccording to the authors READv2 alone can lead to very similar results as the combination of READv1 and lcMLkin. output table by including information such as the number of overlapping SNPs, the number of effectively overlapping SNPs, and the kinship coefficient \\(μ\\) (= 1 - normalized P0)\n\n\n8.4.4 Comparison with other methods\nREADv2 is very similar in its approach to BREADR (Rohrlach et al. 2023) and TKGWV2 (Fernandes et al. 2021), with each tool having its own unique features. READv2 has the functionality to separate the different first-degree relationships, BREADR has a better quantification of uncertainty, and TKGWV2 works well with lower amounts of input data.\n\n\n\nTable 2. Comparison of methods available for relatedness reconstruction in ancient genomic data. From Alaçamlı et al. (2024).\n\n\nFor further comparison of some of the methods available, you can also refer to the paper by Akturk et al. (2023)"
  },
  {
    "objectID": "pmrread.html#usage",
    "href": "pmrread.html#usage",
    "title": "8  Pairwise mismatch rate (PMR) and READ relatedness inference",
    "section": "8.5 Usage",
    "text": "8.5 Usage\n\n8.5.1 Running READ (version 1)\nREAD version 1 is implemented in Python 2.\nThe input for READ is a pair of files in Plink’s TPED/TFAM.\n## Get READ off Bitbucket\ngit clone https://bitbucket.org/tguenther/read.git\nIf starting from eigenstrat files you will need to use for example:\n\nconvertf (Eigensoft) and plink transpose:\n\n## Create a parameter file for Eigensoft convertf:\necho \"\ngenotypename:    myGenotypeFile.geno\nsnpname:         myGenotypeFile.snp\nindivname:       myGenotypeFile.ind\noutputformat:    PACKEDPED\ngenotypeoutname: myGenotypeFile.bed\nsnpoutname:      myGenotypeFile.bim\nindivoutname:    myGenotypeFile.fam\n\" &gt; scratch/myGenotypeFile.GenotToBed.convertf.param\n\n## Convert eigenstrat to packedped\nPathTo_EigensoftConvertf -p myGenotypeFile.GenotToBed.convertf.param\n\n## Transpose packedped to tped\nplink --bfile myGenotypeFile --recode transpose --out myGenotypeFile \n## The output is myGenotypeFile.tped, myGenotypeFile.tfam, myGenotypeFile.nosex, myGenotypeFile.log\nor\n\ntrident genoconvert, if you are working with Poseidon packages:\n\n## Convert eigenstrat-formatted package(s) to plink-formatted\ntrident genoconvert -d ... -d ... --outFormat PLINK\nWith the generated PLINK files you can no run READ:\n## Run READ\npython pathTo_Read.py myGenotypeFile &lt;normalization_method&gt;\n\n8.5.1.1 Options\nnormalization_method:\n\nmedian (default) - assuming that most pairs of compared individuals are unrelated, READ uses the median across all pairs for normalization.\nmean - READ uses the mean across all pairs for normalization, this would be more sensitive to outliers in the data (e.g. recent migrants or identical twins)\nmax - READ uses the maximum across all pairs for normalization. This should be used to test trios where both parents are supposed to be unrelated but the offspring is a first degree relative to both others.\nvalue &lt;val&gt; - READ uses a user-defined value for normalization. This can be used if the value from another population should be used for normalization. That would be useful if only two individuals are available for the test population. A value can be obtained from the NonNormalizedP0 column of the file meansP0_AncientDNA_normalized from a previous run of READ.\n\nOptionally, one can add --window_size &lt;value&gt; at the end in order to modify the window size used (default: 1000000).\n\n\n8.5.1.2 Output:\n“READ_results”:\nPairIndividuals Relationship Z_upper Z_lower I0232I0358 Unrelated NA -17.0238 ... I0354I0360 First Degree 3.8987 -9.6782 I0354I0361 Unrelated NA -15.4353 ... I0421I0430 First Degree 7.5325 -12.1995 I0421I0431 Unrelated NA -3.9037 ...\n““meansP0_AncientDNA_normalized”:\nPairIndividuals Normalized2AlleleDifference StandardError NonNormalizedP0 NonNormalizedStandardError I0232I0358 1.0069 0.0059 0.2531 0.00149 ... I0354I0360 0.7587 0.0138 0.1907 0.0035 I0354I0361 1.0092 0.0067 0.2537 0.0017 ... I0421I0430 0.7409 0.0095 0.1862 0.0024 I0421I0431 1.0115 0.0270 0.2543 0.0068 ...\n“READ_results_plot.pdf”:\n\n\n\n\n\n\n\n\n8.5.2 Running READ v2\nREADv2 is recommended to be ran as conda environment.\n## Preparing the environment\ngit clone https://github.com/GuntherLab/READv2.git\n\nconda create -n readv2 python=3.7 pandas=1.1.1 numpy=1.18.3 pip=22.3.1 matplotlib=3.5.3\n\nconda activate readv2\n\npip install plinkio  ## Used for format conversion of the inpt data.\n## Personally I couldn't make plinkio work and am using Poseidon/trident or eigensoft convertf (as described below).\nThe input for READv2 is a trio of files in Plink’s BED/BIM/FAM format.\nIf starting with files in EIGENSTRAT format, they can be converted to PLINK format using for example:\n\nconvertf (Eigensoft) and plink transpose:\n\n## Create convertf parameter file for converting .geno to .bed files (this can probably also be done using PLINKIO, as implemented in READv2 documentation):\necho \"\ngenotypename:    myGenotypeFile.geno\nsnpname:         myGenotypeFile.snp\nindivname:       myGenotypeFile.ind\noutputformat:    PACKEDPED\ngenotypeoutname: myGenotypeFile.bed\nsnpoutname:      myGenotypeFile.bim\nindivoutname:    myGenotypeFile.fam\n\" &gt; scratch/myGenotypeFile.GenoToBed.convertf.param\n    \n## Convert eigenstrat to packedped\nPathTo_EigensoftConvertf -p myGenotypeFile.GenotToBed.convertf.param\n## The output is myGenotypeFile.bed, myGenotypeFile.fam, myGenotypeFile.bim\nor\n\ntrident genoconvert, if you are working with Poseidon packages:\n\n## Convert eigenstrat-formatted package(s) to plink-formatted       \ntrident genoconvert -d ... -d ... --outFormat PLINK\nWith the PLINK format files you can run READv2:\n## Activate READv2 conda environment\nconda activate readv2\n\n## Run READv2\npython pathTo_Readv2.py -i myGenotypeFile\n\n8.5.2.1 Options\nREADv2 options:\n\n-i, --input_file val – Input file prefix (required). The current READ version only supports Plink bed/bim/fam files.\n-n, --norm_method val – Normalization method (either ‘mean’, ‘median’, ‘max’ or ‘value’).\n\nmedian (default) – assuming that most pairs of compared individuals are unrelated, READ uses the median across all pairs for normalization.\nmean – READ uses the mean across all pairs for normalization, this would be more sensitive to outliers in the data (e.g. recent migrants or identical twins)\nmax – READ uses the maximum across all pairs for normalization. This should be used to test trios where both parents are supposed to be unrelated but the offspring is a first-degree relative to both others.\nvalue – READ uses a user-defined value for normalization. This can be used if the value from another population should be used for normalization. That would be useful if only two individuals are available for the test population. Normalization value needs to be provided through --norm_value\n\n--norm_value val – Provide a user-defined normalization value\n--window_size val – Change window size for block jackknife or for window-based P0 estimates (as in READv1), default: 5000000\n--window_est – Window based estimate of P0 (as opposed to the genome-wide estimate, default in READv2)\n-h, --help – Print help message\n-v, --version – Print version\n\n\n\n8.5.2.2 Output:\n“meansP0_AncientDNA_normalized_READv2”\nPairIndividuals Norm2AlleleDiff StError_2Allele_Norm    Nonnormalized_P0    Nonnormalized_P0_serr   OverlapNSNPs\nI0232I0358  1.0052  0.0076    0.2553  0.0019    261263\n...\nI0234I0423  0.9659  0.0083  0.2453  0.0021    195053\nI0234I0424  0.9985  0.0071    0.2536 0.0018   224327\n...\nI0354I0360  0.7464  0.0137    0.1896 0.0035    21056\nI0354I0361  1.0046  0.0089    0.2552  0.0023    147438\n...\n“Read_Results.tsv”\nPairIndividuals Rel Zup Zdown   P0_mean Nonnormalized_P0    Nonnormalized_P0_serr   1st_Type    Perc_Win_1stdeg_P0  OverlapNSNPs    NSNPsXNorm  KinshipCoefficient\nI0232I0358  Unrelated   NA  12.9726  1.0052  0.2553  0.0019    N/A 0.9150  261263  66362.1298    -0.0052\n...\nI0234I0423  Third Degree    0.3487  7.1920   0.9659  0.2453  0.0021    N/A 0.7974  195053  49544.4533   0.0341\nI0234I0424  Unrelated   NA  12.9201  0.9985  0.2536 0.0018   N/A 0.9020  224327  56980.1981    0.0015\n...\nI0354I0360  First Degree    4.7682   8.7575   0.7464  0.1896 0.0035   N/A 0.3816  21056   5348.3310   0.2536\nI0354I0361  Unrelated   NA  11.0926 1.0046  0.2552  0.0023    N/A 0.9281  147438  37450.0013  -0.0046\n...\n“READ.pdf”:\n\n\n\n\n\n\n\n\n\nAktürk, Şevval, Igor Mapelli, Merve Nur Güler, Kanat Gürün, Büşra Katırcıoğlu, Kıvılcım Başak Vural, Ekin Sağlıcan, et al. 2023. “Benchmarking Kinship Estimation Tools for Ancient Genomes Using Pedigree Simulations.” bioRxiv.\n\n\nAlaçamlı, Erkin, Thijessen Naidoo, Şevval Aktürk, Merve N Güler, Igor Mapelli, Kıvılcım Başak Vural, Mehmet Somel, Helena Malmström, and Torsten Günther. 2024. “READv2: Advanced and User-Friendly Detection of Biological Relatedness in Archaeogenomics.” bioRxiv.\n\n\nFernandes, Daniel M, Olivia Cheronet, Pere Gelabert, and Ron Pinhasi. 2021. “TKGWV2: An Ancient DNA Relatedness Pipeline for Ultra-Low Coverage Whole Genome Shotgun Data.” Sci. Rep. 11 (1): 21262.\n\n\nFurtwängler, Anja, A B Rohrlach, Thiseas C Lamnidis, Luka Papac, Gunnar U Neumann, Inga Siebke, Ella Reiter, et al. 2020. “Ancient Genomes Reveal Social and Genetic Structure of Late Neolithic Switzerland.” Nat. Commun. 11 (1): 1915.\n\n\nHarris, Charles R, K Jarrod Millman, Stéfan J van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” Nature 585 (7825): 357–62.\n\n\nKennett, Douglas J, Stephen Plog, Richard J George, Brendan J Culleton, Adam S Watson, Pontus Skoglund, Nadin Rohland, et al. 2017. “Archaeogenomic Evidence Reveals Prehistoric Matrilineal Dynasty.” Nat. Commun. 8 (February): 14115.\n\n\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in Python,” 56–61.\n\n\nMonroy Kuhn, Jose Manuel, Mattias Jakobsson, and Torsten Günther. 2018. “Estimating Genetic Kin Relationships in Prehistoric Populations.” PLoS One 13 (4): e0195491.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin Rohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich. 2012. “Ancient Admixture in Human History.” Genetics 192 (3): 1065–93.\n\n\nRivollat, Maı̈té, Choongwon Jeong, Stephan Schiffels, İşil Küçükkalıpçı, Marie-Hélène Pemonge, Adam Benjamin Rohrlach, Kurt W Alt, et al. 2020. “Ancient Genome-Wide DNA from France Highlights the Complexity of Interactions Between Mesolithic Hunter-Gatherers and Neolithic Farmers.” Sci Adv 6 (22): eaaz5344.\n\n\nRohrlach, Adam B, Jonathan Tuke, Divyaratan Popli, and Wolfgang Haak. 2023. “BREADR: An r Package for the Bayesian Estimation of Genetic Relatedness from Low-Coverage Genotype Data.” bioRxiv. https://doi.org/10.1101/2023.04.17.537144.\n\n\nWright, Sewall. 1922. “Coefficients of Inbreeding and Relationship.” Am. Nat. 56 (645): 330–38."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aktürk, Şevval, Igor Mapelli, Merve Nur Güler, Kanat Gürün, Büşra\nKatırcıoğlu, Kıvılcım Başak Vural, Ekin Sağlıcan, et al. 2023.\n“Benchmarking Kinship Estimation Tools for Ancient Genomes Using\nPedigree Simulations.” bioRxiv.\n\n\nAlaçamlı, Erkin, Thijessen Naidoo, Şevval Aktürk, Merve N Güler, Igor\nMapelli, Kıvılcım Başak Vural, Mehmet Somel, Helena Malmström, and\nTorsten Günther. 2024. “READv2: Advanced and\nUser-Friendly Detection of Biological Relatedness in\nArchaeogenomics.” bioRxiv.\n\n\nAnnoni, A. et al. 2003. “Map Projections for Europe.”\nTechnical Report EUR 20120 EN. European Commission Joint Research\nCentre. http://mapref.org/LinkedDocuments/MapProjectionsForEurope-EUR-20120.pdf.\n\n\nAppelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan\nWoellauer. 2023. Mapview: Interactive Viewing of Spatial Data in\nr. https://github.com/r-spatial/mapview.\n\n\nBhatia, G, N Patterson, S Sankararaman, and A L Price. 2013.\n“Estimating and Interpreting FST: The Impact of Rare\nVariants.” Genome Research 23 (9): 1514–21. http://genome.cshlp.org/cgi/doi/10.1101/gr.154831.113.\n\n\nEwels, Philip A., Alexander Peltzer, Sven Fillinger, Harshil Patel,\nJohannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso,\nand Sven Nahnsen. 2020. “The Nf-Core Framework for\nCommunity-Curated Bioinformatics Pipelines.” Nature\nBiotechnology 38 (3): 276–78. https://doi.org/10.1038/s41587-020-0439-x.\n\n\nFellows Yates, James A., Thiseas C. Lamnidis, Maxime Borry, Aida\nAndrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U. Garcia,\nJudith Neukamm, and Alexander Peltzer. 2021. “Reproducible,\nPortable, and Efficient Ancient Genome Reconstruction with\nNf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\n\nFernandes, Daniel M, Olivia Cheronet, Pere Gelabert, and Ron Pinhasi.\n2021. “TKGWV2: An Ancient DNA\nRelatedness Pipeline for Ultra-Low Coverage Whole Genome Shotgun\nData.” Sci. Rep. 11 (1): 21262.\n\n\nFurtwängler, Anja, A B Rohrlach, Thiseas C Lamnidis, Luka Papac, Gunnar\nU Neumann, Inga Siebke, Ella Reiter, et al. 2020. “Ancient Genomes\nReveal Social and Genetic Structure of Late Neolithic\nSwitzerland.” Nat. Commun. 11 (1): 1915.\n\n\nGramacy, Robert B. 2016. “laGP:\nLarge-Scale Spatial Modeling via Local Approximate Gaussian Processes in\nR.” Journal of Statistical Software 72 (1):\n1–46. https://doi.org/10.18637/jss.v072.i01.\n\n\n———. 2020. Surrogates: Gaussian Process Modeling,\nDesign and  Optimization for the Applied Sciences. Boca Raton,\nFlorida: Chapman Hall/CRC.\n\n\nGreen, Richard E, Johannes Krause, Adrian W Briggs, Tomislav Maricic,\nUdo Stenzel, Martin Kircher, Nick Patterson, et al. 2010. “A Draft\nSequence of the Neandertal Genome.” Science 328 (5979):\n710–22. http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=20448178&retmode=ref&cmd=prlinks.\n\n\nHarris, Charles R, K Jarrod Millman, Stéfan J van der Walt, Ralf\nGommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020.\n“Array Programming with NumPy.”\nNature 585 (7825): 357–62.\n\n\nHudson, R R, M Slatkin, and W P Maddison. 1992. “Estimation of\nLevels of Gene Flow from DNA Sequence Data.”\nGenetics 132 (2): 583–89. https://doi.org/10.1093/genetics/132.2.583.\n\n\nKennett, Douglas J, Stephen Plog, Richard J George, Brendan J Culleton,\nAdam S Watson, Pontus Skoglund, Nadin Rohland, et al. 2017.\n“Archaeogenomic Evidence Reveals Prehistoric Matrilineal\nDynasty.” Nat. Commun. 8 (February): 14115.\n\n\nLamnidis, Thiseas C, Kerttu Majander, Choongwon Jeong, Elina Salmela,\nAnna Wessman, Vyacheslav Moiseyev, Valery Khartanovich, et al. 2018.\n“Ancient Fennoscandian Genomes Reveal Origin and Spread of\nSiberian Ancestry in Europe.” Nature Communications 9\n(1): 5018. https://doi.org/10.1038/s41467-018-07483-5.\n\n\nLazaridis, Iosif, Nick Patterson, Alissa Mittnik, Gabriel Renaud, Swapan\nMallick, Karola Kirsanow, Peter H Sudmant, et al. 2014. “Ancient\nHuman Genomes Suggest Three Ancestral Populations for Present-Day\nEuropeans.” Nature 513 (7518): 409–13. https://doi.org/10.1038/nature13673.\n\n\nMallick, Swapan, Adam Micco, Matthew Mah, Harald Ringbauer, Iosif\nLazaridis, Iñigo Olalde, Nick Patterson, and David Reich. 2023.\n“The Allen Ancient DNA Resource (AADR):\nA Curated Compendium of Ancient Human Genomes,” April. https://doi.org/10.1101/2023.04.06.535797.\n\n\nMartin, Simon H, John W Davey, and Chris D Jiggins. 2015.\n“Evaluating the Use of ABBA-BABA Statistics to Locate\nIntrogressed Loci.” Molecular Biology and Evolution 32\n(1): 244–57. https://doi.org/10.1093/molbev/msu269.\n\n\nMassicotte, Philippe, and Andy South. 2024. Rnaturalearth: World Map\nData from Natural Earth. https://docs.ropensci.org/rnaturalearth/.\n\n\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in\nPython,” 56–61.\n\n\nMonroy Kuhn, Jose Manuel, Mattias Jakobsson, and Torsten Günther. 2018.\n“Estimating Genetic Kin Relationships in Prehistoric\nPopulations.” PLoS One 13 (4): e0195491.\n\n\nPatterson, Nick, Priya Moorjani, Yontao Luo, Swapan Mallick, Nadin\nRohland, Yiping Zhan, Teri Genschoreck, Teresa Webster, and David Reich.\n2012a. “Ancient Admixture in Human History.”\nGenetics 192 (3): 1065–93. https://doi.org/10.1534/genetics.112.145037.\n\n\n———. 2012b. “Ancient Admixture in Human History.”\nGenetics 192 (3): 1065–93.\n\n\nPebesma, Edzer. 2018. “Simple Features for R:\nStandardized Support for Spatial Vector Data.”\nThe R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nPeter, Benjamin M. 2016. “Admixture, Population Structure, and\nF-Statistics.” Genetics 202 (4): 1485–1501.\nhttps://doi.org/10.1534/genetics.115.183913.\n\n\nReich, David, Nick Patterson, Desmond Campbell, Arti Tandon, Stéphane\nMazieres, Nicolas Ray, Maria V Parra, et al. 2012. “Reconstructing\nNative American Population History.” Nature 488 (7411):\n370–74.\n\n\nRivollat, Maı̈té, Choongwon Jeong, Stephan Schiffels, İşil Küçükkalıpçı,\nMarie-Hélène Pemonge, Adam Benjamin Rohrlach, Kurt W Alt, et al. 2020.\n“Ancient Genome-Wide DNA from France Highlights the\nComplexity of Interactions Between Mesolithic Hunter-Gatherers and\nNeolithic Farmers.” Sci Adv 6 (22): eaaz5344.\n\n\nRohrlach, Adam B, Jonathan Tuke, Divyaratan Popli, and Wolfgang Haak.\n2023. “BREADR: An r Package for the Bayesian Estimation of Genetic\nRelatedness from Low-Coverage Genotype Data.” bioRxiv.\nhttps://doi.org/10.1101/2023.04.17.537144.\n\n\nSchmid, Clemens, and Stephan Schiffels. 2023. “Estimating Human\nMobility in Holocene Western Eurasia with Large-Scale Ancient Genomic\nData.” Proceedings of the National Academy of Sciences\n120 (9). https://doi.org/10.1073/pnas.2218375120.\n\n\nTsoulos, Lysandros. 2003. “An Equal Area Projection for\nStatistical Mapping in the EU.” In Map Projections for\nEurope, edited by A. Annoni et al., 50–55. European Commission\nJoint Research Centre. http://mapref.org/LinkedDocuments/MapProjectionsForEurope-EUR-20120.pdf.\n\n\nWeir, B S, and W G Hill. 2002. “Estimating f-Statistics.”\nAnnual Review of Genetics 36: 721–50. https://doi.org/10.1146/annurev.genet.36.050802.093940.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilke, Claus O. 2024. Cowplot: Streamlined Plot Theme and Plot\nAnnotations for ’Ggplot2’. https://wilkelab.org/cowplot/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific\nData Management and Stewardship.” Scientific Data 3 (1).\nhttps://doi.org/10.1038/sdata.2016.18.\n\n\nWright, Sewall. 1922. “Coefficients of Inbreeding and\nRelationship.” Am. Nat. 56 (645): 330–38."
  }
]